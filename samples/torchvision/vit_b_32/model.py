import torch


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_x_: torch.Tensor,
        L_self_modules_conv_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_conv_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_parameters_class_token_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_parameters_pos_embedding_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_ln_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_ln_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_heads_modules_head_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_heads_modules_head_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_x_ = L_x_
        l_self_modules_conv_proj_parameters_weight_ = (
            L_self_modules_conv_proj_parameters_weight_
        )
        l_self_modules_conv_proj_parameters_bias_ = (
            L_self_modules_conv_proj_parameters_bias_
        )
        l_self_parameters_class_token_ = L_self_parameters_class_token_
        l_self_modules_encoder_parameters_pos_embedding_ = (
            L_self_modules_encoder_parameters_pos_embedding_
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_bias_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_weight_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_weight_
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_bias_ = L_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_bias_
        l_self_modules_encoder_modules_ln_parameters_weight_ = (
            L_self_modules_encoder_modules_ln_parameters_weight_
        )
        l_self_modules_encoder_modules_ln_parameters_bias_ = (
            L_self_modules_encoder_modules_ln_parameters_bias_
        )
        l_self_modules_heads_modules_head_parameters_weight_ = (
            L_self_modules_heads_modules_head_parameters_weight_
        )
        l_self_modules_heads_modules_head_parameters_bias_ = (
            L_self_modules_heads_modules_head_parameters_bias_
        )
        x = torch.conv2d(
            l_x_,
            l_self_modules_conv_proj_parameters_weight_,
            l_self_modules_conv_proj_parameters_bias_,
            (32, 32),
            (0, 0),
            (1, 1),
            1,
        )
        l_x_ = (
            l_self_modules_conv_proj_parameters_weight_
        ) = l_self_modules_conv_proj_parameters_bias_ = None
        x_1 = x.reshape(1, 768, 49)
        x = None
        x_2 = x_1.permute(0, 2, 1)
        x_1 = None
        batch_class_token = l_self_parameters_class_token_.expand(1, -1, -1)
        l_self_parameters_class_token_ = None
        x_3 = torch.cat([batch_class_token, x_2], dim=1)
        batch_class_token = x_2 = None
        input_1 = x_3 + l_self_modules_encoder_parameters_pos_embedding_
        x_3 = l_self_modules_encoder_parameters_pos_embedding_ = None
        dropout = torch.nn.functional.dropout(input_1, 0.0, False, False)
        input_1 = None
        x_4 = torch.nn.functional.layer_norm(
            dropout,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention = torch._native_multi_head_attention(
            x_4,
            x_4,
            x_4,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_4 = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_5 = _native_multi_head_attention[0]
        _native_multi_head_attention = None
        x_6 = torch.nn.functional.dropout(x_5, 0.0, False, False)
        x_5 = None
        x_7 = x_6 + dropout
        x_6 = dropout = None
        y = torch.nn.functional.layer_norm(
            x_7,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_ln_2_parameters_bias_ = (None)
        input_2 = torch._C._nn.linear(
            y,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_bias_,
        )
        y = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_0_parameters_bias_ = (None)
        input_3 = torch._C._nn.gelu(input_2, approximate="none")
        input_2 = None
        input_4 = torch.nn.functional.dropout(input_3, 0.0, False, False)
        input_3 = None
        input_5 = torch._C._nn.linear(
            input_4,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_bias_,
        )
        input_4 = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_0_modules_mlp_modules_3_parameters_bias_ = (None)
        input_6 = torch.nn.functional.dropout(input_5, 0.0, False, False)
        input_5 = None
        input_7 = x_7 + input_6
        x_7 = input_6 = None
        x_8 = torch.nn.functional.layer_norm(
            input_7,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_1 = torch._native_multi_head_attention(
            x_8,
            x_8,
            x_8,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_8 = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_9 = _native_multi_head_attention_1[0]
        _native_multi_head_attention_1 = None
        x_10 = torch.nn.functional.dropout(x_9, 0.0, False, False)
        x_9 = None
        x_11 = x_10 + input_7
        x_10 = input_7 = None
        y_1 = torch.nn.functional.layer_norm(
            x_11,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_ln_2_parameters_bias_ = (None)
        input_8 = torch._C._nn.linear(
            y_1,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_bias_,
        )
        y_1 = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_0_parameters_bias_ = (None)
        input_9 = torch._C._nn.gelu(input_8, approximate="none")
        input_8 = None
        input_10 = torch.nn.functional.dropout(input_9, 0.0, False, False)
        input_9 = None
        input_11 = torch._C._nn.linear(
            input_10,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_bias_,
        )
        input_10 = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_1_modules_mlp_modules_3_parameters_bias_ = (None)
        input_12 = torch.nn.functional.dropout(input_11, 0.0, False, False)
        input_11 = None
        input_13 = x_11 + input_12
        x_11 = input_12 = None
        x_12 = torch.nn.functional.layer_norm(
            input_13,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_2 = torch._native_multi_head_attention(
            x_12,
            x_12,
            x_12,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_12 = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_13 = _native_multi_head_attention_2[0]
        _native_multi_head_attention_2 = None
        x_14 = torch.nn.functional.dropout(x_13, 0.0, False, False)
        x_13 = None
        x_15 = x_14 + input_13
        x_14 = input_13 = None
        y_2 = torch.nn.functional.layer_norm(
            x_15,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_ln_2_parameters_bias_ = (None)
        input_14 = torch._C._nn.linear(
            y_2,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_bias_,
        )
        y_2 = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_0_parameters_bias_ = (None)
        input_15 = torch._C._nn.gelu(input_14, approximate="none")
        input_14 = None
        input_16 = torch.nn.functional.dropout(input_15, 0.0, False, False)
        input_15 = None
        input_17 = torch._C._nn.linear(
            input_16,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_bias_,
        )
        input_16 = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_2_modules_mlp_modules_3_parameters_bias_ = (None)
        input_18 = torch.nn.functional.dropout(input_17, 0.0, False, False)
        input_17 = None
        input_19 = x_15 + input_18
        x_15 = input_18 = None
        x_16 = torch.nn.functional.layer_norm(
            input_19,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_3 = torch._native_multi_head_attention(
            x_16,
            x_16,
            x_16,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_16 = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_17 = _native_multi_head_attention_3[0]
        _native_multi_head_attention_3 = None
        x_18 = torch.nn.functional.dropout(x_17, 0.0, False, False)
        x_17 = None
        x_19 = x_18 + input_19
        x_18 = input_19 = None
        y_3 = torch.nn.functional.layer_norm(
            x_19,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_ln_2_parameters_bias_ = (None)
        input_20 = torch._C._nn.linear(
            y_3,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_bias_,
        )
        y_3 = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_0_parameters_bias_ = (None)
        input_21 = torch._C._nn.gelu(input_20, approximate="none")
        input_20 = None
        input_22 = torch.nn.functional.dropout(input_21, 0.0, False, False)
        input_21 = None
        input_23 = torch._C._nn.linear(
            input_22,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_bias_,
        )
        input_22 = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_3_modules_mlp_modules_3_parameters_bias_ = (None)
        input_24 = torch.nn.functional.dropout(input_23, 0.0, False, False)
        input_23 = None
        input_25 = x_19 + input_24
        x_19 = input_24 = None
        x_20 = torch.nn.functional.layer_norm(
            input_25,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_4 = torch._native_multi_head_attention(
            x_20,
            x_20,
            x_20,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_20 = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_21 = _native_multi_head_attention_4[0]
        _native_multi_head_attention_4 = None
        x_22 = torch.nn.functional.dropout(x_21, 0.0, False, False)
        x_21 = None
        x_23 = x_22 + input_25
        x_22 = input_25 = None
        y_4 = torch.nn.functional.layer_norm(
            x_23,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_ln_2_parameters_bias_ = (None)
        input_26 = torch._C._nn.linear(
            y_4,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_bias_,
        )
        y_4 = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_0_parameters_bias_ = (None)
        input_27 = torch._C._nn.gelu(input_26, approximate="none")
        input_26 = None
        input_28 = torch.nn.functional.dropout(input_27, 0.0, False, False)
        input_27 = None
        input_29 = torch._C._nn.linear(
            input_28,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_bias_,
        )
        input_28 = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_4_modules_mlp_modules_3_parameters_bias_ = (None)
        input_30 = torch.nn.functional.dropout(input_29, 0.0, False, False)
        input_29 = None
        input_31 = x_23 + input_30
        x_23 = input_30 = None
        x_24 = torch.nn.functional.layer_norm(
            input_31,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_5 = torch._native_multi_head_attention(
            x_24,
            x_24,
            x_24,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_24 = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_25 = _native_multi_head_attention_5[0]
        _native_multi_head_attention_5 = None
        x_26 = torch.nn.functional.dropout(x_25, 0.0, False, False)
        x_25 = None
        x_27 = x_26 + input_31
        x_26 = input_31 = None
        y_5 = torch.nn.functional.layer_norm(
            x_27,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_ln_2_parameters_bias_ = (None)
        input_32 = torch._C._nn.linear(
            y_5,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_bias_,
        )
        y_5 = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_0_parameters_bias_ = (None)
        input_33 = torch._C._nn.gelu(input_32, approximate="none")
        input_32 = None
        input_34 = torch.nn.functional.dropout(input_33, 0.0, False, False)
        input_33 = None
        input_35 = torch._C._nn.linear(
            input_34,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_bias_,
        )
        input_34 = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_5_modules_mlp_modules_3_parameters_bias_ = (None)
        input_36 = torch.nn.functional.dropout(input_35, 0.0, False, False)
        input_35 = None
        input_37 = x_27 + input_36
        x_27 = input_36 = None
        x_28 = torch.nn.functional.layer_norm(
            input_37,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_6 = torch._native_multi_head_attention(
            x_28,
            x_28,
            x_28,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_28 = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_29 = _native_multi_head_attention_6[0]
        _native_multi_head_attention_6 = None
        x_30 = torch.nn.functional.dropout(x_29, 0.0, False, False)
        x_29 = None
        x_31 = x_30 + input_37
        x_30 = input_37 = None
        y_6 = torch.nn.functional.layer_norm(
            x_31,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_ln_2_parameters_bias_ = (None)
        input_38 = torch._C._nn.linear(
            y_6,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_bias_,
        )
        y_6 = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_0_parameters_bias_ = (None)
        input_39 = torch._C._nn.gelu(input_38, approximate="none")
        input_38 = None
        input_40 = torch.nn.functional.dropout(input_39, 0.0, False, False)
        input_39 = None
        input_41 = torch._C._nn.linear(
            input_40,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_bias_,
        )
        input_40 = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_6_modules_mlp_modules_3_parameters_bias_ = (None)
        input_42 = torch.nn.functional.dropout(input_41, 0.0, False, False)
        input_41 = None
        input_43 = x_31 + input_42
        x_31 = input_42 = None
        x_32 = torch.nn.functional.layer_norm(
            input_43,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_7 = torch._native_multi_head_attention(
            x_32,
            x_32,
            x_32,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_32 = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_33 = _native_multi_head_attention_7[0]
        _native_multi_head_attention_7 = None
        x_34 = torch.nn.functional.dropout(x_33, 0.0, False, False)
        x_33 = None
        x_35 = x_34 + input_43
        x_34 = input_43 = None
        y_7 = torch.nn.functional.layer_norm(
            x_35,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_ln_2_parameters_bias_ = (None)
        input_44 = torch._C._nn.linear(
            y_7,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_bias_,
        )
        y_7 = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_0_parameters_bias_ = (None)
        input_45 = torch._C._nn.gelu(input_44, approximate="none")
        input_44 = None
        input_46 = torch.nn.functional.dropout(input_45, 0.0, False, False)
        input_45 = None
        input_47 = torch._C._nn.linear(
            input_46,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_bias_,
        )
        input_46 = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_7_modules_mlp_modules_3_parameters_bias_ = (None)
        input_48 = torch.nn.functional.dropout(input_47, 0.0, False, False)
        input_47 = None
        input_49 = x_35 + input_48
        x_35 = input_48 = None
        x_36 = torch.nn.functional.layer_norm(
            input_49,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_8 = torch._native_multi_head_attention(
            x_36,
            x_36,
            x_36,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_36 = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_37 = _native_multi_head_attention_8[0]
        _native_multi_head_attention_8 = None
        x_38 = torch.nn.functional.dropout(x_37, 0.0, False, False)
        x_37 = None
        x_39 = x_38 + input_49
        x_38 = input_49 = None
        y_8 = torch.nn.functional.layer_norm(
            x_39,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_ln_2_parameters_bias_ = (None)
        input_50 = torch._C._nn.linear(
            y_8,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_bias_,
        )
        y_8 = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_0_parameters_bias_ = (None)
        input_51 = torch._C._nn.gelu(input_50, approximate="none")
        input_50 = None
        input_52 = torch.nn.functional.dropout(input_51, 0.0, False, False)
        input_51 = None
        input_53 = torch._C._nn.linear(
            input_52,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_bias_,
        )
        input_52 = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_8_modules_mlp_modules_3_parameters_bias_ = (None)
        input_54 = torch.nn.functional.dropout(input_53, 0.0, False, False)
        input_53 = None
        input_55 = x_39 + input_54
        x_39 = input_54 = None
        x_40 = torch.nn.functional.layer_norm(
            input_55,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_9 = torch._native_multi_head_attention(
            x_40,
            x_40,
            x_40,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_40 = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_41 = _native_multi_head_attention_9[0]
        _native_multi_head_attention_9 = None
        x_42 = torch.nn.functional.dropout(x_41, 0.0, False, False)
        x_41 = None
        x_43 = x_42 + input_55
        x_42 = input_55 = None
        y_9 = torch.nn.functional.layer_norm(
            x_43,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_ln_2_parameters_bias_ = (None)
        input_56 = torch._C._nn.linear(
            y_9,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_bias_,
        )
        y_9 = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_0_parameters_bias_ = (None)
        input_57 = torch._C._nn.gelu(input_56, approximate="none")
        input_56 = None
        input_58 = torch.nn.functional.dropout(input_57, 0.0, False, False)
        input_57 = None
        input_59 = torch._C._nn.linear(
            input_58,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_bias_,
        )
        input_58 = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_9_modules_mlp_modules_3_parameters_bias_ = (None)
        input_60 = torch.nn.functional.dropout(input_59, 0.0, False, False)
        input_59 = None
        input_61 = x_43 + input_60
        x_43 = input_60 = None
        x_44 = torch.nn.functional.layer_norm(
            input_61,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_10 = torch._native_multi_head_attention(
            x_44,
            x_44,
            x_44,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_44 = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_45 = _native_multi_head_attention_10[0]
        _native_multi_head_attention_10 = None
        x_46 = torch.nn.functional.dropout(x_45, 0.0, False, False)
        x_45 = None
        x_47 = x_46 + input_61
        x_46 = input_61 = None
        y_10 = torch.nn.functional.layer_norm(
            x_47,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_ln_2_parameters_bias_ = (None)
        input_62 = torch._C._nn.linear(
            y_10,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_bias_,
        )
        y_10 = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_0_parameters_bias_ = (None)
        input_63 = torch._C._nn.gelu(input_62, approximate="none")
        input_62 = None
        input_64 = torch.nn.functional.dropout(input_63, 0.0, False, False)
        input_63 = None
        input_65 = torch._C._nn.linear(
            input_64,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_bias_,
        )
        input_64 = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_10_modules_mlp_modules_3_parameters_bias_ = (None)
        input_66 = torch.nn.functional.dropout(input_65, 0.0, False, False)
        input_65 = None
        input_67 = x_47 + input_66
        x_47 = input_66 = None
        x_48 = torch.nn.functional.layer_norm(
            input_67,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_1_parameters_bias_ = (None)
        _native_multi_head_attention_11 = torch._native_multi_head_attention(
            x_48,
            x_48,
            x_48,
            768,
            12,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_bias_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_bias_,
            None,
            False,
            True,
            None,
        )
        x_48 = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_parameters_in_proj_bias_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_self_attention_modules_out_proj_parameters_bias_ = (None)
        x_49 = _native_multi_head_attention_11[0]
        _native_multi_head_attention_11 = None
        x_50 = torch.nn.functional.dropout(x_49, 0.0, False, False)
        x_49 = None
        x_51 = x_50 + input_67
        x_50 = input_67 = None
        y_11 = torch.nn.functional.layer_norm(
            x_51,
            (768,),
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_bias_,
            1e-06,
        )
        l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_ln_2_parameters_bias_ = (None)
        input_68 = torch._C._nn.linear(
            y_11,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_bias_,
        )
        y_11 = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_0_parameters_bias_ = (None)
        input_69 = torch._C._nn.gelu(input_68, approximate="none")
        input_68 = None
        input_70 = torch.nn.functional.dropout(input_69, 0.0, False, False)
        input_69 = None
        input_71 = torch._C._nn.linear(
            input_70,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_weight_,
            l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_bias_,
        )
        input_70 = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_weight_ = l_self_modules_encoder_modules_layers_modules_encoder_layer_11_modules_mlp_modules_3_parameters_bias_ = (None)
        input_72 = torch.nn.functional.dropout(input_71, 0.0, False, False)
        input_71 = None
        input_73 = x_51 + input_72
        x_51 = input_72 = None
        x_52 = torch.nn.functional.layer_norm(
            input_73,
            (768,),
            l_self_modules_encoder_modules_ln_parameters_weight_,
            l_self_modules_encoder_modules_ln_parameters_bias_,
            1e-06,
        )
        input_73 = (
            l_self_modules_encoder_modules_ln_parameters_weight_
        ) = l_self_modules_encoder_modules_ln_parameters_bias_ = None
        x_53 = x_52[(slice(None, None, None), 0)]
        x_52 = None
        input_74 = torch._C._nn.linear(
            x_53,
            l_self_modules_heads_modules_head_parameters_weight_,
            l_self_modules_heads_modules_head_parameters_bias_,
        )
        x_53 = (
            l_self_modules_heads_modules_head_parameters_weight_
        ) = l_self_modules_heads_modules_head_parameters_bias_ = None
        return (input_74,)
