import torch


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_self_modules_stem_modules_0_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_x_: torch.Tensor,
        L_self_modules_stem_modules_0_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_stem_modules_0_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_stem_modules_0_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_stem_modules_0_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_stem_modules_1_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_stem_modules_1_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_: torch.Tensor,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_modules_2_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_modules_2_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_modules_3_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_modules_3_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_modules_5_parameters_weight_: torch.nn.parameter.Parameter,
    ):
        l_self_modules_stem_modules_0_modules_0_parameters_weight_ = (
            L_self_modules_stem_modules_0_modules_0_parameters_weight_
        )
        l_x_ = L_x_
        l_self_modules_stem_modules_0_modules_1_buffers_running_mean_ = (
            L_self_modules_stem_modules_0_modules_1_buffers_running_mean_
        )
        l_self_modules_stem_modules_0_modules_1_buffers_running_var_ = (
            L_self_modules_stem_modules_0_modules_1_buffers_running_var_
        )
        l_self_modules_stem_modules_0_modules_1_parameters_weight_ = (
            L_self_modules_stem_modules_0_modules_1_parameters_weight_
        )
        l_self_modules_stem_modules_0_modules_1_parameters_bias_ = (
            L_self_modules_stem_modules_0_modules_1_parameters_bias_
        )
        l_self_modules_stem_modules_1_modules_0_parameters_weight_ = (
            L_self_modules_stem_modules_1_modules_0_parameters_weight_
        )
        l_self_modules_stem_modules_1_modules_0_parameters_bias_ = (
            L_self_modules_stem_modules_1_modules_0_parameters_bias_
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_proj_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_mean_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_buffers_running_var_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_pre_norm_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_a_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_b_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_MBconv_modules_layers_modules_conv_c_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = L_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_
        l_self_modules_classifier_modules_2_parameters_weight_ = (
            L_self_modules_classifier_modules_2_parameters_weight_
        )
        l_self_modules_classifier_modules_2_parameters_bias_ = (
            L_self_modules_classifier_modules_2_parameters_bias_
        )
        l_self_modules_classifier_modules_3_parameters_weight_ = (
            L_self_modules_classifier_modules_3_parameters_weight_
        )
        l_self_modules_classifier_modules_3_parameters_bias_ = (
            L_self_modules_classifier_modules_3_parameters_bias_
        )
        l_self_modules_classifier_modules_5_parameters_weight_ = (
            L_self_modules_classifier_modules_5_parameters_weight_
        )
        input_1 = torch.conv2d(
            l_x_,
            l_self_modules_stem_modules_0_modules_0_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        l_x_ = l_self_modules_stem_modules_0_modules_0_parameters_weight_ = None
        input_2 = torch.nn.functional.batch_norm(
            input_1,
            l_self_modules_stem_modules_0_modules_1_buffers_running_mean_,
            l_self_modules_stem_modules_0_modules_1_buffers_running_var_,
            l_self_modules_stem_modules_0_modules_1_parameters_weight_,
            l_self_modules_stem_modules_0_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_1 = (
            l_self_modules_stem_modules_0_modules_1_buffers_running_mean_
        ) = (
            l_self_modules_stem_modules_0_modules_1_buffers_running_var_
        ) = (
            l_self_modules_stem_modules_0_modules_1_parameters_weight_
        ) = l_self_modules_stem_modules_0_modules_1_parameters_bias_ = None
        input_3 = torch._C._nn.gelu(input_2, approximate="none")
        input_2 = None
        input_4 = torch.conv2d(
            input_3,
            l_self_modules_stem_modules_1_modules_0_parameters_weight_,
            l_self_modules_stem_modules_1_modules_0_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1,
        )
        input_3 = (
            l_self_modules_stem_modules_1_modules_0_parameters_weight_
        ) = l_self_modules_stem_modules_1_modules_0_parameters_bias_ = None
        input_5 = torch._C._nn.avg_pool2d(input_4, 3, 2, 1, False, True, None)
        input_6 = torch.conv2d(
            input_5,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_5 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_ = (None)
        input_7 = torch.nn.functional.batch_norm(
            input_4,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_4 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_8 = torch.conv2d(
            input_7,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_7 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_9 = torch.nn.functional.batch_norm(
            input_8,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_8 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_10 = torch._C._nn.gelu(input_9, approximate="none")
        input_9 = None
        input_11 = torch.conv2d(
            input_10,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            256,
        )
        input_10 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_12 = torch.nn.functional.batch_norm(
            input_11,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_11 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_13 = torch._C._nn.gelu(input_12, approximate="none")
        input_12 = None
        scale = torch.nn.functional.adaptive_avg_pool2d(input_13, 1)
        scale_1 = torch.conv2d(
            scale,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_2 = torch.nn.functional.silu(scale_1, inplace=False)
        scale_1 = None
        scale_3 = torch.conv2d(
            scale_2,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_2 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_4 = torch.sigmoid(scale_3)
        scale_3 = None
        input_14 = scale_4 * input_13
        scale_4 = input_13 = None
        input_15 = torch.conv2d(
            input_14,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_14 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        input_16 = input_6 + input_15
        input_6 = input_15 = None
        x = input_16.reshape(1, 64, 8, 7, 8, 7)
        input_16 = None
        x_1 = x.permute(0, 2, 4, 3, 5, 1)
        x = None
        x_2 = x_1.reshape(1, 64, 49, 64)
        x_1 = None
        input_17 = torch.nn.functional.layer_norm(
            x_2,
            (64,),
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv = torch._C._nn.linear(
            input_17,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_17 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk = torch.chunk(qkv, 3, dim=-1)
        qkv = None
        q = chunk[0]
        k = chunk[1]
        v = chunk[2]
        chunk = None
        reshape_2 = q.reshape(1, 64, 49, 2, 32)
        q = None
        q_1 = reshape_2.permute(0, 1, 3, 2, 4)
        reshape_2 = None
        reshape_3 = k.reshape(1, 64, 49, 2, 32)
        k = None
        k_1 = reshape_3.permute(0, 1, 3, 2, 4)
        reshape_3 = None
        reshape_4 = v.reshape(1, 64, 49, 2, 32)
        v = None
        v_1 = reshape_4.permute(0, 1, 3, 2, 4)
        reshape_4 = None
        k_2 = k_1 * 0.125
        k_1 = None
        dot_prod = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_1, k_2
        )
        q_1 = k_2 = None
        bias_index = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_3 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index
        ]
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index
        ) = None
        relative_bias = getitem_3.view(49, 49, -1)
        getitem_3 = None
        permute_4 = relative_bias.permute(2, 0, 1)
        relative_bias = None
        relative_bias_1 = permute_4.contiguous()
        permute_4 = None
        pos_bias = relative_bias_1.unsqueeze(0)
        relative_bias_1 = None
        add_1 = dot_prod + pos_bias
        dot_prod = pos_bias = None
        dot_prod_1 = torch.nn.functional.softmax(add_1, dim=-1)
        add_1 = None
        out = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_1, v_1
        )
        dot_prod_1 = v_1 = None
        permute_5 = out.permute(0, 1, 3, 2, 4)
        out = None
        out_1 = permute_5.reshape(1, 64, 49, 64)
        permute_5 = None
        out_2 = torch._C._nn.linear(
            out_1,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_1 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_18 = torch.nn.functional.dropout(out_2, 0.0, False, False)
        out_2 = None
        _log_api_usage_once = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once = None
        x_3 = x_2 + input_18
        x_2 = input_18 = None
        input_19 = torch.nn.functional.layer_norm(
            x_3,
            (64,),
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_20 = torch._C._nn.linear(
            input_19,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_19 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_21 = torch._C._nn.gelu(input_20, approximate="none")
        input_20 = None
        input_22 = torch._C._nn.linear(
            input_21,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_21 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_23 = torch.nn.functional.dropout(input_22, 0.0, False, False)
        input_22 = None
        _log_api_usage_once_1 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_1 = None
        x_4 = x_3 + input_23
        x_3 = input_23 = None
        x_5 = x_4.reshape(1, 8, 8, 7, 7, 64)
        x_4 = None
        x_6 = x_5.permute(0, 5, 1, 3, 2, 4)
        x_5 = None
        x_7 = x_6.reshape(1, 64, 56, 56)
        x_6 = None
        x_8 = x_7.reshape(1, 64, 7, 8, 7, 8)
        x_7 = None
        x_9 = x_8.permute(0, 2, 4, 3, 5, 1)
        x_8 = None
        x_10 = x_9.reshape(1, 49, 64, 64)
        x_9 = None
        res = torch.swapaxes(x_10, -2, -3)
        x_10 = None
        input_24 = torch.nn.functional.layer_norm(
            res,
            (64,),
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_1 = torch._C._nn.linear(
            input_24,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_24 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_1 = torch.chunk(qkv_1, 3, dim=-1)
        qkv_1 = None
        q_2 = chunk_1[0]
        k_3 = chunk_1[1]
        v_2 = chunk_1[2]
        chunk_1 = None
        reshape_10 = q_2.reshape(1, 64, 49, 2, 32)
        q_2 = None
        q_3 = reshape_10.permute(0, 1, 3, 2, 4)
        reshape_10 = None
        reshape_11 = k_3.reshape(1, 64, 49, 2, 32)
        k_3 = None
        k_4 = reshape_11.permute(0, 1, 3, 2, 4)
        reshape_11 = None
        reshape_12 = v_2.reshape(1, 64, 49, 2, 32)
        v_2 = None
        v_3 = reshape_12.permute(0, 1, 3, 2, 4)
        reshape_12 = None
        k_5 = k_4 * 0.125
        k_4 = None
        dot_prod_2 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_3, k_5
        )
        q_3 = k_5 = None
        bias_index_1 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_7 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_1
        ]
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_1
        ) = None
        relative_bias_2 = getitem_7.view(49, 49, -1)
        getitem_7 = None
        permute_11 = relative_bias_2.permute(2, 0, 1)
        relative_bias_2 = None
        relative_bias_3 = permute_11.contiguous()
        permute_11 = None
        pos_bias_1 = relative_bias_3.unsqueeze(0)
        relative_bias_3 = None
        add_4 = dot_prod_2 + pos_bias_1
        dot_prod_2 = pos_bias_1 = None
        dot_prod_3 = torch.nn.functional.softmax(add_4, dim=-1)
        add_4 = None
        out_3 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_3, v_3
        )
        dot_prod_3 = v_3 = None
        permute_12 = out_3.permute(0, 1, 3, 2, 4)
        out_3 = None
        out_4 = permute_12.reshape(1, 64, 49, 64)
        permute_12 = None
        out_5 = torch._C._nn.linear(
            out_4,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_4 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_25 = torch.nn.functional.dropout(out_5, 0.0, False, False)
        out_5 = None
        _log_api_usage_once_2 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_2 = None
        x_11 = res + input_25
        res = input_25 = None
        input_26 = torch.nn.functional.layer_norm(
            x_11,
            (64,),
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_27 = torch._C._nn.linear(
            input_26,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_26 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_28 = torch._C._nn.gelu(input_27, approximate="none")
        input_27 = None
        input_29 = torch._C._nn.linear(
            input_28,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_28 = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_30 = torch.nn.functional.dropout(input_29, 0.0, False, False)
        input_29 = None
        _log_api_usage_once_3 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_3 = None
        x_12 = x_11 + input_30
        x_11 = input_30 = None
        res_1 = torch.swapaxes(x_12, -2, -3)
        x_12 = None
        x_13 = res_1.reshape(1, 7, 7, 8, 8, 64)
        res_1 = None
        x_14 = x_13.permute(0, 5, 1, 3, 2, 4)
        x_13 = None
        x_15 = x_14.reshape(1, 64, 56, 56)
        x_14 = None
        input_31 = torch.nn.functional.batch_norm(
            x_15,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_32 = torch.conv2d(
            input_31,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_31 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_33 = torch.nn.functional.batch_norm(
            input_32,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_32 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_34 = torch._C._nn.gelu(input_33, approximate="none")
        input_33 = None
        input_35 = torch.conv2d(
            input_34,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            256,
        )
        input_34 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_36 = torch.nn.functional.batch_norm(
            input_35,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_35 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_37 = torch._C._nn.gelu(input_36, approximate="none")
        input_36 = None
        scale_5 = torch.nn.functional.adaptive_avg_pool2d(input_37, 1)
        scale_6 = torch.conv2d(
            scale_5,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_5 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_7 = torch.nn.functional.silu(scale_6, inplace=False)
        scale_6 = None
        scale_8 = torch.conv2d(
            scale_7,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_7 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_9 = torch.sigmoid(scale_8)
        scale_8 = None
        input_38 = scale_9 * input_37
        scale_9 = input_37 = None
        input_39 = torch.conv2d(
            input_38,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_38 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_4 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_4 = None
        input_40 = x_15 + input_39
        x_15 = input_39 = None
        x_16 = input_40.reshape(1, 64, 8, 7, 8, 7)
        input_40 = None
        x_17 = x_16.permute(0, 2, 4, 3, 5, 1)
        x_16 = None
        x_18 = x_17.reshape(1, 64, 49, 64)
        x_17 = None
        input_41 = torch.nn.functional.layer_norm(
            x_18,
            (64,),
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_2 = torch._C._nn.linear(
            input_41,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_41 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_2 = torch.chunk(qkv_2, 3, dim=-1)
        qkv_2 = None
        q_4 = chunk_2[0]
        k_6 = chunk_2[1]
        v_4 = chunk_2[2]
        chunk_2 = None
        reshape_18 = q_4.reshape(1, 64, 49, 2, 32)
        q_4 = None
        q_5 = reshape_18.permute(0, 1, 3, 2, 4)
        reshape_18 = None
        reshape_19 = k_6.reshape(1, 64, 49, 2, 32)
        k_6 = None
        k_7 = reshape_19.permute(0, 1, 3, 2, 4)
        reshape_19 = None
        reshape_20 = v_4.reshape(1, 64, 49, 2, 32)
        v_4 = None
        v_5 = reshape_20.permute(0, 1, 3, 2, 4)
        reshape_20 = None
        k_8 = k_7 * 0.125
        k_7 = None
        dot_prod_4 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_5, k_8
        )
        q_5 = k_8 = None
        bias_index_2 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_11 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_2
        ]
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_2
        ) = None
        relative_bias_4 = getitem_11.view(49, 49, -1)
        getitem_11 = None
        permute_18 = relative_bias_4.permute(2, 0, 1)
        relative_bias_4 = None
        relative_bias_5 = permute_18.contiguous()
        permute_18 = None
        pos_bias_2 = relative_bias_5.unsqueeze(0)
        relative_bias_5 = None
        add_8 = dot_prod_4 + pos_bias_2
        dot_prod_4 = pos_bias_2 = None
        dot_prod_5 = torch.nn.functional.softmax(add_8, dim=-1)
        add_8 = None
        out_6 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_5, v_5
        )
        dot_prod_5 = v_5 = None
        permute_19 = out_6.permute(0, 1, 3, 2, 4)
        out_6 = None
        out_7 = permute_19.reshape(1, 64, 49, 64)
        permute_19 = None
        out_8 = torch._C._nn.linear(
            out_7,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_7 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_42 = torch.nn.functional.dropout(out_8, 0.0, False, False)
        out_8 = None
        _log_api_usage_once_5 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_5 = None
        x_19 = x_18 + input_42
        x_18 = input_42 = None
        input_43 = torch.nn.functional.layer_norm(
            x_19,
            (64,),
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_44 = torch._C._nn.linear(
            input_43,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_43 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_45 = torch._C._nn.gelu(input_44, approximate="none")
        input_44 = None
        input_46 = torch._C._nn.linear(
            input_45,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_45 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_47 = torch.nn.functional.dropout(input_46, 0.0, False, False)
        input_46 = None
        _log_api_usage_once_6 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_6 = None
        x_20 = x_19 + input_47
        x_19 = input_47 = None
        x_21 = x_20.reshape(1, 8, 8, 7, 7, 64)
        x_20 = None
        x_22 = x_21.permute(0, 5, 1, 3, 2, 4)
        x_21 = None
        x_23 = x_22.reshape(1, 64, 56, 56)
        x_22 = None
        x_24 = x_23.reshape(1, 64, 7, 8, 7, 8)
        x_23 = None
        x_25 = x_24.permute(0, 2, 4, 3, 5, 1)
        x_24 = None
        x_26 = x_25.reshape(1, 49, 64, 64)
        x_25 = None
        res_2 = torch.swapaxes(x_26, -2, -3)
        x_26 = None
        input_48 = torch.nn.functional.layer_norm(
            res_2,
            (64,),
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_3 = torch._C._nn.linear(
            input_48,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_48 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_3 = torch.chunk(qkv_3, 3, dim=-1)
        qkv_3 = None
        q_6 = chunk_3[0]
        k_9 = chunk_3[1]
        v_6 = chunk_3[2]
        chunk_3 = None
        reshape_26 = q_6.reshape(1, 64, 49, 2, 32)
        q_6 = None
        q_7 = reshape_26.permute(0, 1, 3, 2, 4)
        reshape_26 = None
        reshape_27 = k_9.reshape(1, 64, 49, 2, 32)
        k_9 = None
        k_10 = reshape_27.permute(0, 1, 3, 2, 4)
        reshape_27 = None
        reshape_28 = v_6.reshape(1, 64, 49, 2, 32)
        v_6 = None
        v_7 = reshape_28.permute(0, 1, 3, 2, 4)
        reshape_28 = None
        k_11 = k_10 * 0.125
        k_10 = None
        dot_prod_6 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_7, k_11
        )
        q_7 = k_11 = None
        bias_index_3 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_15 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_3
        ]
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_3
        ) = None
        relative_bias_6 = getitem_15.view(49, 49, -1)
        getitem_15 = None
        permute_25 = relative_bias_6.permute(2, 0, 1)
        relative_bias_6 = None
        relative_bias_7 = permute_25.contiguous()
        permute_25 = None
        pos_bias_3 = relative_bias_7.unsqueeze(0)
        relative_bias_7 = None
        add_11 = dot_prod_6 + pos_bias_3
        dot_prod_6 = pos_bias_3 = None
        dot_prod_7 = torch.nn.functional.softmax(add_11, dim=-1)
        add_11 = None
        out_9 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_7, v_7
        )
        dot_prod_7 = v_7 = None
        permute_26 = out_9.permute(0, 1, 3, 2, 4)
        out_9 = None
        out_10 = permute_26.reshape(1, 64, 49, 64)
        permute_26 = None
        out_11 = torch._C._nn.linear(
            out_10,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_10 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_49 = torch.nn.functional.dropout(out_11, 0.0, False, False)
        out_11 = None
        _log_api_usage_once_7 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_7 = None
        x_27 = res_2 + input_49
        res_2 = input_49 = None
        input_50 = torch.nn.functional.layer_norm(
            x_27,
            (64,),
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_51 = torch._C._nn.linear(
            input_50,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_50 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_52 = torch._C._nn.gelu(input_51, approximate="none")
        input_51 = None
        input_53 = torch._C._nn.linear(
            input_52,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_52 = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_0_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_54 = torch.nn.functional.dropout(input_53, 0.0, False, False)
        input_53 = None
        _log_api_usage_once_8 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_8 = None
        x_28 = x_27 + input_54
        x_27 = input_54 = None
        res_3 = torch.swapaxes(x_28, -2, -3)
        x_28 = None
        x_29 = res_3.reshape(1, 7, 7, 8, 8, 64)
        res_3 = None
        x_30 = x_29.permute(0, 5, 1, 3, 2, 4)
        x_29 = None
        x_31 = x_30.reshape(1, 64, 56, 56)
        x_30 = None
        input_55 = torch._C._nn.avg_pool2d(x_31, 3, 2, 1, False, True, None)
        input_56 = torch.conv2d(
            input_55,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_55 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_ = (None)
        input_57 = torch.nn.functional.batch_norm(
            x_31,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        x_31 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_58 = torch.conv2d(
            input_57,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_57 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_59 = torch.nn.functional.batch_norm(
            input_58,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_58 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_60 = torch._C._nn.gelu(input_59, approximate="none")
        input_59 = None
        input_61 = torch.conv2d(
            input_60,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            512,
        )
        input_60 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_62 = torch.nn.functional.batch_norm(
            input_61,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_61 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_63 = torch._C._nn.gelu(input_62, approximate="none")
        input_62 = None
        scale_10 = torch.nn.functional.adaptive_avg_pool2d(input_63, 1)
        scale_11 = torch.conv2d(
            scale_10,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_10 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_12 = torch.nn.functional.silu(scale_11, inplace=False)
        scale_11 = None
        scale_13 = torch.conv2d(
            scale_12,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_12 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_14 = torch.sigmoid(scale_13)
        scale_13 = None
        input_64 = scale_14 * input_63
        scale_14 = input_63 = None
        input_65 = torch.conv2d(
            input_64,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_64 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_9 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_9 = None
        input_66 = input_56 + input_65
        input_56 = input_65 = None
        x_32 = input_66.reshape(1, 128, 4, 7, 4, 7)
        input_66 = None
        x_33 = x_32.permute(0, 2, 4, 3, 5, 1)
        x_32 = None
        x_34 = x_33.reshape(1, 16, 49, 128)
        x_33 = None
        input_67 = torch.nn.functional.layer_norm(
            x_34,
            (128,),
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_4 = torch._C._nn.linear(
            input_67,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_67 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_4 = torch.chunk(qkv_4, 3, dim=-1)
        qkv_4 = None
        q_8 = chunk_4[0]
        k_12 = chunk_4[1]
        v_8 = chunk_4[2]
        chunk_4 = None
        reshape_34 = q_8.reshape(1, 16, 49, 4, 32)
        q_8 = None
        q_9 = reshape_34.permute(0, 1, 3, 2, 4)
        reshape_34 = None
        reshape_35 = k_12.reshape(1, 16, 49, 4, 32)
        k_12 = None
        k_13 = reshape_35.permute(0, 1, 3, 2, 4)
        reshape_35 = None
        reshape_36 = v_8.reshape(1, 16, 49, 4, 32)
        v_8 = None
        v_9 = reshape_36.permute(0, 1, 3, 2, 4)
        reshape_36 = None
        k_14 = k_13 * 0.08838834764831845
        k_13 = None
        dot_prod_8 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_9, k_14
        )
        q_9 = k_14 = None
        bias_index_4 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_19 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_4
        ]
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_4
        ) = None
        relative_bias_8 = getitem_19.view(49, 49, -1)
        getitem_19 = None
        permute_32 = relative_bias_8.permute(2, 0, 1)
        relative_bias_8 = None
        relative_bias_9 = permute_32.contiguous()
        permute_32 = None
        pos_bias_4 = relative_bias_9.unsqueeze(0)
        relative_bias_9 = None
        add_15 = dot_prod_8 + pos_bias_4
        dot_prod_8 = pos_bias_4 = None
        dot_prod_9 = torch.nn.functional.softmax(add_15, dim=-1)
        add_15 = None
        out_12 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_9, v_9
        )
        dot_prod_9 = v_9 = None
        permute_33 = out_12.permute(0, 1, 3, 2, 4)
        out_12 = None
        out_13 = permute_33.reshape(1, 16, 49, 128)
        permute_33 = None
        out_14 = torch._C._nn.linear(
            out_13,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_13 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_68 = torch.nn.functional.dropout(out_14, 0.0, False, False)
        out_14 = None
        _log_api_usage_once_10 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_10 = None
        x_35 = x_34 + input_68
        x_34 = input_68 = None
        input_69 = torch.nn.functional.layer_norm(
            x_35,
            (128,),
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_70 = torch._C._nn.linear(
            input_69,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_69 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_71 = torch._C._nn.gelu(input_70, approximate="none")
        input_70 = None
        input_72 = torch._C._nn.linear(
            input_71,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_71 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_73 = torch.nn.functional.dropout(input_72, 0.0, False, False)
        input_72 = None
        _log_api_usage_once_11 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_11 = None
        x_36 = x_35 + input_73
        x_35 = input_73 = None
        x_37 = x_36.reshape(1, 4, 4, 7, 7, 128)
        x_36 = None
        x_38 = x_37.permute(0, 5, 1, 3, 2, 4)
        x_37 = None
        x_39 = x_38.reshape(1, 128, 28, 28)
        x_38 = None
        x_40 = x_39.reshape(1, 128, 7, 4, 7, 4)
        x_39 = None
        x_41 = x_40.permute(0, 2, 4, 3, 5, 1)
        x_40 = None
        x_42 = x_41.reshape(1, 49, 16, 128)
        x_41 = None
        res_4 = torch.swapaxes(x_42, -2, -3)
        x_42 = None
        input_74 = torch.nn.functional.layer_norm(
            res_4,
            (128,),
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_5 = torch._C._nn.linear(
            input_74,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_74 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_5 = torch.chunk(qkv_5, 3, dim=-1)
        qkv_5 = None
        q_10 = chunk_5[0]
        k_15 = chunk_5[1]
        v_10 = chunk_5[2]
        chunk_5 = None
        reshape_42 = q_10.reshape(1, 16, 49, 4, 32)
        q_10 = None
        q_11 = reshape_42.permute(0, 1, 3, 2, 4)
        reshape_42 = None
        reshape_43 = k_15.reshape(1, 16, 49, 4, 32)
        k_15 = None
        k_16 = reshape_43.permute(0, 1, 3, 2, 4)
        reshape_43 = None
        reshape_44 = v_10.reshape(1, 16, 49, 4, 32)
        v_10 = None
        v_11 = reshape_44.permute(0, 1, 3, 2, 4)
        reshape_44 = None
        k_17 = k_16 * 0.08838834764831845
        k_16 = None
        dot_prod_10 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_11, k_17
        )
        q_11 = k_17 = None
        bias_index_5 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_23 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_5
        ]
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_5
        ) = None
        relative_bias_10 = getitem_23.view(49, 49, -1)
        getitem_23 = None
        permute_39 = relative_bias_10.permute(2, 0, 1)
        relative_bias_10 = None
        relative_bias_11 = permute_39.contiguous()
        permute_39 = None
        pos_bias_5 = relative_bias_11.unsqueeze(0)
        relative_bias_11 = None
        add_18 = dot_prod_10 + pos_bias_5
        dot_prod_10 = pos_bias_5 = None
        dot_prod_11 = torch.nn.functional.softmax(add_18, dim=-1)
        add_18 = None
        out_15 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_11, v_11
        )
        dot_prod_11 = v_11 = None
        permute_40 = out_15.permute(0, 1, 3, 2, 4)
        out_15 = None
        out_16 = permute_40.reshape(1, 16, 49, 128)
        permute_40 = None
        out_17 = torch._C._nn.linear(
            out_16,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_16 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_75 = torch.nn.functional.dropout(out_17, 0.0, False, False)
        out_17 = None
        _log_api_usage_once_12 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_12 = None
        x_43 = res_4 + input_75
        res_4 = input_75 = None
        input_76 = torch.nn.functional.layer_norm(
            x_43,
            (128,),
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_77 = torch._C._nn.linear(
            input_76,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_76 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_78 = torch._C._nn.gelu(input_77, approximate="none")
        input_77 = None
        input_79 = torch._C._nn.linear(
            input_78,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_78 = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_80 = torch.nn.functional.dropout(input_79, 0.0, False, False)
        input_79 = None
        _log_api_usage_once_13 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_13 = None
        x_44 = x_43 + input_80
        x_43 = input_80 = None
        res_5 = torch.swapaxes(x_44, -2, -3)
        x_44 = None
        x_45 = res_5.reshape(1, 7, 7, 4, 4, 128)
        res_5 = None
        x_46 = x_45.permute(0, 5, 1, 3, 2, 4)
        x_45 = None
        x_47 = x_46.reshape(1, 128, 28, 28)
        x_46 = None
        input_81 = torch.nn.functional.batch_norm(
            x_47,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_82 = torch.conv2d(
            input_81,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_81 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_83 = torch.nn.functional.batch_norm(
            input_82,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_82 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_84 = torch._C._nn.gelu(input_83, approximate="none")
        input_83 = None
        input_85 = torch.conv2d(
            input_84,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            512,
        )
        input_84 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_86 = torch.nn.functional.batch_norm(
            input_85,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_85 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_87 = torch._C._nn.gelu(input_86, approximate="none")
        input_86 = None
        scale_15 = torch.nn.functional.adaptive_avg_pool2d(input_87, 1)
        scale_16 = torch.conv2d(
            scale_15,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_15 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_17 = torch.nn.functional.silu(scale_16, inplace=False)
        scale_16 = None
        scale_18 = torch.conv2d(
            scale_17,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_17 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_19 = torch.sigmoid(scale_18)
        scale_18 = None
        input_88 = scale_19 * input_87
        scale_19 = input_87 = None
        input_89 = torch.conv2d(
            input_88,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_88 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_14 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_14 = None
        input_90 = x_47 + input_89
        x_47 = input_89 = None
        x_48 = input_90.reshape(1, 128, 4, 7, 4, 7)
        input_90 = None
        x_49 = x_48.permute(0, 2, 4, 3, 5, 1)
        x_48 = None
        x_50 = x_49.reshape(1, 16, 49, 128)
        x_49 = None
        input_91 = torch.nn.functional.layer_norm(
            x_50,
            (128,),
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_6 = torch._C._nn.linear(
            input_91,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_91 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_6 = torch.chunk(qkv_6, 3, dim=-1)
        qkv_6 = None
        q_12 = chunk_6[0]
        k_18 = chunk_6[1]
        v_12 = chunk_6[2]
        chunk_6 = None
        reshape_50 = q_12.reshape(1, 16, 49, 4, 32)
        q_12 = None
        q_13 = reshape_50.permute(0, 1, 3, 2, 4)
        reshape_50 = None
        reshape_51 = k_18.reshape(1, 16, 49, 4, 32)
        k_18 = None
        k_19 = reshape_51.permute(0, 1, 3, 2, 4)
        reshape_51 = None
        reshape_52 = v_12.reshape(1, 16, 49, 4, 32)
        v_12 = None
        v_13 = reshape_52.permute(0, 1, 3, 2, 4)
        reshape_52 = None
        k_20 = k_19 * 0.08838834764831845
        k_19 = None
        dot_prod_12 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_13, k_20
        )
        q_13 = k_20 = None
        bias_index_6 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_27 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_6
        ]
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_6
        ) = None
        relative_bias_12 = getitem_27.view(49, 49, -1)
        getitem_27 = None
        permute_46 = relative_bias_12.permute(2, 0, 1)
        relative_bias_12 = None
        relative_bias_13 = permute_46.contiguous()
        permute_46 = None
        pos_bias_6 = relative_bias_13.unsqueeze(0)
        relative_bias_13 = None
        add_22 = dot_prod_12 + pos_bias_6
        dot_prod_12 = pos_bias_6 = None
        dot_prod_13 = torch.nn.functional.softmax(add_22, dim=-1)
        add_22 = None
        out_18 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_13, v_13
        )
        dot_prod_13 = v_13 = None
        permute_47 = out_18.permute(0, 1, 3, 2, 4)
        out_18 = None
        out_19 = permute_47.reshape(1, 16, 49, 128)
        permute_47 = None
        out_20 = torch._C._nn.linear(
            out_19,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_19 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_92 = torch.nn.functional.dropout(out_20, 0.0, False, False)
        out_20 = None
        _log_api_usage_once_15 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_15 = None
        x_51 = x_50 + input_92
        x_50 = input_92 = None
        input_93 = torch.nn.functional.layer_norm(
            x_51,
            (128,),
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_94 = torch._C._nn.linear(
            input_93,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_93 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_95 = torch._C._nn.gelu(input_94, approximate="none")
        input_94 = None
        input_96 = torch._C._nn.linear(
            input_95,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_95 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_97 = torch.nn.functional.dropout(input_96, 0.0, False, False)
        input_96 = None
        _log_api_usage_once_16 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_16 = None
        x_52 = x_51 + input_97
        x_51 = input_97 = None
        x_53 = x_52.reshape(1, 4, 4, 7, 7, 128)
        x_52 = None
        x_54 = x_53.permute(0, 5, 1, 3, 2, 4)
        x_53 = None
        x_55 = x_54.reshape(1, 128, 28, 28)
        x_54 = None
        x_56 = x_55.reshape(1, 128, 7, 4, 7, 4)
        x_55 = None
        x_57 = x_56.permute(0, 2, 4, 3, 5, 1)
        x_56 = None
        x_58 = x_57.reshape(1, 49, 16, 128)
        x_57 = None
        res_6 = torch.swapaxes(x_58, -2, -3)
        x_58 = None
        input_98 = torch.nn.functional.layer_norm(
            res_6,
            (128,),
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_7 = torch._C._nn.linear(
            input_98,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_98 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_7 = torch.chunk(qkv_7, 3, dim=-1)
        qkv_7 = None
        q_14 = chunk_7[0]
        k_21 = chunk_7[1]
        v_14 = chunk_7[2]
        chunk_7 = None
        reshape_58 = q_14.reshape(1, 16, 49, 4, 32)
        q_14 = None
        q_15 = reshape_58.permute(0, 1, 3, 2, 4)
        reshape_58 = None
        reshape_59 = k_21.reshape(1, 16, 49, 4, 32)
        k_21 = None
        k_22 = reshape_59.permute(0, 1, 3, 2, 4)
        reshape_59 = None
        reshape_60 = v_14.reshape(1, 16, 49, 4, 32)
        v_14 = None
        v_15 = reshape_60.permute(0, 1, 3, 2, 4)
        reshape_60 = None
        k_23 = k_22 * 0.08838834764831845
        k_22 = None
        dot_prod_14 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_15, k_23
        )
        q_15 = k_23 = None
        bias_index_7 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_31 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_7
        ]
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_7
        ) = None
        relative_bias_14 = getitem_31.view(49, 49, -1)
        getitem_31 = None
        permute_53 = relative_bias_14.permute(2, 0, 1)
        relative_bias_14 = None
        relative_bias_15 = permute_53.contiguous()
        permute_53 = None
        pos_bias_7 = relative_bias_15.unsqueeze(0)
        relative_bias_15 = None
        add_25 = dot_prod_14 + pos_bias_7
        dot_prod_14 = pos_bias_7 = None
        dot_prod_15 = torch.nn.functional.softmax(add_25, dim=-1)
        add_25 = None
        out_21 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_15, v_15
        )
        dot_prod_15 = v_15 = None
        permute_54 = out_21.permute(0, 1, 3, 2, 4)
        out_21 = None
        out_22 = permute_54.reshape(1, 16, 49, 128)
        permute_54 = None
        out_23 = torch._C._nn.linear(
            out_22,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_22 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_99 = torch.nn.functional.dropout(out_23, 0.0, False, False)
        out_23 = None
        _log_api_usage_once_17 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_17 = None
        x_59 = res_6 + input_99
        res_6 = input_99 = None
        input_100 = torch.nn.functional.layer_norm(
            x_59,
            (128,),
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_101 = torch._C._nn.linear(
            input_100,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_100 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_102 = torch._C._nn.gelu(input_101, approximate="none")
        input_101 = None
        input_103 = torch._C._nn.linear(
            input_102,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_102 = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_1_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_104 = torch.nn.functional.dropout(input_103, 0.0, False, False)
        input_103 = None
        _log_api_usage_once_18 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_18 = None
        x_60 = x_59 + input_104
        x_59 = input_104 = None
        res_7 = torch.swapaxes(x_60, -2, -3)
        x_60 = None
        x_61 = res_7.reshape(1, 7, 7, 4, 4, 128)
        res_7 = None
        x_62 = x_61.permute(0, 5, 1, 3, 2, 4)
        x_61 = None
        x_63 = x_62.reshape(1, 128, 28, 28)
        x_62 = None
        input_105 = torch._C._nn.avg_pool2d(x_63, 3, 2, 1, False, True, None)
        input_106 = torch.conv2d(
            input_105,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_105 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_ = (None)
        input_107 = torch.nn.functional.batch_norm(
            x_63,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        x_63 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_108 = torch.conv2d(
            input_107,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_107 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_109 = torch.nn.functional.batch_norm(
            input_108,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_108 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_110 = torch._C._nn.gelu(input_109, approximate="none")
        input_109 = None
        input_111 = torch.conv2d(
            input_110,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            1024,
        )
        input_110 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_112 = torch.nn.functional.batch_norm(
            input_111,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_111 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_113 = torch._C._nn.gelu(input_112, approximate="none")
        input_112 = None
        scale_20 = torch.nn.functional.adaptive_avg_pool2d(input_113, 1)
        scale_21 = torch.conv2d(
            scale_20,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_20 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_22 = torch.nn.functional.silu(scale_21, inplace=False)
        scale_21 = None
        scale_23 = torch.conv2d(
            scale_22,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_22 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_24 = torch.sigmoid(scale_23)
        scale_23 = None
        input_114 = scale_24 * input_113
        scale_24 = input_113 = None
        input_115 = torch.conv2d(
            input_114,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_114 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_19 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_19 = None
        input_116 = input_106 + input_115
        input_106 = input_115 = None
        x_64 = input_116.reshape(1, 256, 2, 7, 2, 7)
        input_116 = None
        x_65 = x_64.permute(0, 2, 4, 3, 5, 1)
        x_64 = None
        x_66 = x_65.reshape(1, 4, 49, 256)
        x_65 = None
        input_117 = torch.nn.functional.layer_norm(
            x_66,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_8 = torch._C._nn.linear(
            input_117,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_117 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_8 = torch.chunk(qkv_8, 3, dim=-1)
        qkv_8 = None
        q_16 = chunk_8[0]
        k_24 = chunk_8[1]
        v_16 = chunk_8[2]
        chunk_8 = None
        reshape_66 = q_16.reshape(1, 4, 49, 8, 32)
        q_16 = None
        q_17 = reshape_66.permute(0, 1, 3, 2, 4)
        reshape_66 = None
        reshape_67 = k_24.reshape(1, 4, 49, 8, 32)
        k_24 = None
        k_25 = reshape_67.permute(0, 1, 3, 2, 4)
        reshape_67 = None
        reshape_68 = v_16.reshape(1, 4, 49, 8, 32)
        v_16 = None
        v_17 = reshape_68.permute(0, 1, 3, 2, 4)
        reshape_68 = None
        k_26 = k_25 * 0.0625
        k_25 = None
        dot_prod_16 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_17, k_26
        )
        q_17 = k_26 = None
        bias_index_8 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_35 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_8
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_8
        ) = None
        relative_bias_16 = getitem_35.view(49, 49, -1)
        getitem_35 = None
        permute_60 = relative_bias_16.permute(2, 0, 1)
        relative_bias_16 = None
        relative_bias_17 = permute_60.contiguous()
        permute_60 = None
        pos_bias_8 = relative_bias_17.unsqueeze(0)
        relative_bias_17 = None
        add_29 = dot_prod_16 + pos_bias_8
        dot_prod_16 = pos_bias_8 = None
        dot_prod_17 = torch.nn.functional.softmax(add_29, dim=-1)
        add_29 = None
        out_24 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_17, v_17
        )
        dot_prod_17 = v_17 = None
        permute_61 = out_24.permute(0, 1, 3, 2, 4)
        out_24 = None
        out_25 = permute_61.reshape(1, 4, 49, 256)
        permute_61 = None
        out_26 = torch._C._nn.linear(
            out_25,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_25 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_118 = torch.nn.functional.dropout(out_26, 0.0, False, False)
        out_26 = None
        _log_api_usage_once_20 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_20 = None
        x_67 = x_66 + input_118
        x_66 = input_118 = None
        input_119 = torch.nn.functional.layer_norm(
            x_67,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_120 = torch._C._nn.linear(
            input_119,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_119 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_121 = torch._C._nn.gelu(input_120, approximate="none")
        input_120 = None
        input_122 = torch._C._nn.linear(
            input_121,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_121 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_123 = torch.nn.functional.dropout(input_122, 0.0, False, False)
        input_122 = None
        _log_api_usage_once_21 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_21 = None
        x_68 = x_67 + input_123
        x_67 = input_123 = None
        x_69 = x_68.reshape(1, 2, 2, 7, 7, 256)
        x_68 = None
        x_70 = x_69.permute(0, 5, 1, 3, 2, 4)
        x_69 = None
        x_71 = x_70.reshape(1, 256, 14, 14)
        x_70 = None
        x_72 = x_71.reshape(1, 256, 7, 2, 7, 2)
        x_71 = None
        x_73 = x_72.permute(0, 2, 4, 3, 5, 1)
        x_72 = None
        x_74 = x_73.reshape(1, 49, 4, 256)
        x_73 = None
        res_8 = torch.swapaxes(x_74, -2, -3)
        x_74 = None
        input_124 = torch.nn.functional.layer_norm(
            res_8,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_9 = torch._C._nn.linear(
            input_124,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_124 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_9 = torch.chunk(qkv_9, 3, dim=-1)
        qkv_9 = None
        q_18 = chunk_9[0]
        k_27 = chunk_9[1]
        v_18 = chunk_9[2]
        chunk_9 = None
        reshape_74 = q_18.reshape(1, 4, 49, 8, 32)
        q_18 = None
        q_19 = reshape_74.permute(0, 1, 3, 2, 4)
        reshape_74 = None
        reshape_75 = k_27.reshape(1, 4, 49, 8, 32)
        k_27 = None
        k_28 = reshape_75.permute(0, 1, 3, 2, 4)
        reshape_75 = None
        reshape_76 = v_18.reshape(1, 4, 49, 8, 32)
        v_18 = None
        v_19 = reshape_76.permute(0, 1, 3, 2, 4)
        reshape_76 = None
        k_29 = k_28 * 0.0625
        k_28 = None
        dot_prod_18 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_19, k_29
        )
        q_19 = k_29 = None
        bias_index_9 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_39 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_9
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_9
        ) = None
        relative_bias_18 = getitem_39.view(49, 49, -1)
        getitem_39 = None
        permute_67 = relative_bias_18.permute(2, 0, 1)
        relative_bias_18 = None
        relative_bias_19 = permute_67.contiguous()
        permute_67 = None
        pos_bias_9 = relative_bias_19.unsqueeze(0)
        relative_bias_19 = None
        add_32 = dot_prod_18 + pos_bias_9
        dot_prod_18 = pos_bias_9 = None
        dot_prod_19 = torch.nn.functional.softmax(add_32, dim=-1)
        add_32 = None
        out_27 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_19, v_19
        )
        dot_prod_19 = v_19 = None
        permute_68 = out_27.permute(0, 1, 3, 2, 4)
        out_27 = None
        out_28 = permute_68.reshape(1, 4, 49, 256)
        permute_68 = None
        out_29 = torch._C._nn.linear(
            out_28,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_28 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_125 = torch.nn.functional.dropout(out_29, 0.0, False, False)
        out_29 = None
        _log_api_usage_once_22 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_22 = None
        x_75 = res_8 + input_125
        res_8 = input_125 = None
        input_126 = torch.nn.functional.layer_norm(
            x_75,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_127 = torch._C._nn.linear(
            input_126,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_126 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_128 = torch._C._nn.gelu(input_127, approximate="none")
        input_127 = None
        input_129 = torch._C._nn.linear(
            input_128,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_128 = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_130 = torch.nn.functional.dropout(input_129, 0.0, False, False)
        input_129 = None
        _log_api_usage_once_23 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_23 = None
        x_76 = x_75 + input_130
        x_75 = input_130 = None
        res_9 = torch.swapaxes(x_76, -2, -3)
        x_76 = None
        x_77 = res_9.reshape(1, 7, 7, 2, 2, 256)
        res_9 = None
        x_78 = x_77.permute(0, 5, 1, 3, 2, 4)
        x_77 = None
        x_79 = x_78.reshape(1, 256, 14, 14)
        x_78 = None
        input_131 = torch.nn.functional.batch_norm(
            x_79,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_132 = torch.conv2d(
            input_131,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_131 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_133 = torch.nn.functional.batch_norm(
            input_132,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_132 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_134 = torch._C._nn.gelu(input_133, approximate="none")
        input_133 = None
        input_135 = torch.conv2d(
            input_134,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        input_134 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_136 = torch.nn.functional.batch_norm(
            input_135,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_135 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_137 = torch._C._nn.gelu(input_136, approximate="none")
        input_136 = None
        scale_25 = torch.nn.functional.adaptive_avg_pool2d(input_137, 1)
        scale_26 = torch.conv2d(
            scale_25,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_25 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_27 = torch.nn.functional.silu(scale_26, inplace=False)
        scale_26 = None
        scale_28 = torch.conv2d(
            scale_27,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_27 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_29 = torch.sigmoid(scale_28)
        scale_28 = None
        input_138 = scale_29 * input_137
        scale_29 = input_137 = None
        input_139 = torch.conv2d(
            input_138,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_138 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_24 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_24 = None
        input_140 = x_79 + input_139
        x_79 = input_139 = None
        x_80 = input_140.reshape(1, 256, 2, 7, 2, 7)
        input_140 = None
        x_81 = x_80.permute(0, 2, 4, 3, 5, 1)
        x_80 = None
        x_82 = x_81.reshape(1, 4, 49, 256)
        x_81 = None
        input_141 = torch.nn.functional.layer_norm(
            x_82,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_10 = torch._C._nn.linear(
            input_141,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_141 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_10 = torch.chunk(qkv_10, 3, dim=-1)
        qkv_10 = None
        q_20 = chunk_10[0]
        k_30 = chunk_10[1]
        v_20 = chunk_10[2]
        chunk_10 = None
        reshape_82 = q_20.reshape(1, 4, 49, 8, 32)
        q_20 = None
        q_21 = reshape_82.permute(0, 1, 3, 2, 4)
        reshape_82 = None
        reshape_83 = k_30.reshape(1, 4, 49, 8, 32)
        k_30 = None
        k_31 = reshape_83.permute(0, 1, 3, 2, 4)
        reshape_83 = None
        reshape_84 = v_20.reshape(1, 4, 49, 8, 32)
        v_20 = None
        v_21 = reshape_84.permute(0, 1, 3, 2, 4)
        reshape_84 = None
        k_32 = k_31 * 0.0625
        k_31 = None
        dot_prod_20 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_21, k_32
        )
        q_21 = k_32 = None
        bias_index_10 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_43 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_10
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_10
        ) = None
        relative_bias_20 = getitem_43.view(49, 49, -1)
        getitem_43 = None
        permute_74 = relative_bias_20.permute(2, 0, 1)
        relative_bias_20 = None
        relative_bias_21 = permute_74.contiguous()
        permute_74 = None
        pos_bias_10 = relative_bias_21.unsqueeze(0)
        relative_bias_21 = None
        add_36 = dot_prod_20 + pos_bias_10
        dot_prod_20 = pos_bias_10 = None
        dot_prod_21 = torch.nn.functional.softmax(add_36, dim=-1)
        add_36 = None
        out_30 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_21, v_21
        )
        dot_prod_21 = v_21 = None
        permute_75 = out_30.permute(0, 1, 3, 2, 4)
        out_30 = None
        out_31 = permute_75.reshape(1, 4, 49, 256)
        permute_75 = None
        out_32 = torch._C._nn.linear(
            out_31,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_31 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_142 = torch.nn.functional.dropout(out_32, 0.0, False, False)
        out_32 = None
        _log_api_usage_once_25 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_25 = None
        x_83 = x_82 + input_142
        x_82 = input_142 = None
        input_143 = torch.nn.functional.layer_norm(
            x_83,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_144 = torch._C._nn.linear(
            input_143,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_143 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_145 = torch._C._nn.gelu(input_144, approximate="none")
        input_144 = None
        input_146 = torch._C._nn.linear(
            input_145,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_145 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_147 = torch.nn.functional.dropout(input_146, 0.0, False, False)
        input_146 = None
        _log_api_usage_once_26 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_26 = None
        x_84 = x_83 + input_147
        x_83 = input_147 = None
        x_85 = x_84.reshape(1, 2, 2, 7, 7, 256)
        x_84 = None
        x_86 = x_85.permute(0, 5, 1, 3, 2, 4)
        x_85 = None
        x_87 = x_86.reshape(1, 256, 14, 14)
        x_86 = None
        x_88 = x_87.reshape(1, 256, 7, 2, 7, 2)
        x_87 = None
        x_89 = x_88.permute(0, 2, 4, 3, 5, 1)
        x_88 = None
        x_90 = x_89.reshape(1, 49, 4, 256)
        x_89 = None
        res_10 = torch.swapaxes(x_90, -2, -3)
        x_90 = None
        input_148 = torch.nn.functional.layer_norm(
            res_10,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_11 = torch._C._nn.linear(
            input_148,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_148 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_11 = torch.chunk(qkv_11, 3, dim=-1)
        qkv_11 = None
        q_22 = chunk_11[0]
        k_33 = chunk_11[1]
        v_22 = chunk_11[2]
        chunk_11 = None
        reshape_90 = q_22.reshape(1, 4, 49, 8, 32)
        q_22 = None
        q_23 = reshape_90.permute(0, 1, 3, 2, 4)
        reshape_90 = None
        reshape_91 = k_33.reshape(1, 4, 49, 8, 32)
        k_33 = None
        k_34 = reshape_91.permute(0, 1, 3, 2, 4)
        reshape_91 = None
        reshape_92 = v_22.reshape(1, 4, 49, 8, 32)
        v_22 = None
        v_23 = reshape_92.permute(0, 1, 3, 2, 4)
        reshape_92 = None
        k_35 = k_34 * 0.0625
        k_34 = None
        dot_prod_22 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_23, k_35
        )
        q_23 = k_35 = None
        bias_index_11 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_47 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_11
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_11
        ) = None
        relative_bias_22 = getitem_47.view(49, 49, -1)
        getitem_47 = None
        permute_81 = relative_bias_22.permute(2, 0, 1)
        relative_bias_22 = None
        relative_bias_23 = permute_81.contiguous()
        permute_81 = None
        pos_bias_11 = relative_bias_23.unsqueeze(0)
        relative_bias_23 = None
        add_39 = dot_prod_22 + pos_bias_11
        dot_prod_22 = pos_bias_11 = None
        dot_prod_23 = torch.nn.functional.softmax(add_39, dim=-1)
        add_39 = None
        out_33 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_23, v_23
        )
        dot_prod_23 = v_23 = None
        permute_82 = out_33.permute(0, 1, 3, 2, 4)
        out_33 = None
        out_34 = permute_82.reshape(1, 4, 49, 256)
        permute_82 = None
        out_35 = torch._C._nn.linear(
            out_34,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_34 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_149 = torch.nn.functional.dropout(out_35, 0.0, False, False)
        out_35 = None
        _log_api_usage_once_27 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_27 = None
        x_91 = res_10 + input_149
        res_10 = input_149 = None
        input_150 = torch.nn.functional.layer_norm(
            x_91,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_151 = torch._C._nn.linear(
            input_150,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_150 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_152 = torch._C._nn.gelu(input_151, approximate="none")
        input_151 = None
        input_153 = torch._C._nn.linear(
            input_152,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_152 = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_154 = torch.nn.functional.dropout(input_153, 0.0, False, False)
        input_153 = None
        _log_api_usage_once_28 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_28 = None
        x_92 = x_91 + input_154
        x_91 = input_154 = None
        res_11 = torch.swapaxes(x_92, -2, -3)
        x_92 = None
        x_93 = res_11.reshape(1, 7, 7, 2, 2, 256)
        res_11 = None
        x_94 = x_93.permute(0, 5, 1, 3, 2, 4)
        x_93 = None
        x_95 = x_94.reshape(1, 256, 14, 14)
        x_94 = None
        input_155 = torch.nn.functional.batch_norm(
            x_95,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_156 = torch.conv2d(
            input_155,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_155 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_157 = torch.nn.functional.batch_norm(
            input_156,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_156 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_158 = torch._C._nn.gelu(input_157, approximate="none")
        input_157 = None
        input_159 = torch.conv2d(
            input_158,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        input_158 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_160 = torch.nn.functional.batch_norm(
            input_159,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_159 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_161 = torch._C._nn.gelu(input_160, approximate="none")
        input_160 = None
        scale_30 = torch.nn.functional.adaptive_avg_pool2d(input_161, 1)
        scale_31 = torch.conv2d(
            scale_30,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_30 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_32 = torch.nn.functional.silu(scale_31, inplace=False)
        scale_31 = None
        scale_33 = torch.conv2d(
            scale_32,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_32 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_34 = torch.sigmoid(scale_33)
        scale_33 = None
        input_162 = scale_34 * input_161
        scale_34 = input_161 = None
        input_163 = torch.conv2d(
            input_162,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_162 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_29 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_29 = None
        input_164 = x_95 + input_163
        x_95 = input_163 = None
        x_96 = input_164.reshape(1, 256, 2, 7, 2, 7)
        input_164 = None
        x_97 = x_96.permute(0, 2, 4, 3, 5, 1)
        x_96 = None
        x_98 = x_97.reshape(1, 4, 49, 256)
        x_97 = None
        input_165 = torch.nn.functional.layer_norm(
            x_98,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_12 = torch._C._nn.linear(
            input_165,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_165 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_12 = torch.chunk(qkv_12, 3, dim=-1)
        qkv_12 = None
        q_24 = chunk_12[0]
        k_36 = chunk_12[1]
        v_24 = chunk_12[2]
        chunk_12 = None
        reshape_98 = q_24.reshape(1, 4, 49, 8, 32)
        q_24 = None
        q_25 = reshape_98.permute(0, 1, 3, 2, 4)
        reshape_98 = None
        reshape_99 = k_36.reshape(1, 4, 49, 8, 32)
        k_36 = None
        k_37 = reshape_99.permute(0, 1, 3, 2, 4)
        reshape_99 = None
        reshape_100 = v_24.reshape(1, 4, 49, 8, 32)
        v_24 = None
        v_25 = reshape_100.permute(0, 1, 3, 2, 4)
        reshape_100 = None
        k_38 = k_37 * 0.0625
        k_37 = None
        dot_prod_24 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_25, k_38
        )
        q_25 = k_38 = None
        bias_index_12 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_51 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_12
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_12
        ) = None
        relative_bias_24 = getitem_51.view(49, 49, -1)
        getitem_51 = None
        permute_88 = relative_bias_24.permute(2, 0, 1)
        relative_bias_24 = None
        relative_bias_25 = permute_88.contiguous()
        permute_88 = None
        pos_bias_12 = relative_bias_25.unsqueeze(0)
        relative_bias_25 = None
        add_43 = dot_prod_24 + pos_bias_12
        dot_prod_24 = pos_bias_12 = None
        dot_prod_25 = torch.nn.functional.softmax(add_43, dim=-1)
        add_43 = None
        out_36 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_25, v_25
        )
        dot_prod_25 = v_25 = None
        permute_89 = out_36.permute(0, 1, 3, 2, 4)
        out_36 = None
        out_37 = permute_89.reshape(1, 4, 49, 256)
        permute_89 = None
        out_38 = torch._C._nn.linear(
            out_37,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_37 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_166 = torch.nn.functional.dropout(out_38, 0.0, False, False)
        out_38 = None
        _log_api_usage_once_30 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_30 = None
        x_99 = x_98 + input_166
        x_98 = input_166 = None
        input_167 = torch.nn.functional.layer_norm(
            x_99,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_168 = torch._C._nn.linear(
            input_167,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_167 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_169 = torch._C._nn.gelu(input_168, approximate="none")
        input_168 = None
        input_170 = torch._C._nn.linear(
            input_169,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_169 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_171 = torch.nn.functional.dropout(input_170, 0.0, False, False)
        input_170 = None
        _log_api_usage_once_31 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_31 = None
        x_100 = x_99 + input_171
        x_99 = input_171 = None
        x_101 = x_100.reshape(1, 2, 2, 7, 7, 256)
        x_100 = None
        x_102 = x_101.permute(0, 5, 1, 3, 2, 4)
        x_101 = None
        x_103 = x_102.reshape(1, 256, 14, 14)
        x_102 = None
        x_104 = x_103.reshape(1, 256, 7, 2, 7, 2)
        x_103 = None
        x_105 = x_104.permute(0, 2, 4, 3, 5, 1)
        x_104 = None
        x_106 = x_105.reshape(1, 49, 4, 256)
        x_105 = None
        res_12 = torch.swapaxes(x_106, -2, -3)
        x_106 = None
        input_172 = torch.nn.functional.layer_norm(
            res_12,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_13 = torch._C._nn.linear(
            input_172,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_172 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_13 = torch.chunk(qkv_13, 3, dim=-1)
        qkv_13 = None
        q_26 = chunk_13[0]
        k_39 = chunk_13[1]
        v_26 = chunk_13[2]
        chunk_13 = None
        reshape_106 = q_26.reshape(1, 4, 49, 8, 32)
        q_26 = None
        q_27 = reshape_106.permute(0, 1, 3, 2, 4)
        reshape_106 = None
        reshape_107 = k_39.reshape(1, 4, 49, 8, 32)
        k_39 = None
        k_40 = reshape_107.permute(0, 1, 3, 2, 4)
        reshape_107 = None
        reshape_108 = v_26.reshape(1, 4, 49, 8, 32)
        v_26 = None
        v_27 = reshape_108.permute(0, 1, 3, 2, 4)
        reshape_108 = None
        k_41 = k_40 * 0.0625
        k_40 = None
        dot_prod_26 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_27, k_41
        )
        q_27 = k_41 = None
        bias_index_13 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_55 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_13
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_13
        ) = None
        relative_bias_26 = getitem_55.view(49, 49, -1)
        getitem_55 = None
        permute_95 = relative_bias_26.permute(2, 0, 1)
        relative_bias_26 = None
        relative_bias_27 = permute_95.contiguous()
        permute_95 = None
        pos_bias_13 = relative_bias_27.unsqueeze(0)
        relative_bias_27 = None
        add_46 = dot_prod_26 + pos_bias_13
        dot_prod_26 = pos_bias_13 = None
        dot_prod_27 = torch.nn.functional.softmax(add_46, dim=-1)
        add_46 = None
        out_39 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_27, v_27
        )
        dot_prod_27 = v_27 = None
        permute_96 = out_39.permute(0, 1, 3, 2, 4)
        out_39 = None
        out_40 = permute_96.reshape(1, 4, 49, 256)
        permute_96 = None
        out_41 = torch._C._nn.linear(
            out_40,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_40 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_173 = torch.nn.functional.dropout(out_41, 0.0, False, False)
        out_41 = None
        _log_api_usage_once_32 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_32 = None
        x_107 = res_12 + input_173
        res_12 = input_173 = None
        input_174 = torch.nn.functional.layer_norm(
            x_107,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_175 = torch._C._nn.linear(
            input_174,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_174 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_176 = torch._C._nn.gelu(input_175, approximate="none")
        input_175 = None
        input_177 = torch._C._nn.linear(
            input_176,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_176 = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_2_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_178 = torch.nn.functional.dropout(input_177, 0.0, False, False)
        input_177 = None
        _log_api_usage_once_33 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_33 = None
        x_108 = x_107 + input_178
        x_107 = input_178 = None
        res_13 = torch.swapaxes(x_108, -2, -3)
        x_108 = None
        x_109 = res_13.reshape(1, 7, 7, 2, 2, 256)
        res_13 = None
        x_110 = x_109.permute(0, 5, 1, 3, 2, 4)
        x_109 = None
        x_111 = x_110.reshape(1, 256, 14, 14)
        x_110 = None
        input_179 = torch.nn.functional.batch_norm(
            x_111,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_180 = torch.conv2d(
            input_179,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_179 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_181 = torch.nn.functional.batch_norm(
            input_180,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_180 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_182 = torch._C._nn.gelu(input_181, approximate="none")
        input_181 = None
        input_183 = torch.conv2d(
            input_182,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        input_182 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_184 = torch.nn.functional.batch_norm(
            input_183,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_183 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_185 = torch._C._nn.gelu(input_184, approximate="none")
        input_184 = None
        scale_35 = torch.nn.functional.adaptive_avg_pool2d(input_185, 1)
        scale_36 = torch.conv2d(
            scale_35,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_35 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_37 = torch.nn.functional.silu(scale_36, inplace=False)
        scale_36 = None
        scale_38 = torch.conv2d(
            scale_37,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_37 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_39 = torch.sigmoid(scale_38)
        scale_38 = None
        input_186 = scale_39 * input_185
        scale_39 = input_185 = None
        input_187 = torch.conv2d(
            input_186,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_186 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_34 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_34 = None
        input_188 = x_111 + input_187
        x_111 = input_187 = None
        x_112 = input_188.reshape(1, 256, 2, 7, 2, 7)
        input_188 = None
        x_113 = x_112.permute(0, 2, 4, 3, 5, 1)
        x_112 = None
        x_114 = x_113.reshape(1, 4, 49, 256)
        x_113 = None
        input_189 = torch.nn.functional.layer_norm(
            x_114,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_14 = torch._C._nn.linear(
            input_189,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_189 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_14 = torch.chunk(qkv_14, 3, dim=-1)
        qkv_14 = None
        q_28 = chunk_14[0]
        k_42 = chunk_14[1]
        v_28 = chunk_14[2]
        chunk_14 = None
        reshape_114 = q_28.reshape(1, 4, 49, 8, 32)
        q_28 = None
        q_29 = reshape_114.permute(0, 1, 3, 2, 4)
        reshape_114 = None
        reshape_115 = k_42.reshape(1, 4, 49, 8, 32)
        k_42 = None
        k_43 = reshape_115.permute(0, 1, 3, 2, 4)
        reshape_115 = None
        reshape_116 = v_28.reshape(1, 4, 49, 8, 32)
        v_28 = None
        v_29 = reshape_116.permute(0, 1, 3, 2, 4)
        reshape_116 = None
        k_44 = k_43 * 0.0625
        k_43 = None
        dot_prod_28 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_29, k_44
        )
        q_29 = k_44 = None
        bias_index_14 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_59 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_14
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_14
        ) = None
        relative_bias_28 = getitem_59.view(49, 49, -1)
        getitem_59 = None
        permute_102 = relative_bias_28.permute(2, 0, 1)
        relative_bias_28 = None
        relative_bias_29 = permute_102.contiguous()
        permute_102 = None
        pos_bias_14 = relative_bias_29.unsqueeze(0)
        relative_bias_29 = None
        add_50 = dot_prod_28 + pos_bias_14
        dot_prod_28 = pos_bias_14 = None
        dot_prod_29 = torch.nn.functional.softmax(add_50, dim=-1)
        add_50 = None
        out_42 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_29, v_29
        )
        dot_prod_29 = v_29 = None
        permute_103 = out_42.permute(0, 1, 3, 2, 4)
        out_42 = None
        out_43 = permute_103.reshape(1, 4, 49, 256)
        permute_103 = None
        out_44 = torch._C._nn.linear(
            out_43,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_43 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_190 = torch.nn.functional.dropout(out_44, 0.0, False, False)
        out_44 = None
        _log_api_usage_once_35 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_35 = None
        x_115 = x_114 + input_190
        x_114 = input_190 = None
        input_191 = torch.nn.functional.layer_norm(
            x_115,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_192 = torch._C._nn.linear(
            input_191,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_191 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_193 = torch._C._nn.gelu(input_192, approximate="none")
        input_192 = None
        input_194 = torch._C._nn.linear(
            input_193,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_193 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_195 = torch.nn.functional.dropout(input_194, 0.0, False, False)
        input_194 = None
        _log_api_usage_once_36 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_36 = None
        x_116 = x_115 + input_195
        x_115 = input_195 = None
        x_117 = x_116.reshape(1, 2, 2, 7, 7, 256)
        x_116 = None
        x_118 = x_117.permute(0, 5, 1, 3, 2, 4)
        x_117 = None
        x_119 = x_118.reshape(1, 256, 14, 14)
        x_118 = None
        x_120 = x_119.reshape(1, 256, 7, 2, 7, 2)
        x_119 = None
        x_121 = x_120.permute(0, 2, 4, 3, 5, 1)
        x_120 = None
        x_122 = x_121.reshape(1, 49, 4, 256)
        x_121 = None
        res_14 = torch.swapaxes(x_122, -2, -3)
        x_122 = None
        input_196 = torch.nn.functional.layer_norm(
            res_14,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_15 = torch._C._nn.linear(
            input_196,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_196 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_15 = torch.chunk(qkv_15, 3, dim=-1)
        qkv_15 = None
        q_30 = chunk_15[0]
        k_45 = chunk_15[1]
        v_30 = chunk_15[2]
        chunk_15 = None
        reshape_122 = q_30.reshape(1, 4, 49, 8, 32)
        q_30 = None
        q_31 = reshape_122.permute(0, 1, 3, 2, 4)
        reshape_122 = None
        reshape_123 = k_45.reshape(1, 4, 49, 8, 32)
        k_45 = None
        k_46 = reshape_123.permute(0, 1, 3, 2, 4)
        reshape_123 = None
        reshape_124 = v_30.reshape(1, 4, 49, 8, 32)
        v_30 = None
        v_31 = reshape_124.permute(0, 1, 3, 2, 4)
        reshape_124 = None
        k_47 = k_46 * 0.0625
        k_46 = None
        dot_prod_30 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_31, k_47
        )
        q_31 = k_47 = None
        bias_index_15 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_63 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_15
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_15
        ) = None
        relative_bias_30 = getitem_63.view(49, 49, -1)
        getitem_63 = None
        permute_109 = relative_bias_30.permute(2, 0, 1)
        relative_bias_30 = None
        relative_bias_31 = permute_109.contiguous()
        permute_109 = None
        pos_bias_15 = relative_bias_31.unsqueeze(0)
        relative_bias_31 = None
        add_53 = dot_prod_30 + pos_bias_15
        dot_prod_30 = pos_bias_15 = None
        dot_prod_31 = torch.nn.functional.softmax(add_53, dim=-1)
        add_53 = None
        out_45 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_31, v_31
        )
        dot_prod_31 = v_31 = None
        permute_110 = out_45.permute(0, 1, 3, 2, 4)
        out_45 = None
        out_46 = permute_110.reshape(1, 4, 49, 256)
        permute_110 = None
        out_47 = torch._C._nn.linear(
            out_46,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_46 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_197 = torch.nn.functional.dropout(out_47, 0.0, False, False)
        out_47 = None
        _log_api_usage_once_37 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_37 = None
        x_123 = res_14 + input_197
        res_14 = input_197 = None
        input_198 = torch.nn.functional.layer_norm(
            x_123,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_199 = torch._C._nn.linear(
            input_198,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_198 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_200 = torch._C._nn.gelu(input_199, approximate="none")
        input_199 = None
        input_201 = torch._C._nn.linear(
            input_200,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_200 = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_3_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_202 = torch.nn.functional.dropout(input_201, 0.0, False, False)
        input_201 = None
        _log_api_usage_once_38 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_38 = None
        x_124 = x_123 + input_202
        x_123 = input_202 = None
        res_15 = torch.swapaxes(x_124, -2, -3)
        x_124 = None
        x_125 = res_15.reshape(1, 7, 7, 2, 2, 256)
        res_15 = None
        x_126 = x_125.permute(0, 5, 1, 3, 2, 4)
        x_125 = None
        x_127 = x_126.reshape(1, 256, 14, 14)
        x_126 = None
        input_203 = torch.nn.functional.batch_norm(
            x_127,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_204 = torch.conv2d(
            input_203,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_203 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_205 = torch.nn.functional.batch_norm(
            input_204,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_204 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_206 = torch._C._nn.gelu(input_205, approximate="none")
        input_205 = None
        input_207 = torch.conv2d(
            input_206,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        input_206 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_208 = torch.nn.functional.batch_norm(
            input_207,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_207 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_209 = torch._C._nn.gelu(input_208, approximate="none")
        input_208 = None
        scale_40 = torch.nn.functional.adaptive_avg_pool2d(input_209, 1)
        scale_41 = torch.conv2d(
            scale_40,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_40 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_42 = torch.nn.functional.silu(scale_41, inplace=False)
        scale_41 = None
        scale_43 = torch.conv2d(
            scale_42,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_42 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_44 = torch.sigmoid(scale_43)
        scale_43 = None
        input_210 = scale_44 * input_209
        scale_44 = input_209 = None
        input_211 = torch.conv2d(
            input_210,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_210 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_39 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_39 = None
        input_212 = x_127 + input_211
        x_127 = input_211 = None
        x_128 = input_212.reshape(1, 256, 2, 7, 2, 7)
        input_212 = None
        x_129 = x_128.permute(0, 2, 4, 3, 5, 1)
        x_128 = None
        x_130 = x_129.reshape(1, 4, 49, 256)
        x_129 = None
        input_213 = torch.nn.functional.layer_norm(
            x_130,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_16 = torch._C._nn.linear(
            input_213,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_213 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_16 = torch.chunk(qkv_16, 3, dim=-1)
        qkv_16 = None
        q_32 = chunk_16[0]
        k_48 = chunk_16[1]
        v_32 = chunk_16[2]
        chunk_16 = None
        reshape_130 = q_32.reshape(1, 4, 49, 8, 32)
        q_32 = None
        q_33 = reshape_130.permute(0, 1, 3, 2, 4)
        reshape_130 = None
        reshape_131 = k_48.reshape(1, 4, 49, 8, 32)
        k_48 = None
        k_49 = reshape_131.permute(0, 1, 3, 2, 4)
        reshape_131 = None
        reshape_132 = v_32.reshape(1, 4, 49, 8, 32)
        v_32 = None
        v_33 = reshape_132.permute(0, 1, 3, 2, 4)
        reshape_132 = None
        k_50 = k_49 * 0.0625
        k_49 = None
        dot_prod_32 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_33, k_50
        )
        q_33 = k_50 = None
        bias_index_16 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_67 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_16
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_16
        ) = None
        relative_bias_32 = getitem_67.view(49, 49, -1)
        getitem_67 = None
        permute_116 = relative_bias_32.permute(2, 0, 1)
        relative_bias_32 = None
        relative_bias_33 = permute_116.contiguous()
        permute_116 = None
        pos_bias_16 = relative_bias_33.unsqueeze(0)
        relative_bias_33 = None
        add_57 = dot_prod_32 + pos_bias_16
        dot_prod_32 = pos_bias_16 = None
        dot_prod_33 = torch.nn.functional.softmax(add_57, dim=-1)
        add_57 = None
        out_48 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_33, v_33
        )
        dot_prod_33 = v_33 = None
        permute_117 = out_48.permute(0, 1, 3, 2, 4)
        out_48 = None
        out_49 = permute_117.reshape(1, 4, 49, 256)
        permute_117 = None
        out_50 = torch._C._nn.linear(
            out_49,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_49 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_214 = torch.nn.functional.dropout(out_50, 0.0, False, False)
        out_50 = None
        _log_api_usage_once_40 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_40 = None
        x_131 = x_130 + input_214
        x_130 = input_214 = None
        input_215 = torch.nn.functional.layer_norm(
            x_131,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_216 = torch._C._nn.linear(
            input_215,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_215 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_217 = torch._C._nn.gelu(input_216, approximate="none")
        input_216 = None
        input_218 = torch._C._nn.linear(
            input_217,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_217 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_219 = torch.nn.functional.dropout(input_218, 0.0, False, False)
        input_218 = None
        _log_api_usage_once_41 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_41 = None
        x_132 = x_131 + input_219
        x_131 = input_219 = None
        x_133 = x_132.reshape(1, 2, 2, 7, 7, 256)
        x_132 = None
        x_134 = x_133.permute(0, 5, 1, 3, 2, 4)
        x_133 = None
        x_135 = x_134.reshape(1, 256, 14, 14)
        x_134 = None
        x_136 = x_135.reshape(1, 256, 7, 2, 7, 2)
        x_135 = None
        x_137 = x_136.permute(0, 2, 4, 3, 5, 1)
        x_136 = None
        x_138 = x_137.reshape(1, 49, 4, 256)
        x_137 = None
        res_16 = torch.swapaxes(x_138, -2, -3)
        x_138 = None
        input_220 = torch.nn.functional.layer_norm(
            res_16,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_17 = torch._C._nn.linear(
            input_220,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_220 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_17 = torch.chunk(qkv_17, 3, dim=-1)
        qkv_17 = None
        q_34 = chunk_17[0]
        k_51 = chunk_17[1]
        v_34 = chunk_17[2]
        chunk_17 = None
        reshape_138 = q_34.reshape(1, 4, 49, 8, 32)
        q_34 = None
        q_35 = reshape_138.permute(0, 1, 3, 2, 4)
        reshape_138 = None
        reshape_139 = k_51.reshape(1, 4, 49, 8, 32)
        k_51 = None
        k_52 = reshape_139.permute(0, 1, 3, 2, 4)
        reshape_139 = None
        reshape_140 = v_34.reshape(1, 4, 49, 8, 32)
        v_34 = None
        v_35 = reshape_140.permute(0, 1, 3, 2, 4)
        reshape_140 = None
        k_53 = k_52 * 0.0625
        k_52 = None
        dot_prod_34 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_35, k_53
        )
        q_35 = k_53 = None
        bias_index_17 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_71 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_17
        ]
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_17
        ) = None
        relative_bias_34 = getitem_71.view(49, 49, -1)
        getitem_71 = None
        permute_123 = relative_bias_34.permute(2, 0, 1)
        relative_bias_34 = None
        relative_bias_35 = permute_123.contiguous()
        permute_123 = None
        pos_bias_17 = relative_bias_35.unsqueeze(0)
        relative_bias_35 = None
        add_60 = dot_prod_34 + pos_bias_17
        dot_prod_34 = pos_bias_17 = None
        dot_prod_35 = torch.nn.functional.softmax(add_60, dim=-1)
        add_60 = None
        out_51 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_35, v_35
        )
        dot_prod_35 = v_35 = None
        permute_124 = out_51.permute(0, 1, 3, 2, 4)
        out_51 = None
        out_52 = permute_124.reshape(1, 4, 49, 256)
        permute_124 = None
        out_53 = torch._C._nn.linear(
            out_52,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_52 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_221 = torch.nn.functional.dropout(out_53, 0.0, False, False)
        out_53 = None
        _log_api_usage_once_42 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_42 = None
        x_139 = res_16 + input_221
        res_16 = input_221 = None
        input_222 = torch.nn.functional.layer_norm(
            x_139,
            (256,),
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_223 = torch._C._nn.linear(
            input_222,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_222 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_224 = torch._C._nn.gelu(input_223, approximate="none")
        input_223 = None
        input_225 = torch._C._nn.linear(
            input_224,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_224 = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_2_modules_layers_modules_4_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_226 = torch.nn.functional.dropout(input_225, 0.0, False, False)
        input_225 = None
        _log_api_usage_once_43 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_43 = None
        x_140 = x_139 + input_226
        x_139 = input_226 = None
        res_17 = torch.swapaxes(x_140, -2, -3)
        x_140 = None
        x_141 = res_17.reshape(1, 7, 7, 2, 2, 256)
        res_17 = None
        x_142 = x_141.permute(0, 5, 1, 3, 2, 4)
        x_141 = None
        x_143 = x_142.reshape(1, 256, 14, 14)
        x_142 = None
        input_227 = torch._C._nn.avg_pool2d(x_143, 3, 2, 1, False, True, None)
        input_228 = torch.conv2d(
            input_227,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_227 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_proj_modules_1_parameters_bias_ = (None)
        input_229 = torch.nn.functional.batch_norm(
            x_143,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        x_143 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_230 = torch.conv2d(
            input_229,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_229 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_231 = torch.nn.functional.batch_norm(
            input_230,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_230 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_232 = torch._C._nn.gelu(input_231, approximate="none")
        input_231 = None
        input_233 = torch.conv2d(
            input_232,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            2048,
        )
        input_232 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_234 = torch.nn.functional.batch_norm(
            input_233,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_233 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_235 = torch._C._nn.gelu(input_234, approximate="none")
        input_234 = None
        scale_45 = torch.nn.functional.adaptive_avg_pool2d(input_235, 1)
        scale_46 = torch.conv2d(
            scale_45,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_45 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_47 = torch.nn.functional.silu(scale_46, inplace=False)
        scale_46 = None
        scale_48 = torch.conv2d(
            scale_47,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_47 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_49 = torch.sigmoid(scale_48)
        scale_48 = None
        input_236 = scale_49 * input_235
        scale_49 = input_235 = None
        input_237 = torch.conv2d(
            input_236,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_236 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_44 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_44 = None
        input_238 = input_228 + input_237
        input_228 = input_237 = None
        x_144 = input_238.reshape(1, 512, 1, 7, 1, 7)
        input_238 = None
        x_145 = x_144.permute(0, 2, 4, 3, 5, 1)
        x_144 = None
        x_146 = x_145.reshape(1, 1, 49, 512)
        x_145 = None
        input_239 = torch.nn.functional.layer_norm(
            x_146,
            (512,),
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_18 = torch._C._nn.linear(
            input_239,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_239 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_18 = torch.chunk(qkv_18, 3, dim=-1)
        qkv_18 = None
        q_36 = chunk_18[0]
        k_54 = chunk_18[1]
        v_36 = chunk_18[2]
        chunk_18 = None
        reshape_146 = q_36.reshape(1, 1, 49, 16, 32)
        q_36 = None
        q_37 = reshape_146.permute(0, 1, 3, 2, 4)
        reshape_146 = None
        reshape_147 = k_54.reshape(1, 1, 49, 16, 32)
        k_54 = None
        k_55 = reshape_147.permute(0, 1, 3, 2, 4)
        reshape_147 = None
        reshape_148 = v_36.reshape(1, 1, 49, 16, 32)
        v_36 = None
        v_37 = reshape_148.permute(0, 1, 3, 2, 4)
        reshape_148 = None
        k_56 = k_55 * 0.04419417382415922
        k_55 = None
        dot_prod_36 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_37, k_56
        )
        q_37 = k_56 = None
        bias_index_18 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_75 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_18
        ]
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_18
        ) = None
        relative_bias_36 = getitem_75.view(49, 49, -1)
        getitem_75 = None
        permute_130 = relative_bias_36.permute(2, 0, 1)
        relative_bias_36 = None
        relative_bias_37 = permute_130.contiguous()
        permute_130 = None
        pos_bias_18 = relative_bias_37.unsqueeze(0)
        relative_bias_37 = None
        add_64 = dot_prod_36 + pos_bias_18
        dot_prod_36 = pos_bias_18 = None
        dot_prod_37 = torch.nn.functional.softmax(add_64, dim=-1)
        add_64 = None
        out_54 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_37, v_37
        )
        dot_prod_37 = v_37 = None
        permute_131 = out_54.permute(0, 1, 3, 2, 4)
        out_54 = None
        out_55 = permute_131.reshape(1, 1, 49, 512)
        permute_131 = None
        out_56 = torch._C._nn.linear(
            out_55,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_55 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_240 = torch.nn.functional.dropout(out_56, 0.0, False, False)
        out_56 = None
        _log_api_usage_once_45 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_45 = None
        x_147 = x_146 + input_240
        x_146 = input_240 = None
        input_241 = torch.nn.functional.layer_norm(
            x_147,
            (512,),
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_242 = torch._C._nn.linear(
            input_241,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_241 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_243 = torch._C._nn.gelu(input_242, approximate="none")
        input_242 = None
        input_244 = torch._C._nn.linear(
            input_243,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_243 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_245 = torch.nn.functional.dropout(input_244, 0.0, False, False)
        input_244 = None
        _log_api_usage_once_46 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_46 = None
        x_148 = x_147 + input_245
        x_147 = input_245 = None
        x_149 = x_148.reshape(1, 1, 1, 7, 7, 512)
        x_148 = None
        x_150 = x_149.permute(0, 5, 1, 3, 2, 4)
        x_149 = None
        x_151 = x_150.reshape(1, 512, 7, 7)
        x_150 = None
        x_152 = x_151.reshape(1, 512, 7, 1, 7, 1)
        x_151 = None
        x_153 = x_152.permute(0, 2, 4, 3, 5, 1)
        x_152 = None
        x_154 = x_153.reshape(1, 49, 1, 512)
        x_153 = None
        res_18 = torch.swapaxes(x_154, -2, -3)
        x_154 = None
        input_246 = torch.nn.functional.layer_norm(
            res_18,
            (512,),
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_19 = torch._C._nn.linear(
            input_246,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_246 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_19 = torch.chunk(qkv_19, 3, dim=-1)
        qkv_19 = None
        q_38 = chunk_19[0]
        k_57 = chunk_19[1]
        v_38 = chunk_19[2]
        chunk_19 = None
        reshape_154 = q_38.reshape(1, 1, 49, 16, 32)
        q_38 = None
        q_39 = reshape_154.permute(0, 1, 3, 2, 4)
        reshape_154 = None
        reshape_155 = k_57.reshape(1, 1, 49, 16, 32)
        k_57 = None
        k_58 = reshape_155.permute(0, 1, 3, 2, 4)
        reshape_155 = None
        reshape_156 = v_38.reshape(1, 1, 49, 16, 32)
        v_38 = None
        v_39 = reshape_156.permute(0, 1, 3, 2, 4)
        reshape_156 = None
        k_59 = k_58 * 0.04419417382415922
        k_58 = None
        dot_prod_38 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_39, k_59
        )
        q_39 = k_59 = None
        bias_index_19 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_79 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_19
        ]
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_19
        ) = None
        relative_bias_38 = getitem_79.view(49, 49, -1)
        getitem_79 = None
        permute_137 = relative_bias_38.permute(2, 0, 1)
        relative_bias_38 = None
        relative_bias_39 = permute_137.contiguous()
        permute_137 = None
        pos_bias_19 = relative_bias_39.unsqueeze(0)
        relative_bias_39 = None
        add_67 = dot_prod_38 + pos_bias_19
        dot_prod_38 = pos_bias_19 = None
        dot_prod_39 = torch.nn.functional.softmax(add_67, dim=-1)
        add_67 = None
        out_57 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_39, v_39
        )
        dot_prod_39 = v_39 = None
        permute_138 = out_57.permute(0, 1, 3, 2, 4)
        out_57 = None
        out_58 = permute_138.reshape(1, 1, 49, 512)
        permute_138 = None
        out_59 = torch._C._nn.linear(
            out_58,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_58 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_247 = torch.nn.functional.dropout(out_59, 0.0, False, False)
        out_59 = None
        _log_api_usage_once_47 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_47 = None
        x_155 = res_18 + input_247
        res_18 = input_247 = None
        input_248 = torch.nn.functional.layer_norm(
            x_155,
            (512,),
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_249 = torch._C._nn.linear(
            input_248,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_248 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_250 = torch._C._nn.gelu(input_249, approximate="none")
        input_249 = None
        input_251 = torch._C._nn.linear(
            input_250,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_250 = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_0_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_252 = torch.nn.functional.dropout(input_251, 0.0, False, False)
        input_251 = None
        _log_api_usage_once_48 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_48 = None
        x_156 = x_155 + input_252
        x_155 = input_252 = None
        res_19 = torch.swapaxes(x_156, -2, -3)
        x_156 = None
        x_157 = res_19.reshape(1, 7, 7, 1, 1, 512)
        res_19 = None
        x_158 = x_157.permute(0, 5, 1, 3, 2, 4)
        x_157 = None
        x_159 = x_158.reshape(1, 512, 7, 7)
        x_158 = None
        input_253 = torch.nn.functional.batch_norm(
            x_159,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_mean_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_buffers_running_var_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_pre_norm_parameters_bias_ = (None)
        input_254 = torch.conv2d(
            input_253,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_,
            None,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_253 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_0_parameters_weight_ = (None)
        input_255 = torch.nn.functional.batch_norm(
            input_254,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_254 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_a_modules_1_parameters_bias_ = (None)
        input_256 = torch._C._nn.gelu(input_255, approximate="none")
        input_255 = None
        input_257 = torch.conv2d(
            input_256,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            2048,
        )
        input_256 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_0_parameters_weight_ = (None)
        input_258 = torch.nn.functional.batch_norm(
            input_257,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_,
            False,
            0.01,
            0.001,
        )
        input_257 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_mean_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_buffers_running_var_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_b_modules_1_parameters_bias_ = (None)
        input_259 = torch._C._nn.gelu(input_258, approximate="none")
        input_258 = None
        scale_50 = torch.nn.functional.adaptive_avg_pool2d(input_259, 1)
        scale_51 = torch.conv2d(
            scale_50,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_50 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc1_parameters_bias_ = (None)
        scale_52 = torch.nn.functional.silu(scale_51, inplace=False)
        scale_51 = None
        scale_53 = torch.conv2d(
            scale_52,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        scale_52 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_squeeze_excitation_modules_fc2_parameters_bias_ = (None)
        scale_54 = torch.sigmoid(scale_53)
        scale_53 = None
        input_260 = scale_54 * input_259
        scale_54 = input_259 = None
        input_261 = torch.conv2d(
            input_260,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        input_260 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_mbconv_modules_layers_modules_conv_c_parameters_bias_ = (None)
        _log_api_usage_once_49 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_49 = None
        input_262 = x_159 + input_261
        x_159 = input_261 = None
        x_160 = input_262.reshape(1, 512, 1, 7, 1, 7)
        input_262 = None
        x_161 = x_160.permute(0, 2, 4, 3, 5, 1)
        x_160 = None
        x_162 = x_161.reshape(1, 1, 49, 512)
        x_161 = None
        input_263 = torch.nn.functional.layer_norm(
            x_162,
            (512,),
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_20 = torch._C._nn.linear(
            input_263,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_263 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_20 = torch.chunk(qkv_20, 3, dim=-1)
        qkv_20 = None
        q_40 = chunk_20[0]
        k_60 = chunk_20[1]
        v_40 = chunk_20[2]
        chunk_20 = None
        reshape_162 = q_40.reshape(1, 1, 49, 16, 32)
        q_40 = None
        q_41 = reshape_162.permute(0, 1, 3, 2, 4)
        reshape_162 = None
        reshape_163 = k_60.reshape(1, 1, 49, 16, 32)
        k_60 = None
        k_61 = reshape_163.permute(0, 1, 3, 2, 4)
        reshape_163 = None
        reshape_164 = v_40.reshape(1, 1, 49, 16, 32)
        v_40 = None
        v_41 = reshape_164.permute(0, 1, 3, 2, 4)
        reshape_164 = None
        k_62 = k_61 * 0.04419417382415922
        k_61 = None
        dot_prod_40 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_41, k_62
        )
        q_41 = k_62 = None
        bias_index_20 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_83 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_20
        ]
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_20
        ) = None
        relative_bias_40 = getitem_83.view(49, 49, -1)
        getitem_83 = None
        permute_144 = relative_bias_40.permute(2, 0, 1)
        relative_bias_40 = None
        relative_bias_41 = permute_144.contiguous()
        permute_144 = None
        pos_bias_20 = relative_bias_41.unsqueeze(0)
        relative_bias_41 = None
        add_71 = dot_prod_40 + pos_bias_20
        dot_prod_40 = pos_bias_20 = None
        dot_prod_41 = torch.nn.functional.softmax(add_71, dim=-1)
        add_71 = None
        out_60 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_41, v_41
        )
        dot_prod_41 = v_41 = None
        permute_145 = out_60.permute(0, 1, 3, 2, 4)
        out_60 = None
        out_61 = permute_145.reshape(1, 1, 49, 512)
        permute_145 = None
        out_62 = torch._C._nn.linear(
            out_61,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_61 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_264 = torch.nn.functional.dropout(out_62, 0.0, False, False)
        out_62 = None
        _log_api_usage_once_50 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_50 = None
        x_163 = x_162 + input_264
        x_162 = input_264 = None
        input_265 = torch.nn.functional.layer_norm(
            x_163,
            (512,),
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_266 = torch._C._nn.linear(
            input_265,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_265 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_267 = torch._C._nn.gelu(input_266, approximate="none")
        input_266 = None
        input_268 = torch._C._nn.linear(
            input_267,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_267 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_window_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_269 = torch.nn.functional.dropout(input_268, 0.0, False, False)
        input_268 = None
        _log_api_usage_once_51 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_51 = None
        x_164 = x_163 + input_269
        x_163 = input_269 = None
        x_165 = x_164.reshape(1, 1, 1, 7, 7, 512)
        x_164 = None
        x_166 = x_165.permute(0, 5, 1, 3, 2, 4)
        x_165 = None
        x_167 = x_166.reshape(1, 512, 7, 7)
        x_166 = None
        x_168 = x_167.reshape(1, 512, 7, 1, 7, 1)
        x_167 = None
        x_169 = x_168.permute(0, 2, 4, 3, 5, 1)
        x_168 = None
        x_170 = x_169.reshape(1, 49, 1, 512)
        x_169 = None
        res_20 = torch.swapaxes(x_170, -2, -3)
        x_170 = None
        input_270 = torch.nn.functional.layer_norm(
            res_20,
            (512,),
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_0_parameters_bias_ = (None)
        qkv_21 = torch._C._nn.linear(
            input_270,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_,
        )
        input_270 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_to_qkv_parameters_bias_ = (None)
        chunk_21 = torch.chunk(qkv_21, 3, dim=-1)
        qkv_21 = None
        q_42 = chunk_21[0]
        k_63 = chunk_21[1]
        v_42 = chunk_21[2]
        chunk_21 = None
        reshape_170 = q_42.reshape(1, 1, 49, 16, 32)
        q_42 = None
        q_43 = reshape_170.permute(0, 1, 3, 2, 4)
        reshape_170 = None
        reshape_171 = k_63.reshape(1, 1, 49, 16, 32)
        k_63 = None
        k_64 = reshape_171.permute(0, 1, 3, 2, 4)
        reshape_171 = None
        reshape_172 = v_42.reshape(1, 1, 49, 16, 32)
        v_42 = None
        v_43 = reshape_172.permute(0, 1, 3, 2, 4)
        reshape_172 = None
        k_65 = k_64 * 0.04419417382415922
        k_64 = None
        dot_prod_42 = torch.functional.einsum(
            "B G H I D, B G H J D -> B G H I J", q_43, k_65
        )
        q_43 = k_65 = None
        bias_index_21 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_.view(
            -1
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_buffers_relative_position_index_ = (
            None
        )
        getitem_87 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_[
            bias_index_21
        ]
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_parameters_relative_position_bias_table_ = (
            bias_index_21
        ) = None
        relative_bias_42 = getitem_87.view(49, 49, -1)
        getitem_87 = None
        permute_151 = relative_bias_42.permute(2, 0, 1)
        relative_bias_42 = None
        relative_bias_43 = permute_151.contiguous()
        permute_151 = None
        pos_bias_21 = relative_bias_43.unsqueeze(0)
        relative_bias_43 = None
        add_74 = dot_prod_42 + pos_bias_21
        dot_prod_42 = pos_bias_21 = None
        dot_prod_43 = torch.nn.functional.softmax(add_74, dim=-1)
        add_74 = None
        out_63 = torch.functional.einsum(
            "B G H I J, B G H J D -> B G H I D", dot_prod_43, v_43
        )
        dot_prod_43 = v_43 = None
        permute_152 = out_63.permute(0, 1, 3, 2, 4)
        out_63 = None
        out_64 = permute_152.reshape(1, 1, 49, 512)
        permute_152 = None
        out_65 = torch._C._nn.linear(
            out_64,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_,
        )
        out_64 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_attn_layer_modules_1_modules_merge_parameters_bias_ = (None)
        input_271 = torch.nn.functional.dropout(out_65, 0.0, False, False)
        out_65 = None
        _log_api_usage_once_52 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_52 = None
        x_171 = res_20 + input_271
        res_20 = input_271 = None
        input_272 = torch.nn.functional.layer_norm(
            x_171,
            (512,),
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_,
            1e-05,
        )
        l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_0_parameters_bias_ = (None)
        input_273 = torch._C._nn.linear(
            input_272,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_,
        )
        input_272 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_1_parameters_bias_ = (None)
        input_274 = torch._C._nn.gelu(input_273, approximate="none")
        input_273 = None
        input_275 = torch._C._nn.linear(
            input_274,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_,
            l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_,
        )
        input_274 = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_weight_ = l_self_modules_blocks_modules_3_modules_layers_modules_1_modules_layers_modules_grid_attention_modules_mlp_layer_modules_3_parameters_bias_ = (None)
        input_276 = torch.nn.functional.dropout(input_275, 0.0, False, False)
        input_275 = None
        _log_api_usage_once_53 = torch._C._log_api_usage_once(
            "torchvision.ops.stochastic_depth.stochastic_depth"
        )
        _log_api_usage_once_53 = None
        x_172 = x_171 + input_276
        x_171 = input_276 = None
        res_21 = torch.swapaxes(x_172, -2, -3)
        x_172 = None
        x_173 = res_21.reshape(1, 7, 7, 1, 1, 512)
        res_21 = None
        x_174 = x_173.permute(0, 5, 1, 3, 2, 4)
        x_173 = None
        x_175 = x_174.reshape(1, 512, 7, 7)
        x_174 = None
        input_277 = torch.nn.functional.adaptive_avg_pool2d(x_175, 1)
        x_175 = None
        input_278 = input_277.flatten(1, -1)
        input_277 = None
        input_279 = torch.nn.functional.layer_norm(
            input_278,
            (512,),
            l_self_modules_classifier_modules_2_parameters_weight_,
            l_self_modules_classifier_modules_2_parameters_bias_,
            1e-05,
        )
        input_278 = (
            l_self_modules_classifier_modules_2_parameters_weight_
        ) = l_self_modules_classifier_modules_2_parameters_bias_ = None
        input_280 = torch._C._nn.linear(
            input_279,
            l_self_modules_classifier_modules_3_parameters_weight_,
            l_self_modules_classifier_modules_3_parameters_bias_,
        )
        input_279 = (
            l_self_modules_classifier_modules_3_parameters_weight_
        ) = l_self_modules_classifier_modules_3_parameters_bias_ = None
        input_281 = torch.tanh(input_280)
        input_280 = None
        input_282 = torch._C._nn.linear(
            input_281, l_self_modules_classifier_modules_5_parameters_weight_, None
        )
        input_281 = l_self_modules_classifier_modules_5_parameters_weight_ = None
        return (input_282,)
