import torch

from torch import device


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_input_values_: torch.Tensor,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_projector_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_projector_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_input_values_ = L_input_values_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_ = L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_
        l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_ = L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_ = L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_
        l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_ = L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_
        l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_ = (
            L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_
        )
        l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_ = (
            L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_
        l_self_modules_projector_parameters_weight_ = (
            L_self_modules_projector_parameters_weight_
        )
        l_self_modules_projector_parameters_bias_ = (
            L_self_modules_projector_parameters_bias_
        )
        l_self_modules_classifier_parameters_weight_ = (
            L_self_modules_classifier_parameters_weight_
        )
        l_self_modules_classifier_parameters_bias_ = (
            L_self_modules_classifier_parameters_bias_
        )
        hidden_states = l_input_values_[(slice(None, None, None), None)]
        l_input_values_ = None
        hidden_states_1 = torch.conv1d(
            hidden_states,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_,
            None,
            (4,),
            (0,),
            (1,),
            1,
        )
        hidden_states = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_ = (None)
        hidden_states_2 = torch.nn.functional.group_norm(
            hidden_states_1,
            32,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_1 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_3 = torch._C._nn.gelu(hidden_states_2)
        hidden_states_2 = None
        hidden_states_4 = torch.conv1d(
            hidden_states_3,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_,
            None,
            (4,),
            (0,),
            (1,),
            1,
        )
        hidden_states_3 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_ = (None)
        hidden_states_5 = torch._C._nn.gelu(hidden_states_4)
        hidden_states_4 = None
        hidden_states_6 = torch.conv1d(
            hidden_states_5,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_,
            None,
            (4,),
            (0,),
            (1,),
            1,
        )
        hidden_states_5 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_ = (None)
        hidden_states_7 = torch._C._nn.gelu(hidden_states_6)
        hidden_states_6 = None
        extract_features = hidden_states_7.transpose(1, 2)
        hidden_states_7 = None
        norm_hidden_states = torch.nn.functional.layer_norm(
            extract_features,
            (32,),
            l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        extract_features = l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_8 = torch._C._nn.linear(
            norm_hidden_states,
            l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_,
            l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_,
        )
        norm_hidden_states = l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_ = l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_ = (None)
        hidden_states_9 = torch.nn.functional.dropout(
            hidden_states_8, 0.0, False, False
        )
        hidden_states_8 = None
        hidden_states_10 = hidden_states_9.transpose(1, 2)
        x = torch._weight_norm(
            l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_,
            l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_,
            2,
        )
        l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_ = l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_ = (None)
        hidden_states_11 = torch.conv1d(
            hidden_states_10,
            x,
            l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_,
            (1,),
            (8,),
            (1,),
            2,
        )
        hidden_states_10 = (
            x
        ) = l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_ = (None)
        hidden_states_12 = hidden_states_11[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        hidden_states_11 = None
        hidden_states_13 = torch._C._nn.gelu(hidden_states_12)
        hidden_states_12 = None
        hidden_states_14 = hidden_states_13.transpose(1, 2)
        hidden_states_13 = None
        hidden_states_15 = hidden_states_9 + hidden_states_14
        hidden_states_9 = hidden_states_14 = None
        hidden_states_16 = torch.nn.functional.layer_norm(
            hidden_states_15,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_15 = (
            l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_
        ) = (
            l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_
        ) = None
        hidden_states_17 = torch.nn.functional.dropout(
            hidden_states_16, 0.1, False, False
        )
        hidden_states_16 = None
        dropout_probability = torch.rand([])
        dropout_probability = None
        arange = torch.arange(1248, dtype=torch.int64)
        context_position = arange[(slice(None, None, None), None)]
        arange = None
        arange_1 = torch.arange(1248, dtype=torch.int64)
        memory_position = arange_1[(None, slice(None, None, None))]
        arange_1 = None
        relative_position = memory_position - context_position
        memory_position = context_position = None
        gt = relative_position > 0
        to = gt.to(torch.int64)
        gt = None
        relative_buckets = to * 160
        to = None
        relative_positions = torch.abs(relative_position)
        relative_position = None
        is_small = relative_positions < 80
        float_1 = relative_positions.float()
        truediv = float_1 / 80
        float_1 = None
        relative_positions_if_large = torch.log(truediv)
        truediv = None
        relative_positions_if_large_1 = relative_positions_if_large / 2.302585092994046
        relative_positions_if_large = None
        relative_positions_if_large_2 = relative_positions_if_large_1 * 80
        relative_positions_if_large_1 = None
        add_1 = 80 + relative_positions_if_large_2
        relative_positions_if_large_2 = None
        relative_position_if_large = add_1.to(torch.int64)
        add_1 = None
        full_like = torch.full_like(relative_position_if_large, 159)
        relative_position_if_large_1 = torch.min(relative_position_if_large, full_like)
        relative_position_if_large = full_like = None
        where = torch.where(is_small, relative_positions, relative_position_if_large_1)
        is_small = relative_positions = relative_position_if_large_1 = None
        relative_buckets += where
        relative_buckets_1 = relative_buckets
        relative_buckets = where = None
        relative_position_bucket = relative_buckets_1.to(device(type="cuda", index=0))
        relative_buckets_1 = None
        values = torch.nn.functional.embedding(
            relative_position_bucket,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_,
            None,
            None,
            2.0,
            False,
            False,
        )
        relative_position_bucket = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_ = (None)
        values_1 = values.permute([2, 0, 1])
        values = None
        unsqueeze = values_1.unsqueeze(0)
        values_1 = None
        repeat = unsqueeze.repeat(1, 1, 1, 1)
        unsqueeze = None
        position_bias = repeat.view(2, 1248, 1248)
        repeat = None
        gated_hidden_states = hidden_states_17.view((1, 1248, 2, -1))
        gated_hidden_states_1 = gated_hidden_states.permute(0, 2, 1, 3)
        gated_hidden_states = None
        relative_position_proj = torch._C._nn.linear(
            gated_hidden_states_1,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_1 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_2 = relative_position_proj.view((1, 2, 1248, 2, 4))
        relative_position_proj = None
        relative_position_proj_1 = view_2.sum(-1)
        view_2 = None
        sigmoid = torch.sigmoid(relative_position_proj_1)
        relative_position_proj_1 = None
        chunk = sigmoid.chunk(2, dim=-1)
        sigmoid = None
        gate_a = chunk[0]
        gate_b = chunk[1]
        chunk = None
        mul_2 = (
            gate_b
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_1 = mul_2 - 1.0
        mul_2 = None
        mul_3 = gate_a * sub_1
        gate_a = sub_1 = None
        gate_output = mul_3 + 2.0
        mul_3 = None
        view_3 = gate_output.view(2, -1, 1)
        gate_output = None
        gated_position_bias = view_3 * position_bias
        view_3 = None
        gated_position_bias_1 = gated_position_bias.view((-1, 1248, 1248))
        gated_position_bias = None
        query = hidden_states_17.transpose(0, 1)
        empty = torch.empty([0])
        cat = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward = torch.nn.functional.multi_head_attention_forward(
            query,
            query,
            query,
            16,
            2,
            empty,
            cat,
            None,
            None,
            False,
            0.1,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_1,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_,
        )
        query = (
            empty
        ) = (
            cat
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_1
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output = multi_head_attention_forward[0]
        multi_head_attention_forward = None
        attn_output_1 = attn_output.transpose(0, 1)
        attn_output = None
        hidden_states_18 = torch.nn.functional.dropout(attn_output_1, 0.1, False, False)
        attn_output_1 = None
        hidden_states_19 = hidden_states_17 + hidden_states_18
        hidden_states_17 = hidden_states_18 = None
        hidden_states_20 = torch.nn.functional.layer_norm(
            hidden_states_19,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_19 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_21 = torch._C._nn.linear(
            hidden_states_20,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_22 = torch._C._nn.gelu(hidden_states_21)
        hidden_states_21 = None
        hidden_states_23 = torch.nn.functional.dropout(
            hidden_states_22, 0.1, False, False
        )
        hidden_states_22 = None
        hidden_states_24 = torch._C._nn.linear(
            hidden_states_23,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_23 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_25 = torch.nn.functional.dropout(
            hidden_states_24, 0.1, False, False
        )
        hidden_states_24 = None
        hidden_states_26 = hidden_states_20 + hidden_states_25
        hidden_states_20 = hidden_states_25 = None
        hidden_states_27 = torch.nn.functional.layer_norm(
            hidden_states_26,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_26 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_1 = torch.rand([])
        dropout_probability_1 = None
        gated_hidden_states_2 = hidden_states_27.view((1, 1248, 2, -1))
        gated_hidden_states_3 = gated_hidden_states_2.permute(0, 2, 1, 3)
        gated_hidden_states_2 = None
        relative_position_proj_2 = torch._C._nn.linear(
            gated_hidden_states_3,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_3 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_6 = relative_position_proj_2.view((1, 2, 1248, 2, 4))
        relative_position_proj_2 = None
        relative_position_proj_3 = view_6.sum(-1)
        view_6 = None
        sigmoid_1 = torch.sigmoid(relative_position_proj_3)
        relative_position_proj_3 = None
        chunk_1 = sigmoid_1.chunk(2, dim=-1)
        sigmoid_1 = None
        gate_a_1 = chunk_1[0]
        gate_b_1 = chunk_1[1]
        chunk_1 = None
        mul_5 = (
            gate_b_1
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_1 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_2 = mul_5 - 1.0
        mul_5 = None
        mul_6 = gate_a_1 * sub_2
        gate_a_1 = sub_2 = None
        gate_output_1 = mul_6 + 2.0
        mul_6 = None
        view_7 = gate_output_1.view(2, -1, 1)
        gate_output_1 = None
        gated_position_bias_2 = view_7 * position_bias
        view_7 = None
        gated_position_bias_3 = gated_position_bias_2.view((-1, 1248, 1248))
        gated_position_bias_2 = None
        query_1 = hidden_states_27.transpose(0, 1)
        empty_1 = torch.empty([0])
        cat_1 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_1 = torch.nn.functional.multi_head_attention_forward(
            query_1,
            query_1,
            query_1,
            16,
            2,
            empty_1,
            cat_1,
            None,
            None,
            False,
            0.1,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_3,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_1 = (
            empty_1
        ) = (
            cat_1
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_3
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_2 = multi_head_attention_forward_1[0]
        multi_head_attention_forward_1 = None
        attn_output_3 = attn_output_2.transpose(0, 1)
        attn_output_2 = None
        hidden_states_28 = torch.nn.functional.dropout(attn_output_3, 0.1, False, False)
        attn_output_3 = None
        hidden_states_29 = hidden_states_27 + hidden_states_28
        hidden_states_27 = hidden_states_28 = None
        hidden_states_30 = torch.nn.functional.layer_norm(
            hidden_states_29,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_29 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_31 = torch._C._nn.linear(
            hidden_states_30,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_32 = torch._C._nn.gelu(hidden_states_31)
        hidden_states_31 = None
        hidden_states_33 = torch.nn.functional.dropout(
            hidden_states_32, 0.1, False, False
        )
        hidden_states_32 = None
        hidden_states_34 = torch._C._nn.linear(
            hidden_states_33,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_33 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_35 = torch.nn.functional.dropout(
            hidden_states_34, 0.1, False, False
        )
        hidden_states_34 = None
        hidden_states_36 = hidden_states_30 + hidden_states_35
        hidden_states_30 = hidden_states_35 = None
        hidden_states_37 = torch.nn.functional.layer_norm(
            hidden_states_36,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_36 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_2 = torch.rand([])
        dropout_probability_2 = None
        gated_hidden_states_4 = hidden_states_37.view((1, 1248, 2, -1))
        gated_hidden_states_5 = gated_hidden_states_4.permute(0, 2, 1, 3)
        gated_hidden_states_4 = None
        relative_position_proj_4 = torch._C._nn.linear(
            gated_hidden_states_5,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_5 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_10 = relative_position_proj_4.view((1, 2, 1248, 2, 4))
        relative_position_proj_4 = None
        relative_position_proj_5 = view_10.sum(-1)
        view_10 = None
        sigmoid_2 = torch.sigmoid(relative_position_proj_5)
        relative_position_proj_5 = None
        chunk_2 = sigmoid_2.chunk(2, dim=-1)
        sigmoid_2 = None
        gate_a_2 = chunk_2[0]
        gate_b_2 = chunk_2[1]
        chunk_2 = None
        mul_8 = (
            gate_b_2
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_2 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_3 = mul_8 - 1.0
        mul_8 = None
        mul_9 = gate_a_2 * sub_3
        gate_a_2 = sub_3 = None
        gate_output_2 = mul_9 + 2.0
        mul_9 = None
        view_11 = gate_output_2.view(2, -1, 1)
        gate_output_2 = None
        gated_position_bias_4 = view_11 * position_bias
        view_11 = None
        gated_position_bias_5 = gated_position_bias_4.view((-1, 1248, 1248))
        gated_position_bias_4 = None
        query_2 = hidden_states_37.transpose(0, 1)
        empty_2 = torch.empty([0])
        cat_2 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_2 = torch.nn.functional.multi_head_attention_forward(
            query_2,
            query_2,
            query_2,
            16,
            2,
            empty_2,
            cat_2,
            None,
            None,
            False,
            0.1,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_5,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_2 = (
            empty_2
        ) = (
            cat_2
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_5
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_4 = multi_head_attention_forward_2[0]
        multi_head_attention_forward_2 = None
        attn_output_5 = attn_output_4.transpose(0, 1)
        attn_output_4 = None
        hidden_states_38 = torch.nn.functional.dropout(attn_output_5, 0.1, False, False)
        attn_output_5 = None
        hidden_states_39 = hidden_states_37 + hidden_states_38
        hidden_states_37 = hidden_states_38 = None
        hidden_states_40 = torch.nn.functional.layer_norm(
            hidden_states_39,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_39 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_41 = torch._C._nn.linear(
            hidden_states_40,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_42 = torch._C._nn.gelu(hidden_states_41)
        hidden_states_41 = None
        hidden_states_43 = torch.nn.functional.dropout(
            hidden_states_42, 0.1, False, False
        )
        hidden_states_42 = None
        hidden_states_44 = torch._C._nn.linear(
            hidden_states_43,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_43 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_45 = torch.nn.functional.dropout(
            hidden_states_44, 0.1, False, False
        )
        hidden_states_44 = None
        hidden_states_46 = hidden_states_40 + hidden_states_45
        hidden_states_40 = hidden_states_45 = None
        hidden_states_47 = torch.nn.functional.layer_norm(
            hidden_states_46,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_46 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_3 = torch.rand([])
        dropout_probability_3 = None
        gated_hidden_states_6 = hidden_states_47.view((1, 1248, 2, -1))
        gated_hidden_states_7 = gated_hidden_states_6.permute(0, 2, 1, 3)
        gated_hidden_states_6 = None
        relative_position_proj_6 = torch._C._nn.linear(
            gated_hidden_states_7,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_7 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_14 = relative_position_proj_6.view((1, 2, 1248, 2, 4))
        relative_position_proj_6 = None
        relative_position_proj_7 = view_14.sum(-1)
        view_14 = None
        sigmoid_3 = torch.sigmoid(relative_position_proj_7)
        relative_position_proj_7 = None
        chunk_3 = sigmoid_3.chunk(2, dim=-1)
        sigmoid_3 = None
        gate_a_3 = chunk_3[0]
        gate_b_3 = chunk_3[1]
        chunk_3 = None
        mul_11 = (
            gate_b_3
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_3 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_4 = mul_11 - 1.0
        mul_11 = None
        mul_12 = gate_a_3 * sub_4
        gate_a_3 = sub_4 = None
        gate_output_3 = mul_12 + 2.0
        mul_12 = None
        view_15 = gate_output_3.view(2, -1, 1)
        gate_output_3 = None
        gated_position_bias_6 = view_15 * position_bias
        view_15 = position_bias = None
        gated_position_bias_7 = gated_position_bias_6.view((-1, 1248, 1248))
        gated_position_bias_6 = None
        query_3 = hidden_states_47.transpose(0, 1)
        empty_3 = torch.empty([0])
        cat_3 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_3 = torch.nn.functional.multi_head_attention_forward(
            query_3,
            query_3,
            query_3,
            16,
            2,
            empty_3,
            cat_3,
            None,
            None,
            False,
            0.1,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_7,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_3 = (
            empty_3
        ) = (
            cat_3
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_7
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_6 = multi_head_attention_forward_3[0]
        multi_head_attention_forward_3 = None
        attn_output_7 = attn_output_6.transpose(0, 1)
        attn_output_6 = None
        hidden_states_48 = torch.nn.functional.dropout(attn_output_7, 0.1, False, False)
        attn_output_7 = None
        hidden_states_49 = hidden_states_47 + hidden_states_48
        hidden_states_47 = hidden_states_48 = None
        hidden_states_50 = torch.nn.functional.layer_norm(
            hidden_states_49,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_49 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_51 = torch._C._nn.linear(
            hidden_states_50,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_52 = torch._C._nn.gelu(hidden_states_51)
        hidden_states_51 = None
        hidden_states_53 = torch.nn.functional.dropout(
            hidden_states_52, 0.1, False, False
        )
        hidden_states_52 = None
        hidden_states_54 = torch._C._nn.linear(
            hidden_states_53,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_53 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_55 = torch.nn.functional.dropout(
            hidden_states_54, 0.1, False, False
        )
        hidden_states_54 = None
        hidden_states_56 = hidden_states_50 + hidden_states_55
        hidden_states_50 = hidden_states_55 = None
        hidden_states_57 = torch.nn.functional.layer_norm(
            hidden_states_56,
            (16,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_56 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_ = (None)
        hidden_states_58 = torch._C._nn.linear(
            hidden_states_57,
            l_self_modules_projector_parameters_weight_,
            l_self_modules_projector_parameters_bias_,
        )
        hidden_states_57 = (
            l_self_modules_projector_parameters_weight_
        ) = l_self_modules_projector_parameters_bias_ = None
        pooled_output = hidden_states_58.mean(dim=1)
        hidden_states_58 = None
        logits = torch._C._nn.linear(
            pooled_output,
            l_self_modules_classifier_parameters_weight_,
            l_self_modules_classifier_parameters_bias_,
        )
        pooled_output = (
            l_self_modules_classifier_parameters_weight_
        ) = l_self_modules_classifier_parameters_bias_ = None
        return (logits,)
