class Program_weight_tensor_meta_L_input_values_:
    name = "L_input_values_"
    shape = [1, 80000]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.003
    std = 0.998
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_:
    name = "L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_"
    shape = [32, 1, 8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.027
    std = 0.485
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_"
    shape = [32]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_"
    shape = [32]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_:
    name = "L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_"
    shape = [32, 32, 8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.003
    std = 0.088
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_:
    name = "L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_"
    shape = [32, 32, 8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.089
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_"
    shape = [32]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_"
    shape = [32]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_:
    name = "L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_"
    shape = [16, 32]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.002
    std = 0.102
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_:
    name = "L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.017
    std = 0.106
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_:
    name = "L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_"
    shape = [1, 1, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.578
    std = 0.015
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_:
    name = "L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_"
    shape = [16, 8, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.051
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_"
    shape = [320, 2]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.047
    std = 0.994
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_"
    shape = [8, 8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_"
    shape = [8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_"
    shape = [1, 2, 1, 1]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.021
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.021
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_"
    shape = [20, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_"
    shape = [20]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_"
    shape = [16, 20]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_"
    shape = [8, 8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.004
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_"
    shape = [8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_"
    shape = [1, 2, 1, 1]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.002
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.018
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_"
    shape = [20, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.018
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_"
    shape = [20]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_"
    shape = [16, 20]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.003
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_"
    shape = [8, 8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.021
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_"
    shape = [8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_"
    shape = [1, 2, 1, 1]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.003
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.021
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_"
    shape = [20, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_"
    shape = [20]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_"
    shape = [16, 20]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.021
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_"
    shape = [8, 8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_"
    shape = [8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_"
    shape = [1, 2, 1, 1]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.002
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.021
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_"
    shape = [16, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_"
    shape = [20, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.003
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_"
    shape = [20]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_"
    shape = [16, 20]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_:
    name = "L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_"
    shape = [16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_projector_parameters_weight_:
    name = "L_self_modules_projector_parameters_weight_"
    shape = [256, 16]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_projector_parameters_bias_:
    name = "L_self_modules_projector_parameters_bias_"
    shape = [256]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_classifier_parameters_weight_:
    name = "L_self_modules_classifier_parameters_weight_"
    shape = [2, 256]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_classifier_parameters_bias_:
    name = "L_self_modules_classifier_parameters_bias_"
    shape = [2]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None
