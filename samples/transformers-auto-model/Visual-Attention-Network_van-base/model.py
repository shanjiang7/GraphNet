import torch


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_pixel_values_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_
        l_pixel_values_ = L_pixel_values_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_
        hidden_state = torch.conv2d(
            l_pixel_values_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_,
            (4, 4),
            (3, 3),
            (1, 1),
            1,
        )
        l_pixel_values_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_1 = torch.nn.functional.batch_norm(
            hidden_state,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_2 = torch.nn.functional.batch_norm(
            hidden_state_1,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_1 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_2 = torch._C._nn.gelu(input_1)
        input_1 = None
        hidden_state_3 = torch.conv2d(
            input_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_4 = torch.conv2d(
            hidden_state_3,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            64,
        )
        hidden_state_3 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_5 = torch.conv2d(
            hidden_state_4,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_4 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended = input_2 * hidden_state_5
        input_2 = hidden_state_5 = None
        hidden_state_6 = torch.conv2d(
            attended,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_7 = hidden_state_6 + hidden_state_2
        hidden_state_6 = hidden_state_2 = None
        unsqueeze = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_1 = unsqueeze.unsqueeze(-1)
        unsqueeze = None
        hidden_state_8 = unsqueeze_1 * hidden_state_7
        unsqueeze_1 = hidden_state_7 = None
        hidden_state_9 = hidden_state_1 + hidden_state_8
        hidden_state_1 = hidden_state_8 = None
        hidden_state_10 = torch.nn.functional.batch_norm(
            hidden_state_9,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_11 = torch.conv2d(
            hidden_state_10,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_10 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_12 = torch.conv2d(
            hidden_state_11,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            512,
        )
        hidden_state_11 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_13 = torch._C._nn.gelu(hidden_state_12)
        hidden_state_12 = None
        hidden_state_14 = torch.nn.functional.dropout(
            hidden_state_13, 0.0, False, False
        )
        hidden_state_13 = None
        hidden_state_15 = torch.conv2d(
            hidden_state_14,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_14 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_16 = torch.nn.functional.dropout(
            hidden_state_15, 0.0, False, False
        )
        hidden_state_15 = None
        unsqueeze_2 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_3 = unsqueeze_2.unsqueeze(-1)
        unsqueeze_2 = None
        hidden_state_17 = unsqueeze_3 * hidden_state_16
        unsqueeze_3 = hidden_state_16 = None
        hidden_state_18 = hidden_state_9 + hidden_state_17
        hidden_state_9 = hidden_state_17 = None
        hidden_state_19 = torch.nn.functional.batch_norm(
            hidden_state_18,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = (None)
        input_3 = torch.conv2d(
            hidden_state_19,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_4 = torch._C._nn.gelu(input_3)
        input_3 = None
        hidden_state_20 = torch.conv2d(
            input_4,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_21 = torch.conv2d(
            hidden_state_20,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            64,
        )
        hidden_state_20 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_22 = torch.conv2d(
            hidden_state_21,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_21 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_1 = input_4 * hidden_state_22
        input_4 = hidden_state_22 = None
        hidden_state_23 = torch.conv2d(
            attended_1,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_1 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_24 = hidden_state_23 + hidden_state_19
        hidden_state_23 = hidden_state_19 = None
        unsqueeze_4 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_5 = unsqueeze_4.unsqueeze(-1)
        unsqueeze_4 = None
        hidden_state_25 = unsqueeze_5 * hidden_state_24
        unsqueeze_5 = hidden_state_24 = None
        hidden_state_26 = hidden_state_18 + hidden_state_25
        hidden_state_18 = hidden_state_25 = None
        hidden_state_27 = torch.nn.functional.batch_norm(
            hidden_state_26,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_28 = torch.conv2d(
            hidden_state_27,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_27 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_29 = torch.conv2d(
            hidden_state_28,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            512,
        )
        hidden_state_28 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_30 = torch._C._nn.gelu(hidden_state_29)
        hidden_state_29 = None
        hidden_state_31 = torch.nn.functional.dropout(
            hidden_state_30, 0.0, False, False
        )
        hidden_state_30 = None
        hidden_state_32 = torch.conv2d(
            hidden_state_31,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_31 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_33 = torch.nn.functional.dropout(
            hidden_state_32, 0.0, False, False
        )
        hidden_state_32 = None
        unsqueeze_6 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_7 = unsqueeze_6.unsqueeze(-1)
        unsqueeze_6 = None
        hidden_state_34 = unsqueeze_7 * hidden_state_33
        unsqueeze_7 = hidden_state_33 = None
        hidden_state_35 = hidden_state_26 + hidden_state_34
        hidden_state_26 = hidden_state_34 = None
        hidden_state_36 = torch.nn.functional.batch_norm(
            hidden_state_35,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = (None)
        input_5 = torch.conv2d(
            hidden_state_36,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_6 = torch._C._nn.gelu(input_5)
        input_5 = None
        hidden_state_37 = torch.conv2d(
            input_6,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_38 = torch.conv2d(
            hidden_state_37,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            64,
        )
        hidden_state_37 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_39 = torch.conv2d(
            hidden_state_38,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_38 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_2 = input_6 * hidden_state_39
        input_6 = hidden_state_39 = None
        hidden_state_40 = torch.conv2d(
            attended_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_2 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_41 = hidden_state_40 + hidden_state_36
        hidden_state_40 = hidden_state_36 = None
        unsqueeze_8 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_9 = unsqueeze_8.unsqueeze(-1)
        unsqueeze_8 = None
        hidden_state_42 = unsqueeze_9 * hidden_state_41
        unsqueeze_9 = hidden_state_41 = None
        hidden_state_43 = hidden_state_35 + hidden_state_42
        hidden_state_35 = hidden_state_42 = None
        hidden_state_44 = torch.nn.functional.batch_norm(
            hidden_state_43,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_45 = torch.conv2d(
            hidden_state_44,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_44 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_46 = torch.conv2d(
            hidden_state_45,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            512,
        )
        hidden_state_45 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_47 = torch._C._nn.gelu(hidden_state_46)
        hidden_state_46 = None
        hidden_state_48 = torch.nn.functional.dropout(
            hidden_state_47, 0.0, False, False
        )
        hidden_state_47 = None
        hidden_state_49 = torch.conv2d(
            hidden_state_48,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_48 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_50 = torch.nn.functional.dropout(
            hidden_state_49, 0.0, False, False
        )
        hidden_state_49 = None
        unsqueeze_10 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_11 = unsqueeze_10.unsqueeze(-1)
        unsqueeze_10 = None
        hidden_state_51 = unsqueeze_11 * hidden_state_50
        unsqueeze_11 = hidden_state_50 = None
        hidden_state_52 = hidden_state_43 + hidden_state_51
        hidden_state_43 = hidden_state_51 = None
        flatten = hidden_state_52.flatten(2)
        hidden_state_52 = None
        hidden_state_53 = flatten.transpose(1, 2)
        flatten = None
        hidden_state_54 = torch.nn.functional.layer_norm(
            hidden_state_53,
            (64,),
            l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_53 = l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_ = (None)
        view = hidden_state_54.view(1, 56, 56, 64)
        hidden_state_54 = None
        hidden_state_55 = view.permute(0, 3, 1, 2)
        view = None
        hidden_state_56 = torch.conv2d(
            hidden_state_55,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_55 = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_57 = torch.nn.functional.batch_norm(
            hidden_state_56,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_56 = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_58 = torch.nn.functional.batch_norm(
            hidden_state_57,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_7 = torch.conv2d(
            hidden_state_58,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_8 = torch._C._nn.gelu(input_7)
        input_7 = None
        hidden_state_59 = torch.conv2d(
            input_8,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_60 = torch.conv2d(
            hidden_state_59,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_59 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_61 = torch.conv2d(
            hidden_state_60,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_60 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_3 = input_8 * hidden_state_61
        input_8 = hidden_state_61 = None
        hidden_state_62 = torch.conv2d(
            attended_3,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_3 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_63 = hidden_state_62 + hidden_state_58
        hidden_state_62 = hidden_state_58 = None
        unsqueeze_12 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_13 = unsqueeze_12.unsqueeze(-1)
        unsqueeze_12 = None
        hidden_state_64 = unsqueeze_13 * hidden_state_63
        unsqueeze_13 = hidden_state_63 = None
        hidden_state_65 = hidden_state_57 + hidden_state_64
        hidden_state_57 = hidden_state_64 = None
        hidden_state_66 = torch.nn.functional.batch_norm(
            hidden_state_65,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_67 = torch.conv2d(
            hidden_state_66,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_66 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_68 = torch.conv2d(
            hidden_state_67,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        hidden_state_67 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_69 = torch._C._nn.gelu(hidden_state_68)
        hidden_state_68 = None
        hidden_state_70 = torch.nn.functional.dropout(
            hidden_state_69, 0.0, False, False
        )
        hidden_state_69 = None
        hidden_state_71 = torch.conv2d(
            hidden_state_70,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_70 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_72 = torch.nn.functional.dropout(
            hidden_state_71, 0.0, False, False
        )
        hidden_state_71 = None
        unsqueeze_14 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_15 = unsqueeze_14.unsqueeze(-1)
        unsqueeze_14 = None
        hidden_state_73 = unsqueeze_15 * hidden_state_72
        unsqueeze_15 = hidden_state_72 = None
        hidden_state_74 = hidden_state_65 + hidden_state_73
        hidden_state_65 = hidden_state_73 = None
        hidden_state_75 = torch.nn.functional.batch_norm(
            hidden_state_74,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = (None)
        input_9 = torch.conv2d(
            hidden_state_75,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_10 = torch._C._nn.gelu(input_9)
        input_9 = None
        hidden_state_76 = torch.conv2d(
            input_10,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_77 = torch.conv2d(
            hidden_state_76,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_76 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_78 = torch.conv2d(
            hidden_state_77,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_77 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_4 = input_10 * hidden_state_78
        input_10 = hidden_state_78 = None
        hidden_state_79 = torch.conv2d(
            attended_4,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_4 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_80 = hidden_state_79 + hidden_state_75
        hidden_state_79 = hidden_state_75 = None
        unsqueeze_16 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_17 = unsqueeze_16.unsqueeze(-1)
        unsqueeze_16 = None
        hidden_state_81 = unsqueeze_17 * hidden_state_80
        unsqueeze_17 = hidden_state_80 = None
        hidden_state_82 = hidden_state_74 + hidden_state_81
        hidden_state_74 = hidden_state_81 = None
        hidden_state_83 = torch.nn.functional.batch_norm(
            hidden_state_82,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_84 = torch.conv2d(
            hidden_state_83,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_83 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_85 = torch.conv2d(
            hidden_state_84,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        hidden_state_84 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_86 = torch._C._nn.gelu(hidden_state_85)
        hidden_state_85 = None
        hidden_state_87 = torch.nn.functional.dropout(
            hidden_state_86, 0.0, False, False
        )
        hidden_state_86 = None
        hidden_state_88 = torch.conv2d(
            hidden_state_87,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_87 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_89 = torch.nn.functional.dropout(
            hidden_state_88, 0.0, False, False
        )
        hidden_state_88 = None
        unsqueeze_18 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_19 = unsqueeze_18.unsqueeze(-1)
        unsqueeze_18 = None
        hidden_state_90 = unsqueeze_19 * hidden_state_89
        unsqueeze_19 = hidden_state_89 = None
        hidden_state_91 = hidden_state_82 + hidden_state_90
        hidden_state_82 = hidden_state_90 = None
        hidden_state_92 = torch.nn.functional.batch_norm(
            hidden_state_91,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = (None)
        input_11 = torch.conv2d(
            hidden_state_92,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_12 = torch._C._nn.gelu(input_11)
        input_11 = None
        hidden_state_93 = torch.conv2d(
            input_12,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_94 = torch.conv2d(
            hidden_state_93,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_93 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_95 = torch.conv2d(
            hidden_state_94,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_94 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_5 = input_12 * hidden_state_95
        input_12 = hidden_state_95 = None
        hidden_state_96 = torch.conv2d(
            attended_5,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_5 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_97 = hidden_state_96 + hidden_state_92
        hidden_state_96 = hidden_state_92 = None
        unsqueeze_20 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_21 = unsqueeze_20.unsqueeze(-1)
        unsqueeze_20 = None
        hidden_state_98 = unsqueeze_21 * hidden_state_97
        unsqueeze_21 = hidden_state_97 = None
        hidden_state_99 = hidden_state_91 + hidden_state_98
        hidden_state_91 = hidden_state_98 = None
        hidden_state_100 = torch.nn.functional.batch_norm(
            hidden_state_99,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_101 = torch.conv2d(
            hidden_state_100,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_100 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_102 = torch.conv2d(
            hidden_state_101,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        hidden_state_101 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_103 = torch._C._nn.gelu(hidden_state_102)
        hidden_state_102 = None
        hidden_state_104 = torch.nn.functional.dropout(
            hidden_state_103, 0.0, False, False
        )
        hidden_state_103 = None
        hidden_state_105 = torch.conv2d(
            hidden_state_104,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_104 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_106 = torch.nn.functional.dropout(
            hidden_state_105, 0.0, False, False
        )
        hidden_state_105 = None
        unsqueeze_22 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_23 = unsqueeze_22.unsqueeze(-1)
        unsqueeze_22 = None
        hidden_state_107 = unsqueeze_23 * hidden_state_106
        unsqueeze_23 = hidden_state_106 = None
        hidden_state_108 = hidden_state_99 + hidden_state_107
        hidden_state_99 = hidden_state_107 = None
        flatten_1 = hidden_state_108.flatten(2)
        hidden_state_108 = None
        hidden_state_109 = flatten_1.transpose(1, 2)
        flatten_1 = None
        hidden_state_110 = torch.nn.functional.layer_norm(
            hidden_state_109,
            (128,),
            l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_109 = l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_ = (None)
        view_1 = hidden_state_110.view(1, 28, 28, 128)
        hidden_state_110 = None
        hidden_state_111 = view_1.permute(0, 3, 1, 2)
        view_1 = None
        hidden_state_112 = torch.conv2d(
            hidden_state_111,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_111 = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_113 = torch.nn.functional.batch_norm(
            hidden_state_112,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_112 = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_114 = torch.nn.functional.batch_norm(
            hidden_state_113,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_13 = torch.conv2d(
            hidden_state_114,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_14 = torch._C._nn.gelu(input_13)
        input_13 = None
        hidden_state_115 = torch.conv2d(
            input_14,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_116 = torch.conv2d(
            hidden_state_115,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_115 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_117 = torch.conv2d(
            hidden_state_116,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_116 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_6 = input_14 * hidden_state_117
        input_14 = hidden_state_117 = None
        hidden_state_118 = torch.conv2d(
            attended_6,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_6 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_119 = hidden_state_118 + hidden_state_114
        hidden_state_118 = hidden_state_114 = None
        unsqueeze_24 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_25 = unsqueeze_24.unsqueeze(-1)
        unsqueeze_24 = None
        hidden_state_120 = unsqueeze_25 * hidden_state_119
        unsqueeze_25 = hidden_state_119 = None
        hidden_state_121 = hidden_state_113 + hidden_state_120
        hidden_state_113 = hidden_state_120 = None
        hidden_state_122 = torch.nn.functional.batch_norm(
            hidden_state_121,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_123 = torch.conv2d(
            hidden_state_122,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_122 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_124 = torch.conv2d(
            hidden_state_123,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_123 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_125 = torch._C._nn.gelu(hidden_state_124)
        hidden_state_124 = None
        hidden_state_126 = torch.nn.functional.dropout(
            hidden_state_125, 0.0, False, False
        )
        hidden_state_125 = None
        hidden_state_127 = torch.conv2d(
            hidden_state_126,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_126 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_128 = torch.nn.functional.dropout(
            hidden_state_127, 0.0, False, False
        )
        hidden_state_127 = None
        unsqueeze_26 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_27 = unsqueeze_26.unsqueeze(-1)
        unsqueeze_26 = None
        hidden_state_129 = unsqueeze_27 * hidden_state_128
        unsqueeze_27 = hidden_state_128 = None
        hidden_state_130 = hidden_state_121 + hidden_state_129
        hidden_state_121 = hidden_state_129 = None
        hidden_state_131 = torch.nn.functional.batch_norm(
            hidden_state_130,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = (None)
        input_15 = torch.conv2d(
            hidden_state_131,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_16 = torch._C._nn.gelu(input_15)
        input_15 = None
        hidden_state_132 = torch.conv2d(
            input_16,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_133 = torch.conv2d(
            hidden_state_132,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_132 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_134 = torch.conv2d(
            hidden_state_133,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_133 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_7 = input_16 * hidden_state_134
        input_16 = hidden_state_134 = None
        hidden_state_135 = torch.conv2d(
            attended_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_136 = hidden_state_135 + hidden_state_131
        hidden_state_135 = hidden_state_131 = None
        unsqueeze_28 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_29 = unsqueeze_28.unsqueeze(-1)
        unsqueeze_28 = None
        hidden_state_137 = unsqueeze_29 * hidden_state_136
        unsqueeze_29 = hidden_state_136 = None
        hidden_state_138 = hidden_state_130 + hidden_state_137
        hidden_state_130 = hidden_state_137 = None
        hidden_state_139 = torch.nn.functional.batch_norm(
            hidden_state_138,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_140 = torch.conv2d(
            hidden_state_139,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_139 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_141 = torch.conv2d(
            hidden_state_140,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_140 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_142 = torch._C._nn.gelu(hidden_state_141)
        hidden_state_141 = None
        hidden_state_143 = torch.nn.functional.dropout(
            hidden_state_142, 0.0, False, False
        )
        hidden_state_142 = None
        hidden_state_144 = torch.conv2d(
            hidden_state_143,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_143 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_145 = torch.nn.functional.dropout(
            hidden_state_144, 0.0, False, False
        )
        hidden_state_144 = None
        unsqueeze_30 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_31 = unsqueeze_30.unsqueeze(-1)
        unsqueeze_30 = None
        hidden_state_146 = unsqueeze_31 * hidden_state_145
        unsqueeze_31 = hidden_state_145 = None
        hidden_state_147 = hidden_state_138 + hidden_state_146
        hidden_state_138 = hidden_state_146 = None
        hidden_state_148 = torch.nn.functional.batch_norm(
            hidden_state_147,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = (None)
        input_17 = torch.conv2d(
            hidden_state_148,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_18 = torch._C._nn.gelu(input_17)
        input_17 = None
        hidden_state_149 = torch.conv2d(
            input_18,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_150 = torch.conv2d(
            hidden_state_149,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_149 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_151 = torch.conv2d(
            hidden_state_150,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_150 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_8 = input_18 * hidden_state_151
        input_18 = hidden_state_151 = None
        hidden_state_152 = torch.conv2d(
            attended_8,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_8 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_153 = hidden_state_152 + hidden_state_148
        hidden_state_152 = hidden_state_148 = None
        unsqueeze_32 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_33 = unsqueeze_32.unsqueeze(-1)
        unsqueeze_32 = None
        hidden_state_154 = unsqueeze_33 * hidden_state_153
        unsqueeze_33 = hidden_state_153 = None
        hidden_state_155 = hidden_state_147 + hidden_state_154
        hidden_state_147 = hidden_state_154 = None
        hidden_state_156 = torch.nn.functional.batch_norm(
            hidden_state_155,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_157 = torch.conv2d(
            hidden_state_156,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_156 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_158 = torch.conv2d(
            hidden_state_157,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_157 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_159 = torch._C._nn.gelu(hidden_state_158)
        hidden_state_158 = None
        hidden_state_160 = torch.nn.functional.dropout(
            hidden_state_159, 0.0, False, False
        )
        hidden_state_159 = None
        hidden_state_161 = torch.conv2d(
            hidden_state_160,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_160 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_162 = torch.nn.functional.dropout(
            hidden_state_161, 0.0, False, False
        )
        hidden_state_161 = None
        unsqueeze_34 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_35 = unsqueeze_34.unsqueeze(-1)
        unsqueeze_34 = None
        hidden_state_163 = unsqueeze_35 * hidden_state_162
        unsqueeze_35 = hidden_state_162 = None
        hidden_state_164 = hidden_state_155 + hidden_state_163
        hidden_state_155 = hidden_state_163 = None
        hidden_state_165 = torch.nn.functional.batch_norm(
            hidden_state_164,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_ = (None)
        input_19 = torch.conv2d(
            hidden_state_165,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_20 = torch._C._nn.gelu(input_19)
        input_19 = None
        hidden_state_166 = torch.conv2d(
            input_20,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_167 = torch.conv2d(
            hidden_state_166,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_166 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_168 = torch.conv2d(
            hidden_state_167,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_167 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_9 = input_20 * hidden_state_168
        input_20 = hidden_state_168 = None
        hidden_state_169 = torch.conv2d(
            attended_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_170 = hidden_state_169 + hidden_state_165
        hidden_state_169 = hidden_state_165 = None
        unsqueeze_36 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_37 = unsqueeze_36.unsqueeze(-1)
        unsqueeze_36 = None
        hidden_state_171 = unsqueeze_37 * hidden_state_170
        unsqueeze_37 = hidden_state_170 = None
        hidden_state_172 = hidden_state_164 + hidden_state_171
        hidden_state_164 = hidden_state_171 = None
        hidden_state_173 = torch.nn.functional.batch_norm(
            hidden_state_172,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_174 = torch.conv2d(
            hidden_state_173,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_173 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_175 = torch.conv2d(
            hidden_state_174,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_174 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_176 = torch._C._nn.gelu(hidden_state_175)
        hidden_state_175 = None
        hidden_state_177 = torch.nn.functional.dropout(
            hidden_state_176, 0.0, False, False
        )
        hidden_state_176 = None
        hidden_state_178 = torch.conv2d(
            hidden_state_177,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_177 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_179 = torch.nn.functional.dropout(
            hidden_state_178, 0.0, False, False
        )
        hidden_state_178 = None
        unsqueeze_38 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_39 = unsqueeze_38.unsqueeze(-1)
        unsqueeze_38 = None
        hidden_state_180 = unsqueeze_39 * hidden_state_179
        unsqueeze_39 = hidden_state_179 = None
        hidden_state_181 = hidden_state_172 + hidden_state_180
        hidden_state_172 = hidden_state_180 = None
        hidden_state_182 = torch.nn.functional.batch_norm(
            hidden_state_181,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_ = (None)
        input_21 = torch.conv2d(
            hidden_state_182,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_22 = torch._C._nn.gelu(input_21)
        input_21 = None
        hidden_state_183 = torch.conv2d(
            input_22,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_184 = torch.conv2d(
            hidden_state_183,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_183 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_185 = torch.conv2d(
            hidden_state_184,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_184 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_10 = input_22 * hidden_state_185
        input_22 = hidden_state_185 = None
        hidden_state_186 = torch.conv2d(
            attended_10,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_10 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_187 = hidden_state_186 + hidden_state_182
        hidden_state_186 = hidden_state_182 = None
        unsqueeze_40 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_41 = unsqueeze_40.unsqueeze(-1)
        unsqueeze_40 = None
        hidden_state_188 = unsqueeze_41 * hidden_state_187
        unsqueeze_41 = hidden_state_187 = None
        hidden_state_189 = hidden_state_181 + hidden_state_188
        hidden_state_181 = hidden_state_188 = None
        hidden_state_190 = torch.nn.functional.batch_norm(
            hidden_state_189,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_191 = torch.conv2d(
            hidden_state_190,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_190 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_192 = torch.conv2d(
            hidden_state_191,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_191 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_193 = torch._C._nn.gelu(hidden_state_192)
        hidden_state_192 = None
        hidden_state_194 = torch.nn.functional.dropout(
            hidden_state_193, 0.0, False, False
        )
        hidden_state_193 = None
        hidden_state_195 = torch.conv2d(
            hidden_state_194,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_194 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_196 = torch.nn.functional.dropout(
            hidden_state_195, 0.0, False, False
        )
        hidden_state_195 = None
        unsqueeze_42 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_43 = unsqueeze_42.unsqueeze(-1)
        unsqueeze_42 = None
        hidden_state_197 = unsqueeze_43 * hidden_state_196
        unsqueeze_43 = hidden_state_196 = None
        hidden_state_198 = hidden_state_189 + hidden_state_197
        hidden_state_189 = hidden_state_197 = None
        hidden_state_199 = torch.nn.functional.batch_norm(
            hidden_state_198,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_ = (None)
        input_23 = torch.conv2d(
            hidden_state_199,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_24 = torch._C._nn.gelu(input_23)
        input_23 = None
        hidden_state_200 = torch.conv2d(
            input_24,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_201 = torch.conv2d(
            hidden_state_200,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_200 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_202 = torch.conv2d(
            hidden_state_201,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_201 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_11 = input_24 * hidden_state_202
        input_24 = hidden_state_202 = None
        hidden_state_203 = torch.conv2d(
            attended_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_204 = hidden_state_203 + hidden_state_199
        hidden_state_203 = hidden_state_199 = None
        unsqueeze_44 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_45 = unsqueeze_44.unsqueeze(-1)
        unsqueeze_44 = None
        hidden_state_205 = unsqueeze_45 * hidden_state_204
        unsqueeze_45 = hidden_state_204 = None
        hidden_state_206 = hidden_state_198 + hidden_state_205
        hidden_state_198 = hidden_state_205 = None
        hidden_state_207 = torch.nn.functional.batch_norm(
            hidden_state_206,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_208 = torch.conv2d(
            hidden_state_207,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_207 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_209 = torch.conv2d(
            hidden_state_208,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_208 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_210 = torch._C._nn.gelu(hidden_state_209)
        hidden_state_209 = None
        hidden_state_211 = torch.nn.functional.dropout(
            hidden_state_210, 0.0, False, False
        )
        hidden_state_210 = None
        hidden_state_212 = torch.conv2d(
            hidden_state_211,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_211 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_213 = torch.nn.functional.dropout(
            hidden_state_212, 0.0, False, False
        )
        hidden_state_212 = None
        unsqueeze_46 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_47 = unsqueeze_46.unsqueeze(-1)
        unsqueeze_46 = None
        hidden_state_214 = unsqueeze_47 * hidden_state_213
        unsqueeze_47 = hidden_state_213 = None
        hidden_state_215 = hidden_state_206 + hidden_state_214
        hidden_state_206 = hidden_state_214 = None
        hidden_state_216 = torch.nn.functional.batch_norm(
            hidden_state_215,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_ = (None)
        input_25 = torch.conv2d(
            hidden_state_216,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_26 = torch._C._nn.gelu(input_25)
        input_25 = None
        hidden_state_217 = torch.conv2d(
            input_26,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_218 = torch.conv2d(
            hidden_state_217,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_217 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_219 = torch.conv2d(
            hidden_state_218,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_218 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_12 = input_26 * hidden_state_219
        input_26 = hidden_state_219 = None
        hidden_state_220 = torch.conv2d(
            attended_12,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_12 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_221 = hidden_state_220 + hidden_state_216
        hidden_state_220 = hidden_state_216 = None
        unsqueeze_48 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_49 = unsqueeze_48.unsqueeze(-1)
        unsqueeze_48 = None
        hidden_state_222 = unsqueeze_49 * hidden_state_221
        unsqueeze_49 = hidden_state_221 = None
        hidden_state_223 = hidden_state_215 + hidden_state_222
        hidden_state_215 = hidden_state_222 = None
        hidden_state_224 = torch.nn.functional.batch_norm(
            hidden_state_223,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_225 = torch.conv2d(
            hidden_state_224,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_224 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_226 = torch.conv2d(
            hidden_state_225,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_225 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_227 = torch._C._nn.gelu(hidden_state_226)
        hidden_state_226 = None
        hidden_state_228 = torch.nn.functional.dropout(
            hidden_state_227, 0.0, False, False
        )
        hidden_state_227 = None
        hidden_state_229 = torch.conv2d(
            hidden_state_228,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_228 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_230 = torch.nn.functional.dropout(
            hidden_state_229, 0.0, False, False
        )
        hidden_state_229 = None
        unsqueeze_50 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_51 = unsqueeze_50.unsqueeze(-1)
        unsqueeze_50 = None
        hidden_state_231 = unsqueeze_51 * hidden_state_230
        unsqueeze_51 = hidden_state_230 = None
        hidden_state_232 = hidden_state_223 + hidden_state_231
        hidden_state_223 = hidden_state_231 = None
        hidden_state_233 = torch.nn.functional.batch_norm(
            hidden_state_232,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_ = (None)
        input_27 = torch.conv2d(
            hidden_state_233,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_28 = torch._C._nn.gelu(input_27)
        input_27 = None
        hidden_state_234 = torch.conv2d(
            input_28,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_235 = torch.conv2d(
            hidden_state_234,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_234 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_236 = torch.conv2d(
            hidden_state_235,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_235 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_13 = input_28 * hidden_state_236
        input_28 = hidden_state_236 = None
        hidden_state_237 = torch.conv2d(
            attended_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_238 = hidden_state_237 + hidden_state_233
        hidden_state_237 = hidden_state_233 = None
        unsqueeze_52 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_53 = unsqueeze_52.unsqueeze(-1)
        unsqueeze_52 = None
        hidden_state_239 = unsqueeze_53 * hidden_state_238
        unsqueeze_53 = hidden_state_238 = None
        hidden_state_240 = hidden_state_232 + hidden_state_239
        hidden_state_232 = hidden_state_239 = None
        hidden_state_241 = torch.nn.functional.batch_norm(
            hidden_state_240,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_242 = torch.conv2d(
            hidden_state_241,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_241 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_243 = torch.conv2d(
            hidden_state_242,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_242 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_244 = torch._C._nn.gelu(hidden_state_243)
        hidden_state_243 = None
        hidden_state_245 = torch.nn.functional.dropout(
            hidden_state_244, 0.0, False, False
        )
        hidden_state_244 = None
        hidden_state_246 = torch.conv2d(
            hidden_state_245,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_245 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_247 = torch.nn.functional.dropout(
            hidden_state_246, 0.0, False, False
        )
        hidden_state_246 = None
        unsqueeze_54 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_55 = unsqueeze_54.unsqueeze(-1)
        unsqueeze_54 = None
        hidden_state_248 = unsqueeze_55 * hidden_state_247
        unsqueeze_55 = hidden_state_247 = None
        hidden_state_249 = hidden_state_240 + hidden_state_248
        hidden_state_240 = hidden_state_248 = None
        hidden_state_250 = torch.nn.functional.batch_norm(
            hidden_state_249,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_ = (None)
        input_29 = torch.conv2d(
            hidden_state_250,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_30 = torch._C._nn.gelu(input_29)
        input_29 = None
        hidden_state_251 = torch.conv2d(
            input_30,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_252 = torch.conv2d(
            hidden_state_251,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_251 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_253 = torch.conv2d(
            hidden_state_252,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_252 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_14 = input_30 * hidden_state_253
        input_30 = hidden_state_253 = None
        hidden_state_254 = torch.conv2d(
            attended_14,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_14 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_255 = hidden_state_254 + hidden_state_250
        hidden_state_254 = hidden_state_250 = None
        unsqueeze_56 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_57 = unsqueeze_56.unsqueeze(-1)
        unsqueeze_56 = None
        hidden_state_256 = unsqueeze_57 * hidden_state_255
        unsqueeze_57 = hidden_state_255 = None
        hidden_state_257 = hidden_state_249 + hidden_state_256
        hidden_state_249 = hidden_state_256 = None
        hidden_state_258 = torch.nn.functional.batch_norm(
            hidden_state_257,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_259 = torch.conv2d(
            hidden_state_258,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_258 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_260 = torch.conv2d(
            hidden_state_259,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_259 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_261 = torch._C._nn.gelu(hidden_state_260)
        hidden_state_260 = None
        hidden_state_262 = torch.nn.functional.dropout(
            hidden_state_261, 0.0, False, False
        )
        hidden_state_261 = None
        hidden_state_263 = torch.conv2d(
            hidden_state_262,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_262 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_264 = torch.nn.functional.dropout(
            hidden_state_263, 0.0, False, False
        )
        hidden_state_263 = None
        unsqueeze_58 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_59 = unsqueeze_58.unsqueeze(-1)
        unsqueeze_58 = None
        hidden_state_265 = unsqueeze_59 * hidden_state_264
        unsqueeze_59 = hidden_state_264 = None
        hidden_state_266 = hidden_state_257 + hidden_state_265
        hidden_state_257 = hidden_state_265 = None
        hidden_state_267 = torch.nn.functional.batch_norm(
            hidden_state_266,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_ = (None)
        input_31 = torch.conv2d(
            hidden_state_267,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_32 = torch._C._nn.gelu(input_31)
        input_31 = None
        hidden_state_268 = torch.conv2d(
            input_32,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_269 = torch.conv2d(
            hidden_state_268,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_268 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_270 = torch.conv2d(
            hidden_state_269,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_269 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_15 = input_32 * hidden_state_270
        input_32 = hidden_state_270 = None
        hidden_state_271 = torch.conv2d(
            attended_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_272 = hidden_state_271 + hidden_state_267
        hidden_state_271 = hidden_state_267 = None
        unsqueeze_60 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_61 = unsqueeze_60.unsqueeze(-1)
        unsqueeze_60 = None
        hidden_state_273 = unsqueeze_61 * hidden_state_272
        unsqueeze_61 = hidden_state_272 = None
        hidden_state_274 = hidden_state_266 + hidden_state_273
        hidden_state_266 = hidden_state_273 = None
        hidden_state_275 = torch.nn.functional.batch_norm(
            hidden_state_274,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_276 = torch.conv2d(
            hidden_state_275,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_275 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_277 = torch.conv2d(
            hidden_state_276,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_276 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_278 = torch._C._nn.gelu(hidden_state_277)
        hidden_state_277 = None
        hidden_state_279 = torch.nn.functional.dropout(
            hidden_state_278, 0.0, False, False
        )
        hidden_state_278 = None
        hidden_state_280 = torch.conv2d(
            hidden_state_279,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_279 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_281 = torch.nn.functional.dropout(
            hidden_state_280, 0.0, False, False
        )
        hidden_state_280 = None
        unsqueeze_62 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_63 = unsqueeze_62.unsqueeze(-1)
        unsqueeze_62 = None
        hidden_state_282 = unsqueeze_63 * hidden_state_281
        unsqueeze_63 = hidden_state_281 = None
        hidden_state_283 = hidden_state_274 + hidden_state_282
        hidden_state_274 = hidden_state_282 = None
        hidden_state_284 = torch.nn.functional.batch_norm(
            hidden_state_283,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_ = (None)
        input_33 = torch.conv2d(
            hidden_state_284,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_34 = torch._C._nn.gelu(input_33)
        input_33 = None
        hidden_state_285 = torch.conv2d(
            input_34,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_286 = torch.conv2d(
            hidden_state_285,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_285 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_287 = torch.conv2d(
            hidden_state_286,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_286 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_16 = input_34 * hidden_state_287
        input_34 = hidden_state_287 = None
        hidden_state_288 = torch.conv2d(
            attended_16,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_16 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_289 = hidden_state_288 + hidden_state_284
        hidden_state_288 = hidden_state_284 = None
        unsqueeze_64 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_65 = unsqueeze_64.unsqueeze(-1)
        unsqueeze_64 = None
        hidden_state_290 = unsqueeze_65 * hidden_state_289
        unsqueeze_65 = hidden_state_289 = None
        hidden_state_291 = hidden_state_283 + hidden_state_290
        hidden_state_283 = hidden_state_290 = None
        hidden_state_292 = torch.nn.functional.batch_norm(
            hidden_state_291,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_293 = torch.conv2d(
            hidden_state_292,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_292 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_294 = torch.conv2d(
            hidden_state_293,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_293 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_295 = torch._C._nn.gelu(hidden_state_294)
        hidden_state_294 = None
        hidden_state_296 = torch.nn.functional.dropout(
            hidden_state_295, 0.0, False, False
        )
        hidden_state_295 = None
        hidden_state_297 = torch.conv2d(
            hidden_state_296,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_296 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_298 = torch.nn.functional.dropout(
            hidden_state_297, 0.0, False, False
        )
        hidden_state_297 = None
        unsqueeze_66 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_67 = unsqueeze_66.unsqueeze(-1)
        unsqueeze_66 = None
        hidden_state_299 = unsqueeze_67 * hidden_state_298
        unsqueeze_67 = hidden_state_298 = None
        hidden_state_300 = hidden_state_291 + hidden_state_299
        hidden_state_291 = hidden_state_299 = None
        hidden_state_301 = torch.nn.functional.batch_norm(
            hidden_state_300,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_ = (None)
        input_35 = torch.conv2d(
            hidden_state_301,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_36 = torch._C._nn.gelu(input_35)
        input_35 = None
        hidden_state_302 = torch.conv2d(
            input_36,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_303 = torch.conv2d(
            hidden_state_302,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_302 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_304 = torch.conv2d(
            hidden_state_303,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_303 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_17 = input_36 * hidden_state_304
        input_36 = hidden_state_304 = None
        hidden_state_305 = torch.conv2d(
            attended_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_306 = hidden_state_305 + hidden_state_301
        hidden_state_305 = hidden_state_301 = None
        unsqueeze_68 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_69 = unsqueeze_68.unsqueeze(-1)
        unsqueeze_68 = None
        hidden_state_307 = unsqueeze_69 * hidden_state_306
        unsqueeze_69 = hidden_state_306 = None
        hidden_state_308 = hidden_state_300 + hidden_state_307
        hidden_state_300 = hidden_state_307 = None
        hidden_state_309 = torch.nn.functional.batch_norm(
            hidden_state_308,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_310 = torch.conv2d(
            hidden_state_309,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_309 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_311 = torch.conv2d(
            hidden_state_310,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_310 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_312 = torch._C._nn.gelu(hidden_state_311)
        hidden_state_311 = None
        hidden_state_313 = torch.nn.functional.dropout(
            hidden_state_312, 0.0, False, False
        )
        hidden_state_312 = None
        hidden_state_314 = torch.conv2d(
            hidden_state_313,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_313 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_315 = torch.nn.functional.dropout(
            hidden_state_314, 0.0, False, False
        )
        hidden_state_314 = None
        unsqueeze_70 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_71 = unsqueeze_70.unsqueeze(-1)
        unsqueeze_70 = None
        hidden_state_316 = unsqueeze_71 * hidden_state_315
        unsqueeze_71 = hidden_state_315 = None
        hidden_state_317 = hidden_state_308 + hidden_state_316
        hidden_state_308 = hidden_state_316 = None
        flatten_2 = hidden_state_317.flatten(2)
        hidden_state_317 = None
        hidden_state_318 = flatten_2.transpose(1, 2)
        flatten_2 = None
        hidden_state_319 = torch.nn.functional.layer_norm(
            hidden_state_318,
            (320,),
            l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_318 = l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_ = (None)
        view_2 = hidden_state_319.view(1, 14, 14, 320)
        hidden_state_319 = None
        hidden_state_320 = view_2.permute(0, 3, 1, 2)
        view_2 = None
        hidden_state_321 = torch.conv2d(
            hidden_state_320,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_320 = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_322 = torch.nn.functional.batch_norm(
            hidden_state_321,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_321 = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_323 = torch.nn.functional.batch_norm(
            hidden_state_322,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_37 = torch.conv2d(
            hidden_state_323,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_38 = torch._C._nn.gelu(input_37)
        input_37 = None
        hidden_state_324 = torch.conv2d(
            input_38,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            512,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_325 = torch.conv2d(
            hidden_state_324,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            512,
        )
        hidden_state_324 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_326 = torch.conv2d(
            hidden_state_325,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_325 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_18 = input_38 * hidden_state_326
        input_38 = hidden_state_326 = None
        hidden_state_327 = torch.conv2d(
            attended_18,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_18 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_328 = hidden_state_327 + hidden_state_323
        hidden_state_327 = hidden_state_323 = None
        unsqueeze_72 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_73 = unsqueeze_72.unsqueeze(-1)
        unsqueeze_72 = None
        hidden_state_329 = unsqueeze_73 * hidden_state_328
        unsqueeze_73 = hidden_state_328 = None
        hidden_state_330 = hidden_state_322 + hidden_state_329
        hidden_state_322 = hidden_state_329 = None
        hidden_state_331 = torch.nn.functional.batch_norm(
            hidden_state_330,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_332 = torch.conv2d(
            hidden_state_331,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_331 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_333 = torch.conv2d(
            hidden_state_332,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            2048,
        )
        hidden_state_332 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_334 = torch._C._nn.gelu(hidden_state_333)
        hidden_state_333 = None
        hidden_state_335 = torch.nn.functional.dropout(
            hidden_state_334, 0.0, False, False
        )
        hidden_state_334 = None
        hidden_state_336 = torch.conv2d(
            hidden_state_335,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_335 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_337 = torch.nn.functional.dropout(
            hidden_state_336, 0.0, False, False
        )
        hidden_state_336 = None
        unsqueeze_74 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_75 = unsqueeze_74.unsqueeze(-1)
        unsqueeze_74 = None
        hidden_state_338 = unsqueeze_75 * hidden_state_337
        unsqueeze_75 = hidden_state_337 = None
        hidden_state_339 = hidden_state_330 + hidden_state_338
        hidden_state_330 = hidden_state_338 = None
        hidden_state_340 = torch.nn.functional.batch_norm(
            hidden_state_339,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = (None)
        input_39 = torch.conv2d(
            hidden_state_340,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_40 = torch._C._nn.gelu(input_39)
        input_39 = None
        hidden_state_341 = torch.conv2d(
            input_40,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            512,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_342 = torch.conv2d(
            hidden_state_341,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            512,
        )
        hidden_state_341 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_343 = torch.conv2d(
            hidden_state_342,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_342 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_19 = input_40 * hidden_state_343
        input_40 = hidden_state_343 = None
        hidden_state_344 = torch.conv2d(
            attended_19,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_19 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_345 = hidden_state_344 + hidden_state_340
        hidden_state_344 = hidden_state_340 = None
        unsqueeze_76 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_77 = unsqueeze_76.unsqueeze(-1)
        unsqueeze_76 = None
        hidden_state_346 = unsqueeze_77 * hidden_state_345
        unsqueeze_77 = hidden_state_345 = None
        hidden_state_347 = hidden_state_339 + hidden_state_346
        hidden_state_339 = hidden_state_346 = None
        hidden_state_348 = torch.nn.functional.batch_norm(
            hidden_state_347,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_349 = torch.conv2d(
            hidden_state_348,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_348 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_350 = torch.conv2d(
            hidden_state_349,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            2048,
        )
        hidden_state_349 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_351 = torch._C._nn.gelu(hidden_state_350)
        hidden_state_350 = None
        hidden_state_352 = torch.nn.functional.dropout(
            hidden_state_351, 0.0, False, False
        )
        hidden_state_351 = None
        hidden_state_353 = torch.conv2d(
            hidden_state_352,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_352 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_354 = torch.nn.functional.dropout(
            hidden_state_353, 0.0, False, False
        )
        hidden_state_353 = None
        unsqueeze_78 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_79 = unsqueeze_78.unsqueeze(-1)
        unsqueeze_78 = None
        hidden_state_355 = unsqueeze_79 * hidden_state_354
        unsqueeze_79 = hidden_state_354 = None
        hidden_state_356 = hidden_state_347 + hidden_state_355
        hidden_state_347 = hidden_state_355 = None
        hidden_state_357 = torch.nn.functional.batch_norm(
            hidden_state_356,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = (None)
        input_41 = torch.conv2d(
            hidden_state_357,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_42 = torch._C._nn.gelu(input_41)
        input_41 = None
        hidden_state_358 = torch.conv2d(
            input_42,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            512,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_359 = torch.conv2d(
            hidden_state_358,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            512,
        )
        hidden_state_358 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_360 = torch.conv2d(
            hidden_state_359,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_359 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_20 = input_42 * hidden_state_360
        input_42 = hidden_state_360 = None
        hidden_state_361 = torch.conv2d(
            attended_20,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_20 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_362 = hidden_state_361 + hidden_state_357
        hidden_state_361 = hidden_state_357 = None
        unsqueeze_80 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_81 = unsqueeze_80.unsqueeze(-1)
        unsqueeze_80 = None
        hidden_state_363 = unsqueeze_81 * hidden_state_362
        unsqueeze_81 = hidden_state_362 = None
        hidden_state_364 = hidden_state_356 + hidden_state_363
        hidden_state_356 = hidden_state_363 = None
        hidden_state_365 = torch.nn.functional.batch_norm(
            hidden_state_364,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_366 = torch.conv2d(
            hidden_state_365,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_365 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_367 = torch.conv2d(
            hidden_state_366,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            2048,
        )
        hidden_state_366 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_368 = torch._C._nn.gelu(hidden_state_367)
        hidden_state_367 = None
        hidden_state_369 = torch.nn.functional.dropout(
            hidden_state_368, 0.0, False, False
        )
        hidden_state_368 = None
        hidden_state_370 = torch.conv2d(
            hidden_state_369,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_369 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_371 = torch.nn.functional.dropout(
            hidden_state_370, 0.0, False, False
        )
        hidden_state_370 = None
        unsqueeze_82 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_83 = unsqueeze_82.unsqueeze(-1)
        unsqueeze_82 = None
        hidden_state_372 = unsqueeze_83 * hidden_state_371
        unsqueeze_83 = hidden_state_371 = None
        hidden_state_373 = hidden_state_364 + hidden_state_372
        hidden_state_364 = hidden_state_372 = None
        flatten_3 = hidden_state_373.flatten(2)
        hidden_state_373 = None
        hidden_state_374 = flatten_3.transpose(1, 2)
        flatten_3 = None
        hidden_state_375 = torch.nn.functional.layer_norm(
            hidden_state_374,
            (512,),
            l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_374 = l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_ = (None)
        view_3 = hidden_state_375.view(1, 7, 7, 512)
        hidden_state_375 = None
        hidden_state_376 = view_3.permute(0, 3, 1, 2)
        view_3 = None
        pooled_output = hidden_state_376.mean(dim=[-2, -1])
        return (hidden_state_376, pooled_output)
