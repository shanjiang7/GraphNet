import torch


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_pixel_values_: torch.Tensor,
        L_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_embeddings_parameters_cls_token_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_embeddings_parameters_position_embeddings_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_embeddings_parameters_time_embeddings_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_layernorm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_timesformer_modules_layernorm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_pixel_values_ = L_pixel_values_
        l_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_weight_ = L_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_weight_
        l_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_bias_ = L_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_bias_
        l_self_modules_timesformer_modules_embeddings_parameters_cls_token_ = (
            L_self_modules_timesformer_modules_embeddings_parameters_cls_token_
        )
        l_self_modules_timesformer_modules_embeddings_parameters_position_embeddings_ = L_self_modules_timesformer_modules_embeddings_parameters_position_embeddings_
        l_self_modules_timesformer_modules_embeddings_parameters_time_embeddings_ = (
            L_self_modules_timesformer_modules_embeddings_parameters_time_embeddings_
        )
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ = L_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_
        l_self_modules_timesformer_modules_layernorm_parameters_weight_ = (
            L_self_modules_timesformer_modules_layernorm_parameters_weight_
        )
        l_self_modules_timesformer_modules_layernorm_parameters_bias_ = (
            L_self_modules_timesformer_modules_layernorm_parameters_bias_
        )
        l_self_modules_classifier_parameters_weight_ = (
            L_self_modules_classifier_parameters_weight_
        )
        l_self_modules_classifier_parameters_bias_ = (
            L_self_modules_classifier_parameters_bias_
        )
        pixel_values = l_pixel_values_.reshape(2, 3, 10, 10)
        l_pixel_values_ = None
        embeddings = torch.conv2d(
            pixel_values,
            l_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_weight_,
            l_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_bias_,
            (2, 2),
            (0, 0),
            (1, 1),
            1,
        )
        pixel_values = l_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_weight_ = l_self_modules_timesformer_modules_embeddings_modules_patch_embeddings_modules_projection_parameters_bias_ = (None)
        flatten = embeddings.flatten(2)
        embeddings = None
        embeddings_1 = flatten.transpose(1, 2)
        flatten = None
        cls_tokens = (
            l_self_modules_timesformer_modules_embeddings_parameters_cls_token_.expand(
                2, -1, -1
            )
        )
        l_self_modules_timesformer_modules_embeddings_parameters_cls_token_ = None
        embeddings_2 = torch.cat((cls_tokens, embeddings_1), dim=1)
        cls_tokens = embeddings_1 = None
        embeddings_3 = (
            embeddings_2
            + l_self_modules_timesformer_modules_embeddings_parameters_position_embeddings_
        )
        embeddings_2 = l_self_modules_timesformer_modules_embeddings_parameters_position_embeddings_ = (None)
        embeddings_4 = torch.nn.functional.dropout(embeddings_3, 0.1, False, False)
        embeddings_3 = None
        getitem = embeddings_4[(slice(None, 1, None), 0, slice(None, None, None))]
        cls_tokens_1 = getitem.unsqueeze(1)
        getitem = None
        embeddings_5 = embeddings_4[(slice(None, None, None), slice(1, None, None))]
        embeddings_4 = None
        reshape_1 = embeddings_5.reshape(1, 2, 25, 32)
        embeddings_5 = None
        permute = reshape_1.permute(0, 2, 1, 3)
        reshape_1 = None
        embeddings_6 = permute.reshape(25, 2, 32)
        permute = None
        embeddings_7 = (
            embeddings_6
            + l_self_modules_timesformer_modules_embeddings_parameters_time_embeddings_
        )
        embeddings_6 = (
            l_self_modules_timesformer_modules_embeddings_parameters_time_embeddings_
        ) = None
        embeddings_8 = torch.nn.functional.dropout(embeddings_7, 0.1, False, False)
        embeddings_7 = None
        view = embeddings_8.view(1, 25, 2, 32)
        embeddings_8 = None
        embeddings_9 = view.reshape(1, 50, 32)
        view = None
        embeddings_10 = torch.cat((cls_tokens_1, embeddings_9), dim=1)
        cls_tokens_1 = embeddings_9 = None
        temporal_embedding = embeddings_10[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        reshape_4 = temporal_embedding.reshape(1, 5, 5, 2, 32)
        temporal_embedding = None
        temporal_embedding_1 = reshape_4.reshape(25, 2, 32)
        reshape_4 = None
        layer_norm = torch.nn.functional.layer_norm(
            temporal_embedding_1,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_bias_,
            1e-06,
        )
        temporal_embedding_1 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_layernorm_parameters_bias_ = (None)
        linear = torch._C._nn.linear(
            layer_norm,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_6 = linear.reshape(25, 2, 3, 4, 8)
        linear = None
        qkv = reshape_6.permute(2, 0, 3, 1, 4)
        reshape_6 = None
        query = qkv[0]
        key = qkv[1]
        value = qkv[2]
        qkv = None
        transpose_1 = key.transpose(-2, -1)
        key = None
        matmul = query @ transpose_1
        query = transpose_1 = None
        attention_probs = matmul * 0.3535533905932738
        matmul = None
        attention_probs_1 = attention_probs.softmax(dim=-1)
        attention_probs = None
        attention_probs_2 = torch.nn.functional.dropout(
            attention_probs_1, 0.1, False, False
        )
        attention_probs_1 = None
        matmul_1 = attention_probs_2 @ value
        attention_probs_2 = value = None
        transpose_2 = matmul_1.transpose(1, 2)
        matmul_1 = None
        context_layer = transpose_2.reshape(25, 2, 32)
        transpose_2 = None
        hidden_states = torch._C._nn.linear(
            context_layer,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_1 = torch.nn.functional.dropout(hidden_states, 0.1, False, False)
        hidden_states = None
        reshape_8 = hidden_states_1.reshape(1, 5, 5, 2, 32)
        hidden_states_1 = None
        residual_temporal = reshape_8.reshape(1, 50, 32)
        reshape_8 = None
        residual_temporal_1 = torch._C._nn.linear(
            residual_temporal,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_bias_,
        )
        residual_temporal = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_temporal_dense_parameters_bias_ = (None)
        getitem_6 = embeddings_10[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        temporal_embedding_2 = getitem_6 + residual_temporal_1
        getitem_6 = residual_temporal_1 = None
        getitem_7 = embeddings_10[(slice(None, None, None), 0, slice(None, None, None))]
        embeddings_10 = None
        init_cls_token = getitem_7.unsqueeze(1)
        getitem_7 = None
        cls_token = init_cls_token.repeat(1, 2, 1)
        cls_token_1 = cls_token.reshape(2, 1, 32)
        cls_token = None
        reshape_11 = temporal_embedding_2.reshape(1, 5, 5, 2, 32)
        permute_2 = reshape_11.permute(0, 3, 1, 2, 4)
        reshape_11 = None
        spatial_embedding = permute_2.reshape(2, 25, 32)
        permute_2 = None
        spatial_embedding_1 = torch.cat((cls_token_1, spatial_embedding), 1)
        cls_token_1 = spatial_embedding = None
        layer_norm_1 = torch.nn.functional.layer_norm(
            spatial_embedding_1,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_bias_,
            1e-06,
        )
        spatial_embedding_1 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_before_parameters_bias_ = (None)
        linear_3 = torch._C._nn.linear(
            layer_norm_1,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_1 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_13 = linear_3.reshape(2, 26, 3, 4, 8)
        linear_3 = None
        qkv_1 = reshape_13.permute(2, 0, 3, 1, 4)
        reshape_13 = None
        query_1 = qkv_1[0]
        key_1 = qkv_1[1]
        value_1 = qkv_1[2]
        qkv_1 = None
        transpose_3 = key_1.transpose(-2, -1)
        key_1 = None
        matmul_2 = query_1 @ transpose_3
        query_1 = transpose_3 = None
        attention_probs_3 = matmul_2 * 0.3535533905932738
        matmul_2 = None
        attention_probs_4 = attention_probs_3.softmax(dim=-1)
        attention_probs_3 = None
        attention_probs_5 = torch.nn.functional.dropout(
            attention_probs_4, 0.1, False, False
        )
        attention_probs_4 = None
        matmul_3 = attention_probs_5 @ value_1
        attention_probs_5 = value_1 = None
        transpose_4 = matmul_3.transpose(1, 2)
        matmul_3 = None
        context_layer_1 = transpose_4.reshape(2, 26, 32)
        transpose_4 = None
        hidden_states_2 = torch._C._nn.linear(
            context_layer_1,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_1 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_3 = torch.nn.functional.dropout(
            hidden_states_2, 0.1, False, False
        )
        hidden_states_2 = None
        cls_token_2 = hidden_states_3[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        cls_token_3 = cls_token_2.reshape(1, 2, 32)
        cls_token_2 = None
        cls_token_4 = torch.mean(cls_token_3, 1, True)
        cls_token_3 = None
        residual_spatial = hidden_states_3[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        hidden_states_3 = None
        reshape_16 = residual_spatial.reshape(1, 2, 5, 5, 32)
        residual_spatial = None
        permute_4 = reshape_16.permute(0, 2, 3, 1, 4)
        reshape_16 = None
        residual_spatial_1 = permute_4.reshape(1, 50, 32)
        permute_4 = None
        cat_3 = torch.cat((init_cls_token, temporal_embedding_2), 1)
        init_cls_token = temporal_embedding_2 = None
        cat_4 = torch.cat((cls_token_4, residual_spatial_1), 1)
        cls_token_4 = residual_spatial_1 = None
        hidden_states_4 = cat_3 + cat_4
        cat_3 = cat_4 = None
        layer_output = torch.nn.functional.layer_norm(
            hidden_states_4,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_bias_,
            1e-06,
        )
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_layernorm_after_parameters_bias_ = (None)
        hidden_states_5 = torch._C._nn.linear(
            layer_output,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_states_6 = torch._C._nn.gelu(hidden_states_5)
        hidden_states_5 = None
        hidden_states_7 = torch.nn.functional.dropout(
            hidden_states_6, 0.1, False, False
        )
        hidden_states_6 = None
        hidden_states_8 = torch._C._nn.linear(
            hidden_states_7,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_,
        )
        hidden_states_7 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_9 = torch.nn.functional.dropout(
            hidden_states_8, 0.1, False, False
        )
        hidden_states_8 = None
        layer_output_1 = hidden_states_4 + hidden_states_9
        hidden_states_4 = hidden_states_9 = None
        temporal_embedding_3 = layer_output_1[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        reshape_18 = temporal_embedding_3.reshape(1, 5, 5, 2, 32)
        temporal_embedding_3 = None
        temporal_embedding_4 = reshape_18.reshape(25, 2, 32)
        reshape_18 = None
        layer_norm_3 = torch.nn.functional.layer_norm(
            temporal_embedding_4,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_bias_,
            1e-06,
        )
        temporal_embedding_4 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_layernorm_parameters_bias_ = (None)
        linear_7 = torch._C._nn.linear(
            layer_norm_3,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_3 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_20 = linear_7.reshape(25, 2, 3, 4, 8)
        linear_7 = None
        qkv_2 = reshape_20.permute(2, 0, 3, 1, 4)
        reshape_20 = None
        query_2 = qkv_2[0]
        key_2 = qkv_2[1]
        value_2 = qkv_2[2]
        qkv_2 = None
        transpose_5 = key_2.transpose(-2, -1)
        key_2 = None
        matmul_4 = query_2 @ transpose_5
        query_2 = transpose_5 = None
        attention_probs_6 = matmul_4 * 0.3535533905932738
        matmul_4 = None
        attention_probs_7 = attention_probs_6.softmax(dim=-1)
        attention_probs_6 = None
        attention_probs_8 = torch.nn.functional.dropout(
            attention_probs_7, 0.1, False, False
        )
        attention_probs_7 = None
        matmul_5 = attention_probs_8 @ value_2
        attention_probs_8 = value_2 = None
        transpose_6 = matmul_5.transpose(1, 2)
        matmul_5 = None
        context_layer_2 = transpose_6.reshape(25, 2, 32)
        transpose_6 = None
        hidden_states_10 = torch._C._nn.linear(
            context_layer_2,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_2 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_11 = torch.nn.functional.dropout(
            hidden_states_10, 0.1, False, False
        )
        hidden_states_10 = None
        reshape_22 = hidden_states_11.reshape(1, 5, 5, 2, 32)
        hidden_states_11 = None
        residual_temporal_2 = reshape_22.reshape(1, 50, 32)
        reshape_22 = None
        residual_temporal_3 = torch._C._nn.linear(
            residual_temporal_2,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_bias_,
        )
        residual_temporal_2 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_temporal_dense_parameters_bias_ = (None)
        getitem_17 = layer_output_1[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        temporal_embedding_5 = getitem_17 + residual_temporal_3
        getitem_17 = residual_temporal_3 = None
        getitem_18 = layer_output_1[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        layer_output_1 = None
        init_cls_token_1 = getitem_18.unsqueeze(1)
        getitem_18 = None
        cls_token_5 = init_cls_token_1.repeat(1, 2, 1)
        cls_token_6 = cls_token_5.reshape(2, 1, 32)
        cls_token_5 = None
        reshape_25 = temporal_embedding_5.reshape(1, 5, 5, 2, 32)
        permute_6 = reshape_25.permute(0, 3, 1, 2, 4)
        reshape_25 = None
        spatial_embedding_2 = permute_6.reshape(2, 25, 32)
        permute_6 = None
        spatial_embedding_3 = torch.cat((cls_token_6, spatial_embedding_2), 1)
        cls_token_6 = spatial_embedding_2 = None
        layer_norm_4 = torch.nn.functional.layer_norm(
            spatial_embedding_3,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_bias_,
            1e-06,
        )
        spatial_embedding_3 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_before_parameters_bias_ = (None)
        linear_10 = torch._C._nn.linear(
            layer_norm_4,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_4 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_27 = linear_10.reshape(2, 26, 3, 4, 8)
        linear_10 = None
        qkv_3 = reshape_27.permute(2, 0, 3, 1, 4)
        reshape_27 = None
        query_3 = qkv_3[0]
        key_3 = qkv_3[1]
        value_3 = qkv_3[2]
        qkv_3 = None
        transpose_7 = key_3.transpose(-2, -1)
        key_3 = None
        matmul_6 = query_3 @ transpose_7
        query_3 = transpose_7 = None
        attention_probs_9 = matmul_6 * 0.3535533905932738
        matmul_6 = None
        attention_probs_10 = attention_probs_9.softmax(dim=-1)
        attention_probs_9 = None
        attention_probs_11 = torch.nn.functional.dropout(
            attention_probs_10, 0.1, False, False
        )
        attention_probs_10 = None
        matmul_7 = attention_probs_11 @ value_3
        attention_probs_11 = value_3 = None
        transpose_8 = matmul_7.transpose(1, 2)
        matmul_7 = None
        context_layer_3 = transpose_8.reshape(2, 26, 32)
        transpose_8 = None
        hidden_states_12 = torch._C._nn.linear(
            context_layer_3,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_3 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_13 = torch.nn.functional.dropout(
            hidden_states_12, 0.1, False, False
        )
        hidden_states_12 = None
        cls_token_7 = hidden_states_13[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        cls_token_8 = cls_token_7.reshape(1, 2, 32)
        cls_token_7 = None
        cls_token_9 = torch.mean(cls_token_8, 1, True)
        cls_token_8 = None
        residual_spatial_2 = hidden_states_13[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        hidden_states_13 = None
        reshape_30 = residual_spatial_2.reshape(1, 2, 5, 5, 32)
        residual_spatial_2 = None
        permute_8 = reshape_30.permute(0, 2, 3, 1, 4)
        reshape_30 = None
        residual_spatial_3 = permute_8.reshape(1, 50, 32)
        permute_8 = None
        cat_6 = torch.cat((init_cls_token_1, temporal_embedding_5), 1)
        init_cls_token_1 = temporal_embedding_5 = None
        cat_7 = torch.cat((cls_token_9, residual_spatial_3), 1)
        cls_token_9 = residual_spatial_3 = None
        hidden_states_14 = cat_6 + cat_7
        cat_6 = cat_7 = None
        layer_output_2 = torch.nn.functional.layer_norm(
            hidden_states_14,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_bias_,
            1e-06,
        )
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_layernorm_after_parameters_bias_ = (None)
        hidden_states_15 = torch._C._nn.linear(
            layer_output_2,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_2 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_states_16 = torch._C._nn.gelu(hidden_states_15)
        hidden_states_15 = None
        hidden_states_17 = torch.nn.functional.dropout(
            hidden_states_16, 0.1, False, False
        )
        hidden_states_16 = None
        hidden_states_18 = torch._C._nn.linear(
            hidden_states_17,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_,
        )
        hidden_states_17 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_19 = torch.nn.functional.dropout(
            hidden_states_18, 0.1, False, False
        )
        hidden_states_18 = None
        layer_output_3 = hidden_states_14 + hidden_states_19
        hidden_states_14 = hidden_states_19 = None
        temporal_embedding_6 = layer_output_3[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        reshape_32 = temporal_embedding_6.reshape(1, 5, 5, 2, 32)
        temporal_embedding_6 = None
        temporal_embedding_7 = reshape_32.reshape(25, 2, 32)
        reshape_32 = None
        layer_norm_6 = torch.nn.functional.layer_norm(
            temporal_embedding_7,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_bias_,
            1e-06,
        )
        temporal_embedding_7 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_layernorm_parameters_bias_ = (None)
        linear_14 = torch._C._nn.linear(
            layer_norm_6,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_6 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_34 = linear_14.reshape(25, 2, 3, 4, 8)
        linear_14 = None
        qkv_4 = reshape_34.permute(2, 0, 3, 1, 4)
        reshape_34 = None
        query_4 = qkv_4[0]
        key_4 = qkv_4[1]
        value_4 = qkv_4[2]
        qkv_4 = None
        transpose_9 = key_4.transpose(-2, -1)
        key_4 = None
        matmul_8 = query_4 @ transpose_9
        query_4 = transpose_9 = None
        attention_probs_12 = matmul_8 * 0.3535533905932738
        matmul_8 = None
        attention_probs_13 = attention_probs_12.softmax(dim=-1)
        attention_probs_12 = None
        attention_probs_14 = torch.nn.functional.dropout(
            attention_probs_13, 0.1, False, False
        )
        attention_probs_13 = None
        matmul_9 = attention_probs_14 @ value_4
        attention_probs_14 = value_4 = None
        transpose_10 = matmul_9.transpose(1, 2)
        matmul_9 = None
        context_layer_4 = transpose_10.reshape(25, 2, 32)
        transpose_10 = None
        hidden_states_20 = torch._C._nn.linear(
            context_layer_4,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_4 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_21 = torch.nn.functional.dropout(
            hidden_states_20, 0.1, False, False
        )
        hidden_states_20 = None
        reshape_36 = hidden_states_21.reshape(1, 5, 5, 2, 32)
        hidden_states_21 = None
        residual_temporal_4 = reshape_36.reshape(1, 50, 32)
        reshape_36 = None
        residual_temporal_5 = torch._C._nn.linear(
            residual_temporal_4,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_bias_,
        )
        residual_temporal_4 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_temporal_dense_parameters_bias_ = (None)
        getitem_28 = layer_output_3[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        temporal_embedding_8 = getitem_28 + residual_temporal_5
        getitem_28 = residual_temporal_5 = None
        getitem_29 = layer_output_3[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        layer_output_3 = None
        init_cls_token_2 = getitem_29.unsqueeze(1)
        getitem_29 = None
        cls_token_10 = init_cls_token_2.repeat(1, 2, 1)
        cls_token_11 = cls_token_10.reshape(2, 1, 32)
        cls_token_10 = None
        reshape_39 = temporal_embedding_8.reshape(1, 5, 5, 2, 32)
        permute_10 = reshape_39.permute(0, 3, 1, 2, 4)
        reshape_39 = None
        spatial_embedding_4 = permute_10.reshape(2, 25, 32)
        permute_10 = None
        spatial_embedding_5 = torch.cat((cls_token_11, spatial_embedding_4), 1)
        cls_token_11 = spatial_embedding_4 = None
        layer_norm_7 = torch.nn.functional.layer_norm(
            spatial_embedding_5,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_bias_,
            1e-06,
        )
        spatial_embedding_5 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_before_parameters_bias_ = (None)
        linear_17 = torch._C._nn.linear(
            layer_norm_7,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_7 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_41 = linear_17.reshape(2, 26, 3, 4, 8)
        linear_17 = None
        qkv_5 = reshape_41.permute(2, 0, 3, 1, 4)
        reshape_41 = None
        query_5 = qkv_5[0]
        key_5 = qkv_5[1]
        value_5 = qkv_5[2]
        qkv_5 = None
        transpose_11 = key_5.transpose(-2, -1)
        key_5 = None
        matmul_10 = query_5 @ transpose_11
        query_5 = transpose_11 = None
        attention_probs_15 = matmul_10 * 0.3535533905932738
        matmul_10 = None
        attention_probs_16 = attention_probs_15.softmax(dim=-1)
        attention_probs_15 = None
        attention_probs_17 = torch.nn.functional.dropout(
            attention_probs_16, 0.1, False, False
        )
        attention_probs_16 = None
        matmul_11 = attention_probs_17 @ value_5
        attention_probs_17 = value_5 = None
        transpose_12 = matmul_11.transpose(1, 2)
        matmul_11 = None
        context_layer_5 = transpose_12.reshape(2, 26, 32)
        transpose_12 = None
        hidden_states_22 = torch._C._nn.linear(
            context_layer_5,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_5 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_23 = torch.nn.functional.dropout(
            hidden_states_22, 0.1, False, False
        )
        hidden_states_22 = None
        cls_token_12 = hidden_states_23[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        cls_token_13 = cls_token_12.reshape(1, 2, 32)
        cls_token_12 = None
        cls_token_14 = torch.mean(cls_token_13, 1, True)
        cls_token_13 = None
        residual_spatial_4 = hidden_states_23[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        hidden_states_23 = None
        reshape_44 = residual_spatial_4.reshape(1, 2, 5, 5, 32)
        residual_spatial_4 = None
        permute_12 = reshape_44.permute(0, 2, 3, 1, 4)
        reshape_44 = None
        residual_spatial_5 = permute_12.reshape(1, 50, 32)
        permute_12 = None
        cat_9 = torch.cat((init_cls_token_2, temporal_embedding_8), 1)
        init_cls_token_2 = temporal_embedding_8 = None
        cat_10 = torch.cat((cls_token_14, residual_spatial_5), 1)
        cls_token_14 = residual_spatial_5 = None
        hidden_states_24 = cat_9 + cat_10
        cat_9 = cat_10 = None
        layer_output_4 = torch.nn.functional.layer_norm(
            hidden_states_24,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_bias_,
            1e-06,
        )
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_layernorm_after_parameters_bias_ = (None)
        hidden_states_25 = torch._C._nn.linear(
            layer_output_4,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_4 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_states_26 = torch._C._nn.gelu(hidden_states_25)
        hidden_states_25 = None
        hidden_states_27 = torch.nn.functional.dropout(
            hidden_states_26, 0.1, False, False
        )
        hidden_states_26 = None
        hidden_states_28 = torch._C._nn.linear(
            hidden_states_27,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_,
        )
        hidden_states_27 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_29 = torch.nn.functional.dropout(
            hidden_states_28, 0.1, False, False
        )
        hidden_states_28 = None
        layer_output_5 = hidden_states_24 + hidden_states_29
        hidden_states_24 = hidden_states_29 = None
        temporal_embedding_9 = layer_output_5[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        reshape_46 = temporal_embedding_9.reshape(1, 5, 5, 2, 32)
        temporal_embedding_9 = None
        temporal_embedding_10 = reshape_46.reshape(25, 2, 32)
        reshape_46 = None
        layer_norm_9 = torch.nn.functional.layer_norm(
            temporal_embedding_10,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_bias_,
            1e-06,
        )
        temporal_embedding_10 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_layernorm_parameters_bias_ = (None)
        linear_21 = torch._C._nn.linear(
            layer_norm_9,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_9 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_48 = linear_21.reshape(25, 2, 3, 4, 8)
        linear_21 = None
        qkv_6 = reshape_48.permute(2, 0, 3, 1, 4)
        reshape_48 = None
        query_6 = qkv_6[0]
        key_6 = qkv_6[1]
        value_6 = qkv_6[2]
        qkv_6 = None
        transpose_13 = key_6.transpose(-2, -1)
        key_6 = None
        matmul_12 = query_6 @ transpose_13
        query_6 = transpose_13 = None
        attention_probs_18 = matmul_12 * 0.3535533905932738
        matmul_12 = None
        attention_probs_19 = attention_probs_18.softmax(dim=-1)
        attention_probs_18 = None
        attention_probs_20 = torch.nn.functional.dropout(
            attention_probs_19, 0.1, False, False
        )
        attention_probs_19 = None
        matmul_13 = attention_probs_20 @ value_6
        attention_probs_20 = value_6 = None
        transpose_14 = matmul_13.transpose(1, 2)
        matmul_13 = None
        context_layer_6 = transpose_14.reshape(25, 2, 32)
        transpose_14 = None
        hidden_states_30 = torch._C._nn.linear(
            context_layer_6,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_6 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_31 = torch.nn.functional.dropout(
            hidden_states_30, 0.1, False, False
        )
        hidden_states_30 = None
        reshape_50 = hidden_states_31.reshape(1, 5, 5, 2, 32)
        hidden_states_31 = None
        residual_temporal_6 = reshape_50.reshape(1, 50, 32)
        reshape_50 = None
        residual_temporal_7 = torch._C._nn.linear(
            residual_temporal_6,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_bias_,
        )
        residual_temporal_6 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_temporal_dense_parameters_bias_ = (None)
        getitem_39 = layer_output_5[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        temporal_embedding_11 = getitem_39 + residual_temporal_7
        getitem_39 = residual_temporal_7 = None
        getitem_40 = layer_output_5[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        layer_output_5 = None
        init_cls_token_3 = getitem_40.unsqueeze(1)
        getitem_40 = None
        cls_token_15 = init_cls_token_3.repeat(1, 2, 1)
        cls_token_16 = cls_token_15.reshape(2, 1, 32)
        cls_token_15 = None
        reshape_53 = temporal_embedding_11.reshape(1, 5, 5, 2, 32)
        permute_14 = reshape_53.permute(0, 3, 1, 2, 4)
        reshape_53 = None
        spatial_embedding_6 = permute_14.reshape(2, 25, 32)
        permute_14 = None
        spatial_embedding_7 = torch.cat((cls_token_16, spatial_embedding_6), 1)
        cls_token_16 = spatial_embedding_6 = None
        layer_norm_10 = torch.nn.functional.layer_norm(
            spatial_embedding_7,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_bias_,
            1e-06,
        )
        spatial_embedding_7 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_before_parameters_bias_ = (None)
        linear_24 = torch._C._nn.linear(
            layer_norm_10,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_10 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_55 = linear_24.reshape(2, 26, 3, 4, 8)
        linear_24 = None
        qkv_7 = reshape_55.permute(2, 0, 3, 1, 4)
        reshape_55 = None
        query_7 = qkv_7[0]
        key_7 = qkv_7[1]
        value_7 = qkv_7[2]
        qkv_7 = None
        transpose_15 = key_7.transpose(-2, -1)
        key_7 = None
        matmul_14 = query_7 @ transpose_15
        query_7 = transpose_15 = None
        attention_probs_21 = matmul_14 * 0.3535533905932738
        matmul_14 = None
        attention_probs_22 = attention_probs_21.softmax(dim=-1)
        attention_probs_21 = None
        attention_probs_23 = torch.nn.functional.dropout(
            attention_probs_22, 0.1, False, False
        )
        attention_probs_22 = None
        matmul_15 = attention_probs_23 @ value_7
        attention_probs_23 = value_7 = None
        transpose_16 = matmul_15.transpose(1, 2)
        matmul_15 = None
        context_layer_7 = transpose_16.reshape(2, 26, 32)
        transpose_16 = None
        hidden_states_32 = torch._C._nn.linear(
            context_layer_7,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_7 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_33 = torch.nn.functional.dropout(
            hidden_states_32, 0.1, False, False
        )
        hidden_states_32 = None
        cls_token_17 = hidden_states_33[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        cls_token_18 = cls_token_17.reshape(1, 2, 32)
        cls_token_17 = None
        cls_token_19 = torch.mean(cls_token_18, 1, True)
        cls_token_18 = None
        residual_spatial_6 = hidden_states_33[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        hidden_states_33 = None
        reshape_58 = residual_spatial_6.reshape(1, 2, 5, 5, 32)
        residual_spatial_6 = None
        permute_16 = reshape_58.permute(0, 2, 3, 1, 4)
        reshape_58 = None
        residual_spatial_7 = permute_16.reshape(1, 50, 32)
        permute_16 = None
        cat_12 = torch.cat((init_cls_token_3, temporal_embedding_11), 1)
        init_cls_token_3 = temporal_embedding_11 = None
        cat_13 = torch.cat((cls_token_19, residual_spatial_7), 1)
        cls_token_19 = residual_spatial_7 = None
        hidden_states_34 = cat_12 + cat_13
        cat_12 = cat_13 = None
        layer_output_6 = torch.nn.functional.layer_norm(
            hidden_states_34,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_bias_,
            1e-06,
        )
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_layernorm_after_parameters_bias_ = (None)
        hidden_states_35 = torch._C._nn.linear(
            layer_output_6,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_6 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_states_36 = torch._C._nn.gelu(hidden_states_35)
        hidden_states_35 = None
        hidden_states_37 = torch.nn.functional.dropout(
            hidden_states_36, 0.1, False, False
        )
        hidden_states_36 = None
        hidden_states_38 = torch._C._nn.linear(
            hidden_states_37,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_,
        )
        hidden_states_37 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_39 = torch.nn.functional.dropout(
            hidden_states_38, 0.1, False, False
        )
        hidden_states_38 = None
        layer_output_7 = hidden_states_34 + hidden_states_39
        hidden_states_34 = hidden_states_39 = None
        temporal_embedding_12 = layer_output_7[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        reshape_60 = temporal_embedding_12.reshape(1, 5, 5, 2, 32)
        temporal_embedding_12 = None
        temporal_embedding_13 = reshape_60.reshape(25, 2, 32)
        reshape_60 = None
        layer_norm_12 = torch.nn.functional.layer_norm(
            temporal_embedding_13,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_bias_,
            1e-06,
        )
        temporal_embedding_13 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_layernorm_parameters_bias_ = (None)
        linear_28 = torch._C._nn.linear(
            layer_norm_12,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_12 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_62 = linear_28.reshape(25, 2, 3, 4, 8)
        linear_28 = None
        qkv_8 = reshape_62.permute(2, 0, 3, 1, 4)
        reshape_62 = None
        query_8 = qkv_8[0]
        key_8 = qkv_8[1]
        value_8 = qkv_8[2]
        qkv_8 = None
        transpose_17 = key_8.transpose(-2, -1)
        key_8 = None
        matmul_16 = query_8 @ transpose_17
        query_8 = transpose_17 = None
        attention_probs_24 = matmul_16 * 0.3535533905932738
        matmul_16 = None
        attention_probs_25 = attention_probs_24.softmax(dim=-1)
        attention_probs_24 = None
        attention_probs_26 = torch.nn.functional.dropout(
            attention_probs_25, 0.1, False, False
        )
        attention_probs_25 = None
        matmul_17 = attention_probs_26 @ value_8
        attention_probs_26 = value_8 = None
        transpose_18 = matmul_17.transpose(1, 2)
        matmul_17 = None
        context_layer_8 = transpose_18.reshape(25, 2, 32)
        transpose_18 = None
        hidden_states_40 = torch._C._nn.linear(
            context_layer_8,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_8 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_41 = torch.nn.functional.dropout(
            hidden_states_40, 0.1, False, False
        )
        hidden_states_40 = None
        reshape_64 = hidden_states_41.reshape(1, 5, 5, 2, 32)
        hidden_states_41 = None
        residual_temporal_8 = reshape_64.reshape(1, 50, 32)
        reshape_64 = None
        residual_temporal_9 = torch._C._nn.linear(
            residual_temporal_8,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_bias_,
        )
        residual_temporal_8 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_temporal_dense_parameters_bias_ = (None)
        getitem_50 = layer_output_7[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        temporal_embedding_14 = getitem_50 + residual_temporal_9
        getitem_50 = residual_temporal_9 = None
        getitem_51 = layer_output_7[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        layer_output_7 = None
        init_cls_token_4 = getitem_51.unsqueeze(1)
        getitem_51 = None
        cls_token_20 = init_cls_token_4.repeat(1, 2, 1)
        cls_token_21 = cls_token_20.reshape(2, 1, 32)
        cls_token_20 = None
        reshape_67 = temporal_embedding_14.reshape(1, 5, 5, 2, 32)
        permute_18 = reshape_67.permute(0, 3, 1, 2, 4)
        reshape_67 = None
        spatial_embedding_8 = permute_18.reshape(2, 25, 32)
        permute_18 = None
        spatial_embedding_9 = torch.cat((cls_token_21, spatial_embedding_8), 1)
        cls_token_21 = spatial_embedding_8 = None
        layer_norm_13 = torch.nn.functional.layer_norm(
            spatial_embedding_9,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_bias_,
            1e-06,
        )
        spatial_embedding_9 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_before_parameters_bias_ = (None)
        linear_31 = torch._C._nn.linear(
            layer_norm_13,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_bias_,
        )
        layer_norm_13 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_attention_modules_qkv_parameters_bias_ = (None)
        reshape_69 = linear_31.reshape(2, 26, 3, 4, 8)
        linear_31 = None
        qkv_9 = reshape_69.permute(2, 0, 3, 1, 4)
        reshape_69 = None
        query_9 = qkv_9[0]
        key_9 = qkv_9[1]
        value_9 = qkv_9[2]
        qkv_9 = None
        transpose_19 = key_9.transpose(-2, -1)
        key_9 = None
        matmul_18 = query_9 @ transpose_19
        query_9 = transpose_19 = None
        attention_probs_27 = matmul_18 * 0.3535533905932738
        matmul_18 = None
        attention_probs_28 = attention_probs_27.softmax(dim=-1)
        attention_probs_27 = None
        attention_probs_29 = torch.nn.functional.dropout(
            attention_probs_28, 0.1, False, False
        )
        attention_probs_28 = None
        matmul_19 = attention_probs_29 @ value_9
        attention_probs_29 = value_9 = None
        transpose_20 = matmul_19.transpose(1, 2)
        matmul_19 = None
        context_layer_9 = transpose_20.reshape(2, 26, 32)
        transpose_20 = None
        hidden_states_42 = torch._C._nn.linear(
            context_layer_9,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_layer_9 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_43 = torch.nn.functional.dropout(
            hidden_states_42, 0.1, False, False
        )
        hidden_states_42 = None
        cls_token_22 = hidden_states_43[
            (slice(None, None, None), 0, slice(None, None, None))
        ]
        cls_token_23 = cls_token_22.reshape(1, 2, 32)
        cls_token_22 = None
        cls_token_24 = torch.mean(cls_token_23, 1, True)
        cls_token_23 = None
        residual_spatial_8 = hidden_states_43[
            (slice(None, None, None), slice(1, None, None), slice(None, None, None))
        ]
        hidden_states_43 = None
        reshape_72 = residual_spatial_8.reshape(1, 2, 5, 5, 32)
        residual_spatial_8 = None
        permute_20 = reshape_72.permute(0, 2, 3, 1, 4)
        reshape_72 = None
        residual_spatial_9 = permute_20.reshape(1, 50, 32)
        permute_20 = None
        cat_15 = torch.cat((init_cls_token_4, temporal_embedding_14), 1)
        init_cls_token_4 = temporal_embedding_14 = None
        cat_16 = torch.cat((cls_token_24, residual_spatial_9), 1)
        cls_token_24 = residual_spatial_9 = None
        hidden_states_44 = cat_15 + cat_16
        cat_15 = cat_16 = None
        layer_output_8 = torch.nn.functional.layer_norm(
            hidden_states_44,
            (32,),
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_bias_,
            1e-06,
        )
        l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_layernorm_after_parameters_bias_ = (None)
        hidden_states_45 = torch._C._nn.linear(
            layer_output_8,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_8 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_states_46 = torch._C._nn.gelu(hidden_states_45)
        hidden_states_45 = None
        hidden_states_47 = torch.nn.functional.dropout(
            hidden_states_46, 0.1, False, False
        )
        hidden_states_46 = None
        hidden_states_48 = torch._C._nn.linear(
            hidden_states_47,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_,
            l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_,
        )
        hidden_states_47 = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ = l_self_modules_timesformer_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_states_49 = torch.nn.functional.dropout(
            hidden_states_48, 0.1, False, False
        )
        hidden_states_48 = None
        layer_output_9 = hidden_states_44 + hidden_states_49
        hidden_states_44 = hidden_states_49 = None
        sequence_output = torch.nn.functional.layer_norm(
            layer_output_9,
            (32,),
            l_self_modules_timesformer_modules_layernorm_parameters_weight_,
            l_self_modules_timesformer_modules_layernorm_parameters_bias_,
            1e-06,
        )
        layer_output_9 = (
            l_self_modules_timesformer_modules_layernorm_parameters_weight_
        ) = l_self_modules_timesformer_modules_layernorm_parameters_bias_ = None
        sequence_output_1 = sequence_output[(slice(None, None, None), 0)]
        sequence_output = None
        logits = torch._C._nn.linear(
            sequence_output_1,
            l_self_modules_classifier_parameters_weight_,
            l_self_modules_classifier_parameters_bias_,
        )
        sequence_output_1 = (
            l_self_modules_classifier_parameters_weight_
        ) = l_self_modules_classifier_parameters_bias_ = None
        return (logits,)
