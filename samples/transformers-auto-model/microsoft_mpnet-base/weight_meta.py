class Program_weight_tensor_meta_L_input_ids_:
    name = "L_input_ids_"
    shape = [1, 11]
    dtype = "torch.int64"
    device = "cuda:0"
    mean = None
    std = None
    data = [0, 10633, 7163, 2007, 2113, 2004, 14821, 15082, 19291, 1016, 2]


class Program_weight_tensor_meta_L_attention_mask_:
    name = "L_attention_mask_"
    shape = [1, 11]
    dtype = "torch.int64"
    device = "cuda:0"
    mean = None
    std = None
    data = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]


class Program_weight_tensor_meta_L_self_modules_embeddings_modules_word_embeddings_parameters_weight_:
    name = "L_self_modules_embeddings_modules_word_embeddings_parameters_weight_"
    shape = [30527, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.011
    std = 0.085
    data = None


class Program_weight_tensor_meta_L_self_modules_embeddings_modules_position_embeddings_parameters_weight_:
    name = "L_self_modules_embeddings_modules_position_embeddings_parameters_weight_"
    shape = [514, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_embeddings_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_embeddings_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.309
    std = 0.046
    data = None


class Program_weight_tensor_meta_L_self_modules_embeddings_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_embeddings_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.011
    std = 0.044
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_relative_attention_bias_parameters_weight_:
    name = "L_self_modules_encoder_modules_relative_attention_bias_parameters_weight_"
    shape = [32, 12]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -1.035
    std = 2.186
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.102
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.017
    std = 0.293
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.104
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.002
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.045
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.039
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.046
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.002
    std = 0.069
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.461
    std = 0.079
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.005
    std = 0.220
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.066
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.103
    std = 0.080
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.054
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.007
    std = 0.108
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.301
    std = 0.047
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.039
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.096
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.012
    std = 0.298
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.098
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.002
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.051
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.027
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.052
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.047
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.430
    std = 0.104
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.008
    std = 0.188
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.070
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.102
    std = 0.080
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.054
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.004
    std = 0.073
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.304
    std = 0.050
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.003
    std = 0.046
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.096
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.006
    std = 0.343
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.099
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.002
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.051
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.002
    std = 0.035
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.051
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.057
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.412
    std = 0.119
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.016
    std = 0.178
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.070
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.107
    std = 0.090
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.052
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.071
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.311
    std = 0.049
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.013
    std = 0.048
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.095
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.008
    std = 0.348
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.097
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.006
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.051
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.026
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.053
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.040
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.411
    std = 0.117
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.032
    std = 0.161
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.003
    std = 0.068
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.116
    std = 0.114
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.050
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.095
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.339
    std = 0.050
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.017
    std = 0.045
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.096
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.303
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.098
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.008
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.048
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.024
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.051
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.064
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.418
    std = 0.117
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.069
    std = 0.121
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.009
    std = 0.065
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.112
    std = 0.103
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.047
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.002
    std = 0.084
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.331
    std = 0.040
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.016
    std = 0.034
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.094
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.006
    std = 0.300
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.095
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.033
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.052
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.055
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.042
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.414
    std = 0.119
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.071
    std = 0.109
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.010
    std = 0.062
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.116
    std = 0.102
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.045
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.086
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.355
    std = 0.041
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.019
    std = 0.024
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.097
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.006
    std = 0.294
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.096
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.003
    std = 0.146
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.047
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.050
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.057
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.419
    std = 0.106
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.082
    std = 0.071
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.013
    std = 0.061
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.101
    std = 0.090
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.043
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.070
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.353
    std = 0.033
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.016
    std = 0.034
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.095
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.011
    std = 0.289
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.095
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.002
    std = 0.322
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.049
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.017
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.049
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.064
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.409
    std = 0.101
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.074
    std = 0.079
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.012
    std = 0.062
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.099
    std = 0.088
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.044
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.066
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.352
    std = 0.031
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.015
    std = 0.041
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.095
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.013
    std = 0.313
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.096
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.011
    std = 0.389
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.046
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.016
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.045
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.041
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.400
    std = 0.110
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.072
    std = 0.069
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.012
    std = 0.061
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.100
    std = 0.091
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.044
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.068
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.350
    std = 0.030
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.011
    std = 0.034
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.093
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.002
    std = 0.406
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.095
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.005
    std = 0.668
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.045
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.025
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.045
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.044
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.424
    std = 0.123
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.070
    std = 0.086
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.012
    std = 0.057
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.101
    std = 0.077
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.043
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.002
    std = 0.063
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.359
    std = 0.034
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.015
    std = 0.017
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.095
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.011
    std = 0.476
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.098
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.021
    std = 0.772
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.042
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.037
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.041
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.001
    std = 0.070
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.469
    std = 0.122
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.068
    std = 0.061
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.010
    std = 0.050
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.091
    std = 0.065
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.041
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.049
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.359
    std = 0.056
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.012
    std = 0.055
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_q_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_q_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.100
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_q_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_q_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.009
    std = 0.347
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_k_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_k_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.103
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_k_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_k_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.002
    std = 0.392
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_v_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_v_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.046
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_v_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_v_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.019
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_o_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_o_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.042
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_o_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_attn_modules_o_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.004
    std = 0.050
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.482
    std = 0.027
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.068
    std = 0.060
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_"
    shape = [3072, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.009
    std = 0.045
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_"
    shape = [3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.072
    std = 0.038
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_"
    shape = [768, 3072]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.035
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.001
    std = 0.059
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.132
    std = 0.027
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_:
    name = "L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.019
    std = 0.032
    data = None


class Program_weight_tensor_meta_L_self_modules_pooler_modules_dense_parameters_weight_:
    name = "L_self_modules_pooler_modules_dense_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_pooler_modules_dense_parameters_bias_:
    name = "L_self_modules_pooler_modules_dense_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None
