class Program_weight_tensor_meta_L_flattened_patches_:
    name = "L_flattened_patches_"
    shape = [1, 2048, 770]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.059
    std = 1.485
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_embeddings_modules_patch_projection_parameters_weight_:
    name = "L_self_modules_encoder_modules_embeddings_modules_patch_projection_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_embeddings_modules_patch_projection_parameters_bias_:
    name = "L_self_modules_encoder_modules_embeddings_modules_patch_projection_parameters_bias_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_embeddings_modules_row_embedder_parameters_weight_:
    name = "L_self_modules_encoder_modules_embeddings_modules_row_embedder_parameters_weight_"
    shape = [4096, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_embeddings_modules_column_embedder_parameters_weight_:
    name = "L_self_modules_encoder_modules_embeddings_modules_column_embedder_parameters_weight_"
    shape = [4096, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_0_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_1_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_2_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_3_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_4_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_5_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_6_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_7_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_8_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_9_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_10_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_pre_attention_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_pre_attention_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_pre_mlp_layer_norm_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_pre_mlp_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_mlp_modules_wi_0_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_mlp_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_mlp_modules_wi_1_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_mlp_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_mlp_modules_wo_parameters_weight_:
    name = "L_self_modules_encoder_modules_encoder_modules_layer_modules_11_modules_mlp_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_encoder_modules_layernorm_parameters_weight_:
    name = "L_self_modules_encoder_modules_layernorm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_decoder_input_ids_:
    name = "L_decoder_input_ids_"
    shape = [1, 1]
    dtype = "torch.int64"
    device = "cuda:0"
    mean = None
    std = None
    data = [0]


class Program_weight_tensor_meta_L_self_modules_decoder_modules_embed_tokens_parameters_weight_:
    name = "L_self_modules_decoder_modules_embed_tokens_parameters_weight_"
    shape = [50244, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_relative_attention_bias_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_relative_attention_bias_parameters_weight_"
    shape = [32, 12]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.002
    std = 0.038
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_0_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_0_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_1_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_1_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_2_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_2_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_3_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_3_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_4_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_4_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_5_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_5_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_6_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_6_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_7_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_7_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_8_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_8_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_9_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_9_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_10_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_10_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_self_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_attention_modules_query_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.001
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_attention_modules_key_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_attention_modules_value_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_encoder_decoder_attention_modules_attention_modules_output_parameters_weight_"
    shape = [768, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.010
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_mlp_modules_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_mlp_modules_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_mlp_modules_DenseReluDense_modules_wi_0_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_mlp_modules_DenseReluDense_modules_wi_1_parameters_weight_"
    shape = [2048, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_layer_modules_11_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_:
    name = "L_self_modules_decoder_modules_layer_modules_11_modules_mlp_modules_DenseReluDense_modules_wo_parameters_weight_"
    shape = [768, 2048]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.000
    std = 0.022
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_final_layer_norm_parameters_weight_:
    name = "L_self_modules_decoder_modules_final_layer_norm_parameters_weight_"
    shape = [768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_decoder_modules_lm_head_parameters_weight_:
    name = "L_self_modules_decoder_modules_lm_head_parameters_weight_"
    shape = [50244, 768]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = -0.000
    std = 0.036
    data = None
