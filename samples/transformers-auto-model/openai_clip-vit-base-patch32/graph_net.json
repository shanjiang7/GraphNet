{
    "framework": "torch",
    "num_devices_required": 1,
    "num_nodes_required": 1,
    "dynamic": false,
    "model_name": "openai/clip-vit-base-patch32",
    "source": "huggingface_hub",
    "original_tag": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "clip",
        "zero-shot-image-classification",
        "vision",
        "arxiv:2103.00020",
        "arxiv:1908.04913",
        "endpoints_compatible",
        "region:us"
    ],
    "heuristic_tag": "multimodal",
    "symbolic_dimension_reifier": "naive_cv_sym_dim_reifier",
    "data_type_generalization_passes": [
        "dtype_generalization_pass_float16",
        "dtype_generalization_pass_bfloat16"
    ]
}