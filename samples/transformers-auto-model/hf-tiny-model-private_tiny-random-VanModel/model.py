import torch


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_pixel_values_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_
        l_pixel_values_ = L_pixel_values_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_
        hidden_state = torch.conv2d(
            l_pixel_values_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_,
            (4, 4),
            (3, 3),
            (1, 1),
            1,
        )
        l_pixel_values_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_1 = torch.nn.functional.batch_norm(
            hidden_state,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_2 = torch.nn.functional.batch_norm(
            hidden_state_1,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_1 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_2 = torch._C._nn.gelu(input_1)
        input_1 = None
        hidden_state_3 = torch.conv2d(
            input_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            16,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_4 = torch.conv2d(
            hidden_state_3,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            16,
        )
        hidden_state_3 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_5 = torch.conv2d(
            hidden_state_4,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_4 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended = input_2 * hidden_state_5
        input_2 = hidden_state_5 = None
        hidden_state_6 = torch.conv2d(
            attended,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_7 = hidden_state_6 + hidden_state_2
        hidden_state_6 = hidden_state_2 = None
        unsqueeze = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_1 = unsqueeze.unsqueeze(-1)
        unsqueeze = None
        hidden_state_8 = unsqueeze_1 * hidden_state_7
        unsqueeze_1 = hidden_state_7 = None
        hidden_state_9 = hidden_state_1 + hidden_state_8
        hidden_state_1 = hidden_state_8 = None
        hidden_state_10 = torch.nn.functional.batch_norm(
            hidden_state_9,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_11 = torch.conv2d(
            hidden_state_10,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_10 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_12 = torch.conv2d(
            hidden_state_11,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            128,
        )
        hidden_state_11 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_13 = torch._C._nn.gelu(hidden_state_12)
        hidden_state_12 = None
        hidden_state_14 = torch.nn.functional.dropout(
            hidden_state_13, 0.0, False, False
        )
        hidden_state_13 = None
        hidden_state_15 = torch.conv2d(
            hidden_state_14,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_14 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_16 = torch.nn.functional.dropout(
            hidden_state_15, 0.0, False, False
        )
        hidden_state_15 = None
        unsqueeze_2 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_3 = unsqueeze_2.unsqueeze(-1)
        unsqueeze_2 = None
        hidden_state_17 = unsqueeze_3 * hidden_state_16
        unsqueeze_3 = hidden_state_16 = None
        hidden_state_18 = hidden_state_9 + hidden_state_17
        hidden_state_9 = hidden_state_17 = None
        flatten = hidden_state_18.flatten(2)
        hidden_state_18 = None
        hidden_state_19 = flatten.transpose(1, 2)
        flatten = None
        hidden_state_20 = torch.nn.functional.layer_norm(
            hidden_state_19,
            (16,),
            l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_19 = l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_ = (None)
        view = hidden_state_20.view(1, 56, 56, 16)
        hidden_state_20 = None
        hidden_state_21 = view.permute(0, 3, 1, 2)
        view = None
        hidden_state_22 = torch.conv2d(
            hidden_state_21,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_21 = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_23 = torch.nn.functional.batch_norm(
            hidden_state_22,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_22 = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_24 = torch.nn.functional.batch_norm(
            hidden_state_23,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_3 = torch.conv2d(
            hidden_state_24,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_4 = torch._C._nn.gelu(input_3)
        input_3 = None
        hidden_state_25 = torch.conv2d(
            input_4,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            32,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_26 = torch.conv2d(
            hidden_state_25,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            32,
        )
        hidden_state_25 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_27 = torch.conv2d(
            hidden_state_26,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_26 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_1 = input_4 * hidden_state_27
        input_4 = hidden_state_27 = None
        hidden_state_28 = torch.conv2d(
            attended_1,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_1 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_29 = hidden_state_28 + hidden_state_24
        hidden_state_28 = hidden_state_24 = None
        unsqueeze_4 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_5 = unsqueeze_4.unsqueeze(-1)
        unsqueeze_4 = None
        hidden_state_30 = unsqueeze_5 * hidden_state_29
        unsqueeze_5 = hidden_state_29 = None
        hidden_state_31 = hidden_state_23 + hidden_state_30
        hidden_state_23 = hidden_state_30 = None
        hidden_state_32 = torch.nn.functional.batch_norm(
            hidden_state_31,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_33 = torch.conv2d(
            hidden_state_32,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_32 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_34 = torch.conv2d(
            hidden_state_33,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            256,
        )
        hidden_state_33 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_35 = torch._C._nn.gelu(hidden_state_34)
        hidden_state_34 = None
        hidden_state_36 = torch.nn.functional.dropout(
            hidden_state_35, 0.0, False, False
        )
        hidden_state_35 = None
        hidden_state_37 = torch.conv2d(
            hidden_state_36,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_36 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_38 = torch.nn.functional.dropout(
            hidden_state_37, 0.0, False, False
        )
        hidden_state_37 = None
        unsqueeze_6 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_7 = unsqueeze_6.unsqueeze(-1)
        unsqueeze_6 = None
        hidden_state_39 = unsqueeze_7 * hidden_state_38
        unsqueeze_7 = hidden_state_38 = None
        hidden_state_40 = hidden_state_31 + hidden_state_39
        hidden_state_31 = hidden_state_39 = None
        flatten_1 = hidden_state_40.flatten(2)
        hidden_state_40 = None
        hidden_state_41 = flatten_1.transpose(1, 2)
        flatten_1 = None
        hidden_state_42 = torch.nn.functional.layer_norm(
            hidden_state_41,
            (32,),
            l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_41 = l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_ = (None)
        view_1 = hidden_state_42.view(1, 28, 28, 32)
        hidden_state_42 = None
        hidden_state_43 = view_1.permute(0, 3, 1, 2)
        view_1 = None
        hidden_state_44 = torch.conv2d(
            hidden_state_43,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_43 = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_45 = torch.nn.functional.batch_norm(
            hidden_state_44,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_44 = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_46 = torch.nn.functional.batch_norm(
            hidden_state_45,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_5 = torch.conv2d(
            hidden_state_46,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_6 = torch._C._nn.gelu(input_5)
        input_5 = None
        hidden_state_47 = torch.conv2d(
            input_6,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_48 = torch.conv2d(
            hidden_state_47,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            64,
        )
        hidden_state_47 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_49 = torch.conv2d(
            hidden_state_48,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_48 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_2 = input_6 * hidden_state_49
        input_6 = hidden_state_49 = None
        hidden_state_50 = torch.conv2d(
            attended_2,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_2 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_51 = hidden_state_50 + hidden_state_46
        hidden_state_50 = hidden_state_46 = None
        unsqueeze_8 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_9 = unsqueeze_8.unsqueeze(-1)
        unsqueeze_8 = None
        hidden_state_52 = unsqueeze_9 * hidden_state_51
        unsqueeze_9 = hidden_state_51 = None
        hidden_state_53 = hidden_state_45 + hidden_state_52
        hidden_state_45 = hidden_state_52 = None
        hidden_state_54 = torch.nn.functional.batch_norm(
            hidden_state_53,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_55 = torch.conv2d(
            hidden_state_54,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_54 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_56 = torch.conv2d(
            hidden_state_55,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            256,
        )
        hidden_state_55 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_57 = torch._C._nn.gelu(hidden_state_56)
        hidden_state_56 = None
        hidden_state_58 = torch.nn.functional.dropout(
            hidden_state_57, 0.0, False, False
        )
        hidden_state_57 = None
        hidden_state_59 = torch.conv2d(
            hidden_state_58,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_58 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_60 = torch.nn.functional.dropout(
            hidden_state_59, 0.0, False, False
        )
        hidden_state_59 = None
        unsqueeze_10 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_11 = unsqueeze_10.unsqueeze(-1)
        unsqueeze_10 = None
        hidden_state_61 = unsqueeze_11 * hidden_state_60
        unsqueeze_11 = hidden_state_60 = None
        hidden_state_62 = hidden_state_53 + hidden_state_61
        hidden_state_53 = hidden_state_61 = None
        flatten_2 = hidden_state_62.flatten(2)
        hidden_state_62 = None
        hidden_state_63 = flatten_2.transpose(1, 2)
        flatten_2 = None
        hidden_state_64 = torch.nn.functional.layer_norm(
            hidden_state_63,
            (64,),
            l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_63 = l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_ = (None)
        view_2 = hidden_state_64.view(1, 14, 14, 64)
        hidden_state_64 = None
        hidden_state_65 = view_2.permute(0, 3, 1, 2)
        view_2 = None
        hidden_state_66 = torch.conv2d(
            hidden_state_65,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_65 = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_67 = torch.nn.functional.batch_norm(
            hidden_state_66,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_66 = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_68 = torch.nn.functional.batch_norm(
            hidden_state_67,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_7 = torch.conv2d(
            hidden_state_68,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_8 = torch._C._nn.gelu(input_7)
        input_7 = None
        hidden_state_69 = torch.conv2d(
            input_8,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_70 = torch.conv2d(
            hidden_state_69,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_69 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_71 = torch.conv2d(
            hidden_state_70,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_70 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_3 = input_8 * hidden_state_71
        input_8 = hidden_state_71 = None
        hidden_state_72 = torch.conv2d(
            attended_3,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_3 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_73 = hidden_state_72 + hidden_state_68
        hidden_state_72 = hidden_state_68 = None
        unsqueeze_12 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_13 = unsqueeze_12.unsqueeze(-1)
        unsqueeze_12 = None
        hidden_state_74 = unsqueeze_13 * hidden_state_73
        unsqueeze_13 = hidden_state_73 = None
        hidden_state_75 = hidden_state_67 + hidden_state_74
        hidden_state_67 = hidden_state_74 = None
        hidden_state_76 = torch.nn.functional.batch_norm(
            hidden_state_75,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_77 = torch.conv2d(
            hidden_state_76,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_76 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_78 = torch.conv2d(
            hidden_state_77,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            512,
        )
        hidden_state_77 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_79 = torch._C._nn.gelu(hidden_state_78)
        hidden_state_78 = None
        hidden_state_80 = torch.nn.functional.dropout(
            hidden_state_79, 0.0, False, False
        )
        hidden_state_79 = None
        hidden_state_81 = torch.conv2d(
            hidden_state_80,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_80 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_82 = torch.nn.functional.dropout(
            hidden_state_81, 0.0, False, False
        )
        hidden_state_81 = None
        unsqueeze_14 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_15 = unsqueeze_14.unsqueeze(-1)
        unsqueeze_14 = None
        hidden_state_83 = unsqueeze_15 * hidden_state_82
        unsqueeze_15 = hidden_state_82 = None
        hidden_state_84 = hidden_state_75 + hidden_state_83
        hidden_state_75 = hidden_state_83 = None
        flatten_3 = hidden_state_84.flatten(2)
        hidden_state_84 = None
        hidden_state_85 = flatten_3.transpose(1, 2)
        flatten_3 = None
        hidden_state_86 = torch.nn.functional.layer_norm(
            hidden_state_85,
            (128,),
            l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_85 = l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_ = (None)
        view_3 = hidden_state_86.view(1, 7, 7, 128)
        hidden_state_86 = None
        hidden_state_87 = view_3.permute(0, 3, 1, 2)
        view_3 = None
        pooled_output = hidden_state_87.mean(dim=[-2, -1])
        return (hidden_state_87, pooled_output)
