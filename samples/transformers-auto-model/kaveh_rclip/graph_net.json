{
    "framework": "torch",
    "num_devices_required": 1,
    "num_nodes_required": 1,
    "dynamic": false,
    "model_name": "kaveh/rclip",
    "source": "huggingface_hub",
    "original_tag": [
        "transformers",
        "pytorch",
        "tensorboard",
        "safetensors",
        "vision-text-dual-encoder",
        "feature-extraction",
        "clip",
        "vision",
        "medical",
        "bert",
        "zero-shot-image-classification",
        "en",
        "base_model:openai/clip-vit-large-patch14",
        "base_model:finetune:openai/clip-vit-large-patch14",
        "doi:10.57967/hf/0896",
        "license:gpl-3.0",
        "endpoints_compatible",
        "region:us"
    ],
    "heuristic_tag": "multimodal",
    "symbolic_dimension_reifier": "naive_cv_sym_dim_reifier",
    "data_type_generalization_passes": [
        "dtype_generalization_pass_float16",
        "dtype_generalization_pass_bfloat16"
    ]
}