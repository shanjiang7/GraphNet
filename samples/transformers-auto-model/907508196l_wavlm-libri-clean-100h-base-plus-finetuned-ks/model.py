import torch

from torch import device


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_input_values_: torch.Tensor,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_projector_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_projector_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_classifier_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_input_values_ = L_input_values_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_ = L_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_
        l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_ = L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_
        l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_ = L_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_ = L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_
        l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_ = L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_
        l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_ = (
            L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_
        )
        l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_ = (
            L_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_ = L_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_
        l_self_modules_projector_parameters_weight_ = (
            L_self_modules_projector_parameters_weight_
        )
        l_self_modules_projector_parameters_bias_ = (
            L_self_modules_projector_parameters_bias_
        )
        l_self_modules_classifier_parameters_weight_ = (
            L_self_modules_classifier_parameters_weight_
        )
        l_self_modules_classifier_parameters_bias_ = (
            L_self_modules_classifier_parameters_bias_
        )
        hidden_states = l_input_values_[(slice(None, None, None), None)]
        l_input_values_ = None
        hidden_states_1 = torch.conv1d(
            hidden_states,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_,
            None,
            (5,),
            (0,),
            (1,),
            1,
        )
        hidden_states = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_ = (None)
        hidden_states_2 = torch.nn.functional.group_norm(
            hidden_states_1,
            512,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_1 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_3 = torch._C._nn.gelu(hidden_states_2)
        hidden_states_2 = None
        hidden_states_4 = torch.conv1d(
            hidden_states_3,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        hidden_states_3 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_ = (None)
        hidden_states_5 = torch._C._nn.gelu(hidden_states_4)
        hidden_states_4 = None
        hidden_states_6 = torch.conv1d(
            hidden_states_5,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        hidden_states_5 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_ = (None)
        hidden_states_7 = torch._C._nn.gelu(hidden_states_6)
        hidden_states_6 = None
        hidden_states_8 = torch.conv1d(
            hidden_states_7,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        hidden_states_7 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_ = (None)
        hidden_states_9 = torch._C._nn.gelu(hidden_states_8)
        hidden_states_8 = None
        hidden_states_10 = torch.conv1d(
            hidden_states_9,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        hidden_states_9 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_ = (None)
        hidden_states_11 = torch._C._nn.gelu(hidden_states_10)
        hidden_states_10 = None
        hidden_states_12 = torch.conv1d(
            hidden_states_11,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        hidden_states_11 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_ = (None)
        hidden_states_13 = torch._C._nn.gelu(hidden_states_12)
        hidden_states_12 = None
        hidden_states_14 = torch.conv1d(
            hidden_states_13,
            l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        hidden_states_13 = l_self_modules_wavlm_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_ = (None)
        hidden_states_15 = torch._C._nn.gelu(hidden_states_14)
        hidden_states_14 = None
        extract_features = hidden_states_15.transpose(1, 2)
        hidden_states_15 = None
        norm_hidden_states = torch.nn.functional.layer_norm(
            extract_features,
            (512,),
            l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        extract_features = l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_feature_projection_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_16 = torch._C._nn.linear(
            norm_hidden_states,
            l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_,
            l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_,
        )
        norm_hidden_states = l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_weight_ = l_self_modules_wavlm_modules_feature_projection_modules_projection_parameters_bias_ = (None)
        hidden_states_17 = torch.nn.functional.dropout(
            hidden_states_16, 0.0, False, False
        )
        hidden_states_16 = None
        hidden_states_18 = hidden_states_17.transpose(1, 2)
        x = torch._weight_norm(
            l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_,
            l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_,
            2,
        )
        l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_ = l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_ = (None)
        hidden_states_19 = torch.conv1d(
            hidden_states_18,
            x,
            l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_,
            (1,),
            (64,),
            (1,),
            16,
        )
        hidden_states_18 = (
            x
        ) = l_self_modules_wavlm_modules_encoder_modules_pos_conv_embed_modules_conv_parameters_bias_ = (None)
        hidden_states_20 = hidden_states_19[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        hidden_states_19 = None
        hidden_states_21 = torch._C._nn.gelu(hidden_states_20)
        hidden_states_20 = None
        hidden_states_22 = hidden_states_21.transpose(1, 2)
        hidden_states_21 = None
        hidden_states_23 = hidden_states_17 + hidden_states_22
        hidden_states_17 = hidden_states_22 = None
        hidden_states_24 = torch.nn.functional.layer_norm(
            hidden_states_23,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_23 = (
            l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_weight_
        ) = (
            l_self_modules_wavlm_modules_encoder_modules_layer_norm_parameters_bias_
        ) = None
        hidden_states_25 = torch.nn.functional.dropout(
            hidden_states_24, 0.0, False, False
        )
        hidden_states_24 = None
        dropout_probability = torch.rand([])
        dropout_probability = None
        arange = torch.arange(249, dtype=torch.int64)
        context_position = arange[(slice(None, None, None), None)]
        arange = None
        arange_1 = torch.arange(249, dtype=torch.int64)
        memory_position = arange_1[(None, slice(None, None, None))]
        arange_1 = None
        relative_position = memory_position - context_position
        memory_position = context_position = None
        gt = relative_position > 0
        to = gt.to(torch.int64)
        gt = None
        relative_buckets = to * 160
        to = None
        relative_positions = torch.abs(relative_position)
        relative_position = None
        is_small = relative_positions < 80
        float_1 = relative_positions.float()
        truediv = float_1 / 80
        float_1 = None
        relative_positions_if_large = torch.log(truediv)
        truediv = None
        relative_positions_if_large_1 = relative_positions_if_large / 2.302585092994046
        relative_positions_if_large = None
        relative_positions_if_large_2 = relative_positions_if_large_1 * 80
        relative_positions_if_large_1 = None
        add_1 = 80 + relative_positions_if_large_2
        relative_positions_if_large_2 = None
        relative_position_if_large = add_1.to(torch.int64)
        add_1 = None
        full_like = torch.full_like(relative_position_if_large, 159)
        relative_position_if_large_1 = torch.min(relative_position_if_large, full_like)
        relative_position_if_large = full_like = None
        where = torch.where(is_small, relative_positions, relative_position_if_large_1)
        is_small = relative_positions = relative_position_if_large_1 = None
        relative_buckets += where
        relative_buckets_1 = relative_buckets
        relative_buckets = where = None
        relative_position_bucket = relative_buckets_1.to(device(type="cuda", index=0))
        relative_buckets_1 = None
        values = torch.nn.functional.embedding(
            relative_position_bucket,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_,
            None,
            None,
            2.0,
            False,
            False,
        )
        relative_position_bucket = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_ = (None)
        values_1 = values.permute([2, 0, 1])
        values = None
        unsqueeze = values_1.unsqueeze(0)
        values_1 = None
        repeat = unsqueeze.repeat(1, 1, 1, 1)
        unsqueeze = None
        position_bias = repeat.view(12, 249, 249)
        repeat = None
        gated_hidden_states = hidden_states_25.view((1, 249, 12, -1))
        gated_hidden_states_1 = gated_hidden_states.permute(0, 2, 1, 3)
        gated_hidden_states = None
        relative_position_proj = torch._C._nn.linear(
            gated_hidden_states_1,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_1 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_2 = relative_position_proj.view((1, 12, 249, 2, 4))
        relative_position_proj = None
        relative_position_proj_1 = view_2.sum(-1)
        view_2 = None
        sigmoid = torch.sigmoid(relative_position_proj_1)
        relative_position_proj_1 = None
        chunk = sigmoid.chunk(2, dim=-1)
        sigmoid = None
        gate_a = chunk[0]
        gate_b = chunk[1]
        chunk = None
        mul_2 = (
            gate_b
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_1 = mul_2 - 1.0
        mul_2 = None
        mul_3 = gate_a * sub_1
        gate_a = sub_1 = None
        gate_output = mul_3 + 2.0
        mul_3 = None
        view_3 = gate_output.view(12, -1, 1)
        gate_output = None
        gated_position_bias = view_3 * position_bias
        view_3 = None
        gated_position_bias_1 = gated_position_bias.view((-1, 249, 249))
        gated_position_bias = None
        query = hidden_states_25.transpose(0, 1)
        empty = torch.empty([0])
        cat = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward = torch.nn.functional.multi_head_attention_forward(
            query,
            query,
            query,
            768,
            12,
            empty,
            cat,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_1,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_,
        )
        query = (
            empty
        ) = (
            cat
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_1
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output = multi_head_attention_forward[0]
        multi_head_attention_forward = None
        attn_output_1 = attn_output.transpose(0, 1)
        attn_output = None
        hidden_states_26 = torch.nn.functional.dropout(attn_output_1, 0.0, False, False)
        attn_output_1 = None
        hidden_states_27 = hidden_states_25 + hidden_states_26
        hidden_states_25 = hidden_states_26 = None
        hidden_states_28 = torch.nn.functional.layer_norm(
            hidden_states_27,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_27 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_29 = torch._C._nn.linear(
            hidden_states_28,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_30 = torch._C._nn.gelu(hidden_states_29)
        hidden_states_29 = None
        hidden_states_31 = torch.nn.functional.dropout(
            hidden_states_30, 0.0, False, False
        )
        hidden_states_30 = None
        hidden_states_32 = torch._C._nn.linear(
            hidden_states_31,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_31 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_33 = torch.nn.functional.dropout(
            hidden_states_32, 0.0, False, False
        )
        hidden_states_32 = None
        hidden_states_34 = hidden_states_28 + hidden_states_33
        hidden_states_28 = hidden_states_33 = None
        hidden_states_35 = torch.nn.functional.layer_norm(
            hidden_states_34,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_34 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_1 = torch.rand([])
        dropout_probability_1 = None
        gated_hidden_states_2 = hidden_states_35.view((1, 249, 12, -1))
        gated_hidden_states_3 = gated_hidden_states_2.permute(0, 2, 1, 3)
        gated_hidden_states_2 = None
        relative_position_proj_2 = torch._C._nn.linear(
            gated_hidden_states_3,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_3 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_6 = relative_position_proj_2.view((1, 12, 249, 2, 4))
        relative_position_proj_2 = None
        relative_position_proj_3 = view_6.sum(-1)
        view_6 = None
        sigmoid_1 = torch.sigmoid(relative_position_proj_3)
        relative_position_proj_3 = None
        chunk_1 = sigmoid_1.chunk(2, dim=-1)
        sigmoid_1 = None
        gate_a_1 = chunk_1[0]
        gate_b_1 = chunk_1[1]
        chunk_1 = None
        mul_5 = (
            gate_b_1
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_1 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_2 = mul_5 - 1.0
        mul_5 = None
        mul_6 = gate_a_1 * sub_2
        gate_a_1 = sub_2 = None
        gate_output_1 = mul_6 + 2.0
        mul_6 = None
        view_7 = gate_output_1.view(12, -1, 1)
        gate_output_1 = None
        gated_position_bias_2 = view_7 * position_bias
        view_7 = None
        gated_position_bias_3 = gated_position_bias_2.view((-1, 249, 249))
        gated_position_bias_2 = None
        query_1 = hidden_states_35.transpose(0, 1)
        empty_1 = torch.empty([0])
        cat_1 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_1 = torch.nn.functional.multi_head_attention_forward(
            query_1,
            query_1,
            query_1,
            768,
            12,
            empty_1,
            cat_1,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_3,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_1 = (
            empty_1
        ) = (
            cat_1
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_3
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_2 = multi_head_attention_forward_1[0]
        multi_head_attention_forward_1 = None
        attn_output_3 = attn_output_2.transpose(0, 1)
        attn_output_2 = None
        hidden_states_36 = torch.nn.functional.dropout(attn_output_3, 0.0, False, False)
        attn_output_3 = None
        hidden_states_37 = hidden_states_35 + hidden_states_36
        hidden_states_35 = hidden_states_36 = None
        hidden_states_38 = torch.nn.functional.layer_norm(
            hidden_states_37,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_37 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_39 = torch._C._nn.linear(
            hidden_states_38,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_40 = torch._C._nn.gelu(hidden_states_39)
        hidden_states_39 = None
        hidden_states_41 = torch.nn.functional.dropout(
            hidden_states_40, 0.0, False, False
        )
        hidden_states_40 = None
        hidden_states_42 = torch._C._nn.linear(
            hidden_states_41,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_41 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_43 = torch.nn.functional.dropout(
            hidden_states_42, 0.0, False, False
        )
        hidden_states_42 = None
        hidden_states_44 = hidden_states_38 + hidden_states_43
        hidden_states_38 = hidden_states_43 = None
        hidden_states_45 = torch.nn.functional.layer_norm(
            hidden_states_44,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_44 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_2 = torch.rand([])
        dropout_probability_2 = None
        gated_hidden_states_4 = hidden_states_45.view((1, 249, 12, -1))
        gated_hidden_states_5 = gated_hidden_states_4.permute(0, 2, 1, 3)
        gated_hidden_states_4 = None
        relative_position_proj_4 = torch._C._nn.linear(
            gated_hidden_states_5,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_5 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_10 = relative_position_proj_4.view((1, 12, 249, 2, 4))
        relative_position_proj_4 = None
        relative_position_proj_5 = view_10.sum(-1)
        view_10 = None
        sigmoid_2 = torch.sigmoid(relative_position_proj_5)
        relative_position_proj_5 = None
        chunk_2 = sigmoid_2.chunk(2, dim=-1)
        sigmoid_2 = None
        gate_a_2 = chunk_2[0]
        gate_b_2 = chunk_2[1]
        chunk_2 = None
        mul_8 = (
            gate_b_2
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_2 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_3 = mul_8 - 1.0
        mul_8 = None
        mul_9 = gate_a_2 * sub_3
        gate_a_2 = sub_3 = None
        gate_output_2 = mul_9 + 2.0
        mul_9 = None
        view_11 = gate_output_2.view(12, -1, 1)
        gate_output_2 = None
        gated_position_bias_4 = view_11 * position_bias
        view_11 = None
        gated_position_bias_5 = gated_position_bias_4.view((-1, 249, 249))
        gated_position_bias_4 = None
        query_2 = hidden_states_45.transpose(0, 1)
        empty_2 = torch.empty([0])
        cat_2 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_2 = torch.nn.functional.multi_head_attention_forward(
            query_2,
            query_2,
            query_2,
            768,
            12,
            empty_2,
            cat_2,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_5,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_2 = (
            empty_2
        ) = (
            cat_2
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_5
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_4 = multi_head_attention_forward_2[0]
        multi_head_attention_forward_2 = None
        attn_output_5 = attn_output_4.transpose(0, 1)
        attn_output_4 = None
        hidden_states_46 = torch.nn.functional.dropout(attn_output_5, 0.0, False, False)
        attn_output_5 = None
        hidden_states_47 = hidden_states_45 + hidden_states_46
        hidden_states_45 = hidden_states_46 = None
        hidden_states_48 = torch.nn.functional.layer_norm(
            hidden_states_47,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_47 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_49 = torch._C._nn.linear(
            hidden_states_48,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_50 = torch._C._nn.gelu(hidden_states_49)
        hidden_states_49 = None
        hidden_states_51 = torch.nn.functional.dropout(
            hidden_states_50, 0.0, False, False
        )
        hidden_states_50 = None
        hidden_states_52 = torch._C._nn.linear(
            hidden_states_51,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_51 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_53 = torch.nn.functional.dropout(
            hidden_states_52, 0.0, False, False
        )
        hidden_states_52 = None
        hidden_states_54 = hidden_states_48 + hidden_states_53
        hidden_states_48 = hidden_states_53 = None
        hidden_states_55 = torch.nn.functional.layer_norm(
            hidden_states_54,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_54 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_3 = torch.rand([])
        dropout_probability_3 = None
        gated_hidden_states_6 = hidden_states_55.view((1, 249, 12, -1))
        gated_hidden_states_7 = gated_hidden_states_6.permute(0, 2, 1, 3)
        gated_hidden_states_6 = None
        relative_position_proj_6 = torch._C._nn.linear(
            gated_hidden_states_7,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_7 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_14 = relative_position_proj_6.view((1, 12, 249, 2, 4))
        relative_position_proj_6 = None
        relative_position_proj_7 = view_14.sum(-1)
        view_14 = None
        sigmoid_3 = torch.sigmoid(relative_position_proj_7)
        relative_position_proj_7 = None
        chunk_3 = sigmoid_3.chunk(2, dim=-1)
        sigmoid_3 = None
        gate_a_3 = chunk_3[0]
        gate_b_3 = chunk_3[1]
        chunk_3 = None
        mul_11 = (
            gate_b_3
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_3 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_4 = mul_11 - 1.0
        mul_11 = None
        mul_12 = gate_a_3 * sub_4
        gate_a_3 = sub_4 = None
        gate_output_3 = mul_12 + 2.0
        mul_12 = None
        view_15 = gate_output_3.view(12, -1, 1)
        gate_output_3 = None
        gated_position_bias_6 = view_15 * position_bias
        view_15 = None
        gated_position_bias_7 = gated_position_bias_6.view((-1, 249, 249))
        gated_position_bias_6 = None
        query_3 = hidden_states_55.transpose(0, 1)
        empty_3 = torch.empty([0])
        cat_3 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_3 = torch.nn.functional.multi_head_attention_forward(
            query_3,
            query_3,
            query_3,
            768,
            12,
            empty_3,
            cat_3,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_7,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_3 = (
            empty_3
        ) = (
            cat_3
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_7
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_6 = multi_head_attention_forward_3[0]
        multi_head_attention_forward_3 = None
        attn_output_7 = attn_output_6.transpose(0, 1)
        attn_output_6 = None
        hidden_states_56 = torch.nn.functional.dropout(attn_output_7, 0.0, False, False)
        attn_output_7 = None
        hidden_states_57 = hidden_states_55 + hidden_states_56
        hidden_states_55 = hidden_states_56 = None
        hidden_states_58 = torch.nn.functional.layer_norm(
            hidden_states_57,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_57 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_59 = torch._C._nn.linear(
            hidden_states_58,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_60 = torch._C._nn.gelu(hidden_states_59)
        hidden_states_59 = None
        hidden_states_61 = torch.nn.functional.dropout(
            hidden_states_60, 0.0, False, False
        )
        hidden_states_60 = None
        hidden_states_62 = torch._C._nn.linear(
            hidden_states_61,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_61 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_63 = torch.nn.functional.dropout(
            hidden_states_62, 0.0, False, False
        )
        hidden_states_62 = None
        hidden_states_64 = hidden_states_58 + hidden_states_63
        hidden_states_58 = hidden_states_63 = None
        hidden_states_65 = torch.nn.functional.layer_norm(
            hidden_states_64,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_64 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_4 = torch.rand([])
        dropout_probability_4 = None
        gated_hidden_states_8 = hidden_states_65.view((1, 249, 12, -1))
        gated_hidden_states_9 = gated_hidden_states_8.permute(0, 2, 1, 3)
        gated_hidden_states_8 = None
        relative_position_proj_8 = torch._C._nn.linear(
            gated_hidden_states_9,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_9 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_18 = relative_position_proj_8.view((1, 12, 249, 2, 4))
        relative_position_proj_8 = None
        relative_position_proj_9 = view_18.sum(-1)
        view_18 = None
        sigmoid_4 = torch.sigmoid(relative_position_proj_9)
        relative_position_proj_9 = None
        chunk_4 = sigmoid_4.chunk(2, dim=-1)
        sigmoid_4 = None
        gate_a_4 = chunk_4[0]
        gate_b_4 = chunk_4[1]
        chunk_4 = None
        mul_14 = (
            gate_b_4
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_4 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_5 = mul_14 - 1.0
        mul_14 = None
        mul_15 = gate_a_4 * sub_5
        gate_a_4 = sub_5 = None
        gate_output_4 = mul_15 + 2.0
        mul_15 = None
        view_19 = gate_output_4.view(12, -1, 1)
        gate_output_4 = None
        gated_position_bias_8 = view_19 * position_bias
        view_19 = None
        gated_position_bias_9 = gated_position_bias_8.view((-1, 249, 249))
        gated_position_bias_8 = None
        query_4 = hidden_states_65.transpose(0, 1)
        empty_4 = torch.empty([0])
        cat_4 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_4 = torch.nn.functional.multi_head_attention_forward(
            query_4,
            query_4,
            query_4,
            768,
            12,
            empty_4,
            cat_4,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_9,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_4 = (
            empty_4
        ) = (
            cat_4
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_9
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_8 = multi_head_attention_forward_4[0]
        multi_head_attention_forward_4 = None
        attn_output_9 = attn_output_8.transpose(0, 1)
        attn_output_8 = None
        hidden_states_66 = torch.nn.functional.dropout(attn_output_9, 0.0, False, False)
        attn_output_9 = None
        hidden_states_67 = hidden_states_65 + hidden_states_66
        hidden_states_65 = hidden_states_66 = None
        hidden_states_68 = torch.nn.functional.layer_norm(
            hidden_states_67,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_67 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_69 = torch._C._nn.linear(
            hidden_states_68,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_70 = torch._C._nn.gelu(hidden_states_69)
        hidden_states_69 = None
        hidden_states_71 = torch.nn.functional.dropout(
            hidden_states_70, 0.0, False, False
        )
        hidden_states_70 = None
        hidden_states_72 = torch._C._nn.linear(
            hidden_states_71,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_71 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_73 = torch.nn.functional.dropout(
            hidden_states_72, 0.0, False, False
        )
        hidden_states_72 = None
        hidden_states_74 = hidden_states_68 + hidden_states_73
        hidden_states_68 = hidden_states_73 = None
        hidden_states_75 = torch.nn.functional.layer_norm(
            hidden_states_74,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_74 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_5 = torch.rand([])
        dropout_probability_5 = None
        gated_hidden_states_10 = hidden_states_75.view((1, 249, 12, -1))
        gated_hidden_states_11 = gated_hidden_states_10.permute(0, 2, 1, 3)
        gated_hidden_states_10 = None
        relative_position_proj_10 = torch._C._nn.linear(
            gated_hidden_states_11,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_11 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_22 = relative_position_proj_10.view((1, 12, 249, 2, 4))
        relative_position_proj_10 = None
        relative_position_proj_11 = view_22.sum(-1)
        view_22 = None
        sigmoid_5 = torch.sigmoid(relative_position_proj_11)
        relative_position_proj_11 = None
        chunk_5 = sigmoid_5.chunk(2, dim=-1)
        sigmoid_5 = None
        gate_a_5 = chunk_5[0]
        gate_b_5 = chunk_5[1]
        chunk_5 = None
        mul_17 = (
            gate_b_5
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_5 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_6 = mul_17 - 1.0
        mul_17 = None
        mul_18 = gate_a_5 * sub_6
        gate_a_5 = sub_6 = None
        gate_output_5 = mul_18 + 2.0
        mul_18 = None
        view_23 = gate_output_5.view(12, -1, 1)
        gate_output_5 = None
        gated_position_bias_10 = view_23 * position_bias
        view_23 = None
        gated_position_bias_11 = gated_position_bias_10.view((-1, 249, 249))
        gated_position_bias_10 = None
        query_5 = hidden_states_75.transpose(0, 1)
        empty_5 = torch.empty([0])
        cat_5 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_5 = torch.nn.functional.multi_head_attention_forward(
            query_5,
            query_5,
            query_5,
            768,
            12,
            empty_5,
            cat_5,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_11,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_5 = (
            empty_5
        ) = (
            cat_5
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_11
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_10 = multi_head_attention_forward_5[0]
        multi_head_attention_forward_5 = None
        attn_output_11 = attn_output_10.transpose(0, 1)
        attn_output_10 = None
        hidden_states_76 = torch.nn.functional.dropout(
            attn_output_11, 0.0, False, False
        )
        attn_output_11 = None
        hidden_states_77 = hidden_states_75 + hidden_states_76
        hidden_states_75 = hidden_states_76 = None
        hidden_states_78 = torch.nn.functional.layer_norm(
            hidden_states_77,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_77 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_79 = torch._C._nn.linear(
            hidden_states_78,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_80 = torch._C._nn.gelu(hidden_states_79)
        hidden_states_79 = None
        hidden_states_81 = torch.nn.functional.dropout(
            hidden_states_80, 0.0, False, False
        )
        hidden_states_80 = None
        hidden_states_82 = torch._C._nn.linear(
            hidden_states_81,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_81 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_83 = torch.nn.functional.dropout(
            hidden_states_82, 0.0, False, False
        )
        hidden_states_82 = None
        hidden_states_84 = hidden_states_78 + hidden_states_83
        hidden_states_78 = hidden_states_83 = None
        hidden_states_85 = torch.nn.functional.layer_norm(
            hidden_states_84,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_84 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_6 = torch.rand([])
        dropout_probability_6 = None
        gated_hidden_states_12 = hidden_states_85.view((1, 249, 12, -1))
        gated_hidden_states_13 = gated_hidden_states_12.permute(0, 2, 1, 3)
        gated_hidden_states_12 = None
        relative_position_proj_12 = torch._C._nn.linear(
            gated_hidden_states_13,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_13 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_26 = relative_position_proj_12.view((1, 12, 249, 2, 4))
        relative_position_proj_12 = None
        relative_position_proj_13 = view_26.sum(-1)
        view_26 = None
        sigmoid_6 = torch.sigmoid(relative_position_proj_13)
        relative_position_proj_13 = None
        chunk_6 = sigmoid_6.chunk(2, dim=-1)
        sigmoid_6 = None
        gate_a_6 = chunk_6[0]
        gate_b_6 = chunk_6[1]
        chunk_6 = None
        mul_20 = (
            gate_b_6
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_6 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_7 = mul_20 - 1.0
        mul_20 = None
        mul_21 = gate_a_6 * sub_7
        gate_a_6 = sub_7 = None
        gate_output_6 = mul_21 + 2.0
        mul_21 = None
        view_27 = gate_output_6.view(12, -1, 1)
        gate_output_6 = None
        gated_position_bias_12 = view_27 * position_bias
        view_27 = None
        gated_position_bias_13 = gated_position_bias_12.view((-1, 249, 249))
        gated_position_bias_12 = None
        query_6 = hidden_states_85.transpose(0, 1)
        empty_6 = torch.empty([0])
        cat_6 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_6 = torch.nn.functional.multi_head_attention_forward(
            query_6,
            query_6,
            query_6,
            768,
            12,
            empty_6,
            cat_6,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_13,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_6 = (
            empty_6
        ) = (
            cat_6
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_13
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_12 = multi_head_attention_forward_6[0]
        multi_head_attention_forward_6 = None
        attn_output_13 = attn_output_12.transpose(0, 1)
        attn_output_12 = None
        hidden_states_86 = torch.nn.functional.dropout(
            attn_output_13, 0.0, False, False
        )
        attn_output_13 = None
        hidden_states_87 = hidden_states_85 + hidden_states_86
        hidden_states_85 = hidden_states_86 = None
        hidden_states_88 = torch.nn.functional.layer_norm(
            hidden_states_87,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_87 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_89 = torch._C._nn.linear(
            hidden_states_88,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_90 = torch._C._nn.gelu(hidden_states_89)
        hidden_states_89 = None
        hidden_states_91 = torch.nn.functional.dropout(
            hidden_states_90, 0.0, False, False
        )
        hidden_states_90 = None
        hidden_states_92 = torch._C._nn.linear(
            hidden_states_91,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_91 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_93 = torch.nn.functional.dropout(
            hidden_states_92, 0.0, False, False
        )
        hidden_states_92 = None
        hidden_states_94 = hidden_states_88 + hidden_states_93
        hidden_states_88 = hidden_states_93 = None
        hidden_states_95 = torch.nn.functional.layer_norm(
            hidden_states_94,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_94 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_7 = torch.rand([])
        dropout_probability_7 = None
        gated_hidden_states_14 = hidden_states_95.view((1, 249, 12, -1))
        gated_hidden_states_15 = gated_hidden_states_14.permute(0, 2, 1, 3)
        gated_hidden_states_14 = None
        relative_position_proj_14 = torch._C._nn.linear(
            gated_hidden_states_15,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_15 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_30 = relative_position_proj_14.view((1, 12, 249, 2, 4))
        relative_position_proj_14 = None
        relative_position_proj_15 = view_30.sum(-1)
        view_30 = None
        sigmoid_7 = torch.sigmoid(relative_position_proj_15)
        relative_position_proj_15 = None
        chunk_7 = sigmoid_7.chunk(2, dim=-1)
        sigmoid_7 = None
        gate_a_7 = chunk_7[0]
        gate_b_7 = chunk_7[1]
        chunk_7 = None
        mul_23 = (
            gate_b_7
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_7 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_8 = mul_23 - 1.0
        mul_23 = None
        mul_24 = gate_a_7 * sub_8
        gate_a_7 = sub_8 = None
        gate_output_7 = mul_24 + 2.0
        mul_24 = None
        view_31 = gate_output_7.view(12, -1, 1)
        gate_output_7 = None
        gated_position_bias_14 = view_31 * position_bias
        view_31 = None
        gated_position_bias_15 = gated_position_bias_14.view((-1, 249, 249))
        gated_position_bias_14 = None
        query_7 = hidden_states_95.transpose(0, 1)
        empty_7 = torch.empty([0])
        cat_7 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_7 = torch.nn.functional.multi_head_attention_forward(
            query_7,
            query_7,
            query_7,
            768,
            12,
            empty_7,
            cat_7,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_15,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_7 = (
            empty_7
        ) = (
            cat_7
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_15
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_14 = multi_head_attention_forward_7[0]
        multi_head_attention_forward_7 = None
        attn_output_15 = attn_output_14.transpose(0, 1)
        attn_output_14 = None
        hidden_states_96 = torch.nn.functional.dropout(
            attn_output_15, 0.0, False, False
        )
        attn_output_15 = None
        hidden_states_97 = hidden_states_95 + hidden_states_96
        hidden_states_95 = hidden_states_96 = None
        hidden_states_98 = torch.nn.functional.layer_norm(
            hidden_states_97,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_97 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_99 = torch._C._nn.linear(
            hidden_states_98,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_100 = torch._C._nn.gelu(hidden_states_99)
        hidden_states_99 = None
        hidden_states_101 = torch.nn.functional.dropout(
            hidden_states_100, 0.0, False, False
        )
        hidden_states_100 = None
        hidden_states_102 = torch._C._nn.linear(
            hidden_states_101,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_101 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_103 = torch.nn.functional.dropout(
            hidden_states_102, 0.0, False, False
        )
        hidden_states_102 = None
        hidden_states_104 = hidden_states_98 + hidden_states_103
        hidden_states_98 = hidden_states_103 = None
        hidden_states_105 = torch.nn.functional.layer_norm(
            hidden_states_104,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_104 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_8 = torch.rand([])
        dropout_probability_8 = None
        gated_hidden_states_16 = hidden_states_105.view((1, 249, 12, -1))
        gated_hidden_states_17 = gated_hidden_states_16.permute(0, 2, 1, 3)
        gated_hidden_states_16 = None
        relative_position_proj_16 = torch._C._nn.linear(
            gated_hidden_states_17,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_17 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_34 = relative_position_proj_16.view((1, 12, 249, 2, 4))
        relative_position_proj_16 = None
        relative_position_proj_17 = view_34.sum(-1)
        view_34 = None
        sigmoid_8 = torch.sigmoid(relative_position_proj_17)
        relative_position_proj_17 = None
        chunk_8 = sigmoid_8.chunk(2, dim=-1)
        sigmoid_8 = None
        gate_a_8 = chunk_8[0]
        gate_b_8 = chunk_8[1]
        chunk_8 = None
        mul_26 = (
            gate_b_8
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_8 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_9 = mul_26 - 1.0
        mul_26 = None
        mul_27 = gate_a_8 * sub_9
        gate_a_8 = sub_9 = None
        gate_output_8 = mul_27 + 2.0
        mul_27 = None
        view_35 = gate_output_8.view(12, -1, 1)
        gate_output_8 = None
        gated_position_bias_16 = view_35 * position_bias
        view_35 = None
        gated_position_bias_17 = gated_position_bias_16.view((-1, 249, 249))
        gated_position_bias_16 = None
        query_8 = hidden_states_105.transpose(0, 1)
        empty_8 = torch.empty([0])
        cat_8 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_8 = torch.nn.functional.multi_head_attention_forward(
            query_8,
            query_8,
            query_8,
            768,
            12,
            empty_8,
            cat_8,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_17,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_8 = (
            empty_8
        ) = (
            cat_8
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_17
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_16 = multi_head_attention_forward_8[0]
        multi_head_attention_forward_8 = None
        attn_output_17 = attn_output_16.transpose(0, 1)
        attn_output_16 = None
        hidden_states_106 = torch.nn.functional.dropout(
            attn_output_17, 0.0, False, False
        )
        attn_output_17 = None
        hidden_states_107 = hidden_states_105 + hidden_states_106
        hidden_states_105 = hidden_states_106 = None
        hidden_states_108 = torch.nn.functional.layer_norm(
            hidden_states_107,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_107 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_109 = torch._C._nn.linear(
            hidden_states_108,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_110 = torch._C._nn.gelu(hidden_states_109)
        hidden_states_109 = None
        hidden_states_111 = torch.nn.functional.dropout(
            hidden_states_110, 0.0, False, False
        )
        hidden_states_110 = None
        hidden_states_112 = torch._C._nn.linear(
            hidden_states_111,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_111 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_113 = torch.nn.functional.dropout(
            hidden_states_112, 0.0, False, False
        )
        hidden_states_112 = None
        hidden_states_114 = hidden_states_108 + hidden_states_113
        hidden_states_108 = hidden_states_113 = None
        hidden_states_115 = torch.nn.functional.layer_norm(
            hidden_states_114,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_114 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_9 = torch.rand([])
        dropout_probability_9 = None
        gated_hidden_states_18 = hidden_states_115.view((1, 249, 12, -1))
        gated_hidden_states_19 = gated_hidden_states_18.permute(0, 2, 1, 3)
        gated_hidden_states_18 = None
        relative_position_proj_18 = torch._C._nn.linear(
            gated_hidden_states_19,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_19 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_38 = relative_position_proj_18.view((1, 12, 249, 2, 4))
        relative_position_proj_18 = None
        relative_position_proj_19 = view_38.sum(-1)
        view_38 = None
        sigmoid_9 = torch.sigmoid(relative_position_proj_19)
        relative_position_proj_19 = None
        chunk_9 = sigmoid_9.chunk(2, dim=-1)
        sigmoid_9 = None
        gate_a_9 = chunk_9[0]
        gate_b_9 = chunk_9[1]
        chunk_9 = None
        mul_29 = (
            gate_b_9
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_9 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_10 = mul_29 - 1.0
        mul_29 = None
        mul_30 = gate_a_9 * sub_10
        gate_a_9 = sub_10 = None
        gate_output_9 = mul_30 + 2.0
        mul_30 = None
        view_39 = gate_output_9.view(12, -1, 1)
        gate_output_9 = None
        gated_position_bias_18 = view_39 * position_bias
        view_39 = None
        gated_position_bias_19 = gated_position_bias_18.view((-1, 249, 249))
        gated_position_bias_18 = None
        query_9 = hidden_states_115.transpose(0, 1)
        empty_9 = torch.empty([0])
        cat_9 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_9 = torch.nn.functional.multi_head_attention_forward(
            query_9,
            query_9,
            query_9,
            768,
            12,
            empty_9,
            cat_9,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_19,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_9 = (
            empty_9
        ) = (
            cat_9
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_19
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_18 = multi_head_attention_forward_9[0]
        multi_head_attention_forward_9 = None
        attn_output_19 = attn_output_18.transpose(0, 1)
        attn_output_18 = None
        hidden_states_116 = torch.nn.functional.dropout(
            attn_output_19, 0.0, False, False
        )
        attn_output_19 = None
        hidden_states_117 = hidden_states_115 + hidden_states_116
        hidden_states_115 = hidden_states_116 = None
        hidden_states_118 = torch.nn.functional.layer_norm(
            hidden_states_117,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_117 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_119 = torch._C._nn.linear(
            hidden_states_118,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_120 = torch._C._nn.gelu(hidden_states_119)
        hidden_states_119 = None
        hidden_states_121 = torch.nn.functional.dropout(
            hidden_states_120, 0.0, False, False
        )
        hidden_states_120 = None
        hidden_states_122 = torch._C._nn.linear(
            hidden_states_121,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_121 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_123 = torch.nn.functional.dropout(
            hidden_states_122, 0.0, False, False
        )
        hidden_states_122 = None
        hidden_states_124 = hidden_states_118 + hidden_states_123
        hidden_states_118 = hidden_states_123 = None
        hidden_states_125 = torch.nn.functional.layer_norm(
            hidden_states_124,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_124 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_10 = torch.rand([])
        dropout_probability_10 = None
        gated_hidden_states_20 = hidden_states_125.view((1, 249, 12, -1))
        gated_hidden_states_21 = gated_hidden_states_20.permute(0, 2, 1, 3)
        gated_hidden_states_20 = None
        relative_position_proj_20 = torch._C._nn.linear(
            gated_hidden_states_21,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_21 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_42 = relative_position_proj_20.view((1, 12, 249, 2, 4))
        relative_position_proj_20 = None
        relative_position_proj_21 = view_42.sum(-1)
        view_42 = None
        sigmoid_10 = torch.sigmoid(relative_position_proj_21)
        relative_position_proj_21 = None
        chunk_10 = sigmoid_10.chunk(2, dim=-1)
        sigmoid_10 = None
        gate_a_10 = chunk_10[0]
        gate_b_10 = chunk_10[1]
        chunk_10 = None
        mul_32 = (
            gate_b_10
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_10 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_11 = mul_32 - 1.0
        mul_32 = None
        mul_33 = gate_a_10 * sub_11
        gate_a_10 = sub_11 = None
        gate_output_10 = mul_33 + 2.0
        mul_33 = None
        view_43 = gate_output_10.view(12, -1, 1)
        gate_output_10 = None
        gated_position_bias_20 = view_43 * position_bias
        view_43 = None
        gated_position_bias_21 = gated_position_bias_20.view((-1, 249, 249))
        gated_position_bias_20 = None
        query_10 = hidden_states_125.transpose(0, 1)
        empty_10 = torch.empty([0])
        cat_10 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_10 = torch.nn.functional.multi_head_attention_forward(
            query_10,
            query_10,
            query_10,
            768,
            12,
            empty_10,
            cat_10,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_21,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_10 = (
            empty_10
        ) = (
            cat_10
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_21
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_20 = multi_head_attention_forward_10[0]
        multi_head_attention_forward_10 = None
        attn_output_21 = attn_output_20.transpose(0, 1)
        attn_output_20 = None
        hidden_states_126 = torch.nn.functional.dropout(
            attn_output_21, 0.0, False, False
        )
        attn_output_21 = None
        hidden_states_127 = hidden_states_125 + hidden_states_126
        hidden_states_125 = hidden_states_126 = None
        hidden_states_128 = torch.nn.functional.layer_norm(
            hidden_states_127,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_127 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_129 = torch._C._nn.linear(
            hidden_states_128,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_130 = torch._C._nn.gelu(hidden_states_129)
        hidden_states_129 = None
        hidden_states_131 = torch.nn.functional.dropout(
            hidden_states_130, 0.0, False, False
        )
        hidden_states_130 = None
        hidden_states_132 = torch._C._nn.linear(
            hidden_states_131,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_131 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_133 = torch.nn.functional.dropout(
            hidden_states_132, 0.0, False, False
        )
        hidden_states_132 = None
        hidden_states_134 = hidden_states_128 + hidden_states_133
        hidden_states_128 = hidden_states_133 = None
        hidden_states_135 = torch.nn.functional.layer_norm(
            hidden_states_134,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_134 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_ = (None)
        dropout_probability_11 = torch.rand([])
        dropout_probability_11 = None
        gated_hidden_states_22 = hidden_states_135.view((1, 249, 12, -1))
        gated_hidden_states_23 = gated_hidden_states_22.permute(0, 2, 1, 3)
        gated_hidden_states_22 = None
        relative_position_proj_22 = torch._C._nn.linear(
            gated_hidden_states_23,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        gated_hidden_states_23 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_46 = relative_position_proj_22.view((1, 12, 249, 2, 4))
        relative_position_proj_22 = None
        relative_position_proj_23 = view_46.sum(-1)
        view_46 = None
        sigmoid_11 = torch.sigmoid(relative_position_proj_23)
        relative_position_proj_23 = None
        chunk_11 = sigmoid_11.chunk(2, dim=-1)
        sigmoid_11 = None
        gate_a_11 = chunk_11[0]
        gate_b_11 = chunk_11[1]
        chunk_11 = None
        mul_35 = (
            gate_b_11
            * l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_11 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_12 = mul_35 - 1.0
        mul_35 = None
        mul_36 = gate_a_11 * sub_12
        gate_a_11 = sub_12 = None
        gate_output_11 = mul_36 + 2.0
        mul_36 = None
        view_47 = gate_output_11.view(12, -1, 1)
        gate_output_11 = None
        gated_position_bias_22 = view_47 * position_bias
        view_47 = position_bias = None
        gated_position_bias_23 = gated_position_bias_22.view((-1, 249, 249))
        gated_position_bias_22 = None
        query_11 = hidden_states_135.transpose(0, 1)
        empty_11 = torch.empty([0])
        cat_11 = torch.cat(
            (
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_bias_,
                l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_bias_,
            )
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_bias_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_bias_ = (None)
        multi_head_attention_forward_11 = torch.nn.functional.multi_head_attention_forward(
            query_11,
            query_11,
            query_11,
            768,
            12,
            empty_11,
            cat_11,
            None,
            None,
            False,
            0.0,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_bias_,
            False,
            None,
            False,
            gated_position_bias_23,
            use_separate_proj_weight=True,
            q_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_weight_,
            k_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_weight_,
            v_proj_weight=l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_weight_,
        )
        query_11 = (
            empty_11
        ) = (
            cat_11
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_out_proj_parameters_bias_ = (
            gated_position_bias_23
        ) = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_q_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_k_proj_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_attention_modules_v_proj_parameters_weight_ = (None)
        attn_output_22 = multi_head_attention_forward_11[0]
        multi_head_attention_forward_11 = None
        attn_output_23 = attn_output_22.transpose(0, 1)
        attn_output_22 = None
        hidden_states_136 = torch.nn.functional.dropout(
            attn_output_23, 0.0, False, False
        )
        attn_output_23 = None
        hidden_states_137 = hidden_states_135 + hidden_states_136
        hidden_states_135 = hidden_states_136 = None
        hidden_states_138 = torch.nn.functional.layer_norm(
            hidden_states_137,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_137 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_layer_norm_parameters_bias_ = (None)
        hidden_states_139 = torch._C._nn.linear(
            hidden_states_138,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        hidden_states_140 = torch._C._nn.gelu(hidden_states_139)
        hidden_states_139 = None
        hidden_states_141 = torch.nn.functional.dropout(
            hidden_states_140, 0.0, False, False
        )
        hidden_states_140 = None
        hidden_states_142 = torch._C._nn.linear(
            hidden_states_141,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        hidden_states_141 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        hidden_states_143 = torch.nn.functional.dropout(
            hidden_states_142, 0.0, False, False
        )
        hidden_states_142 = None
        hidden_states_144 = hidden_states_138 + hidden_states_143
        hidden_states_138 = hidden_states_143 = None
        hidden_states_145 = torch.nn.functional.layer_norm(
            hidden_states_144,
            (768,),
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_,
            l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        hidden_states_144 = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_ = l_self_modules_wavlm_modules_encoder_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_ = (None)
        hidden_states_146 = torch._C._nn.linear(
            hidden_states_145,
            l_self_modules_projector_parameters_weight_,
            l_self_modules_projector_parameters_bias_,
        )
        hidden_states_145 = (
            l_self_modules_projector_parameters_weight_
        ) = l_self_modules_projector_parameters_bias_ = None
        pooled_output = hidden_states_146.mean(dim=1)
        hidden_states_146 = None
        logits = torch._C._nn.linear(
            pooled_output,
            l_self_modules_classifier_parameters_weight_,
            l_self_modules_classifier_parameters_bias_,
        )
        pooled_output = (
            l_self_modules_classifier_parameters_weight_
        ) = l_self_modules_classifier_parameters_bias_ = None
        return (logits,)
