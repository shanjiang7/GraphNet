import torch

from torch import device


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_input_ids_: torch.Tensor,
        L_self_modules_encoder_modules_embed_tokens_parameters_weight_: torch.nn.parameter.Parameter,
        L_attention_mask_: torch.Tensor,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_relative_attention_bias_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_relative_attention_bias_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_decoder_input_ids_: torch.Tensor,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_relative_attention_bias_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_decoder_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
    ):
        l_input_ids_ = L_input_ids_
        l_self_modules_encoder_modules_embed_tokens_parameters_weight_ = (
            L_self_modules_encoder_modules_embed_tokens_parameters_weight_
        )
        l_attention_mask_ = L_attention_mask_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_relative_attention_bias_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_relative_attention_bias_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_relative_attention_bias_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_relative_attention_bias_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_global_input_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_q_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_k_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_v_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_TransientGlobalSelfAttention_modules_o_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_encoder_modules_final_layer_norm_parameters_weight_ = (
            L_self_modules_encoder_modules_final_layer_norm_parameters_weight_
        )
        l_decoder_input_ids_ = L_decoder_input_ids_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_relative_attention_bias_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_relative_attention_bias_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_SelfAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_SelfAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_SelfAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_SelfAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_EncDecAttention_modules_q_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_EncDecAttention_modules_k_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_EncDecAttention_modules_v_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_EncDecAttention_modules_o_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_DenseReluDense_modules_wi_0_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_DenseReluDense_modules_wi_1_parameters_weight_
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = L_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_DenseReluDense_modules_wo_parameters_weight_
        l_self_modules_decoder_modules_final_layer_norm_parameters_weight_ = (
            L_self_modules_decoder_modules_final_layer_norm_parameters_weight_
        )
        input_ids = l_input_ids_.view(-1, 12)
        l_input_ids_ = None
        inputs_embeds = torch.nn.functional.embedding(
            input_ids,
            l_self_modules_encoder_modules_embed_tokens_parameters_weight_,
            None,
            None,
            2.0,
            False,
            False,
        )
        input_ids = None
        cache_position = torch.arange(0, 12, device=device(type="cuda", index=0))
        cache_position = None
        hidden_states = torch.nn.functional.dropout(inputs_embeds, 0.1, False, False)
        inputs_embeds = None
        to = hidden_states.to(torch.float32)
        pow_1 = to.pow(2)
        to = None
        variance = pow_1.mean(-1, keepdim=True)
        pow_1 = None
        add = variance + 1e-06
        variance = None
        rsqrt = torch.rsqrt(add)
        add = None
        hidden_states_1 = hidden_states * rsqrt
        rsqrt = None
        normed_hidden_states = (
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_1
        )
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_1
        ) = None
        ones_like = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask = ones_like / 16
        ones_like = None
        cumsum = torch.cumsum(fixed_block_mask, axis=1)
        fixed_block_mask_1 = cumsum - fixed_block_mask
        cumsum = fixed_block_mask = None
        ne = l_attention_mask_ != 0.0
        where = torch.where(ne, 1.0, -1000.0)
        ne = None
        mask = where.type(torch.int64)
        where = None
        add_1 = mask + fixed_block_mask_1
        mask = fixed_block_mask_1 = None
        sub_1 = add_1 - 1.0
        add_1 = None
        floor = torch.floor(sub_1)
        sub_1 = None
        global_block_ids = floor.type(torch.int64)
        floor = None
        _global_block_ids_lower_bound = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt = global_block_ids > _global_block_ids_lower_bound
        global_block_ids_1 = torch.where(
            gt, global_block_ids, _global_block_ids_lower_bound
        )
        gt = global_block_ids = _global_block_ids_lower_bound = None
        mul_2 = global_block_ids_1 * l_attention_mask_
        global_block_ids_1 = None
        sub_2 = l_attention_mask_ - 1
        global_block_ids_2 = mul_2 + sub_2
        mul_2 = sub_2 = None
        arange_1 = torch.arange(12)
        mod = arange_1 % 16
        arange_1 = None
        block_ends = mod.__eq__(15)
        mod = None
        block_ends_1 = block_ends.to(device(type="cuda", index=0))
        block_ends = None
        ge = global_block_ids_2 >= 0
        true_block_ends = torch.logical_and(block_ends_1, ge)
        block_ends_1 = ge = None
        sum_1 = true_block_ends.sum(-1)
        true_block_ends = None
        unsqueeze = sum_1.unsqueeze(-1)
        sum_1 = None
        type_3 = unsqueeze.type(torch.int64)
        unsqueeze = None
        full_blocks = type_3 - 1
        type_3 = None
        lt = global_block_ids_2 < full_blocks
        block_ids = torch.where(lt, global_block_ids_2, full_blocks)
        lt = global_block_ids_2 = full_blocks = None
        _sequence_block_ids_max = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones = torch.ones(1, 0)
        cumsum_1 = torch.cumsum(ones, dim=-1)
        ones = None
        global_segment_ids = cumsum_1 - 1
        cumsum_1 = None
        global_segment_ids_1 = global_segment_ids.to(device(type="cuda", index=0))
        global_segment_ids = None
        le = global_segment_ids_1 <= _sequence_block_ids_max
        global_segment_ids_1 = _sequence_block_ids_max = None
        global_segment_ids_2 = torch.where(le, 1, 0)
        le = None
        block_ids_1 = block_ids.type(torch.int32)
        block_ids = None
        global_segment_ids_3 = global_segment_ids_2.type(torch.int32)
        global_segment_ids_2 = None
        ge_1 = block_ids_1 >= 0
        tensor_1 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_2 = block_ids_1.where(ge_1, tensor_1)
        block_ids_1 = ge_1 = tensor_1 = None
        type_6 = block_ids_2.type(torch.int64)
        block_ids_2 = None
        one_hot = torch._C._nn.one_hot(type_6, 1)
        type_6 = None
        one_hot_block_ids = one_hot[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot = None
        type_7 = one_hot_block_ids.type(torch.float32)
        one_hot_block_ids = None
        global_inputs = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states, type_7
        )
        type_7 = None
        to_3 = global_inputs.to(torch.float32)
        pow_2 = to_3.pow(2)
        to_3 = None
        variance_1 = pow_2.mean(-1, keepdim=True)
        pow_2 = None
        add_3 = variance_1 + 1e-06
        variance_1 = None
        rsqrt_1 = torch.rsqrt(add_3)
        add_3 = None
        hidden_states_2 = global_inputs * rsqrt_1
        global_inputs = rsqrt_1 = None
        global_inputs_1 = (
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_2
        )
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_2
        ) = None
        linear = torch._C._nn.linear(
            normed_hidden_states,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states = linear.view(1, -1, 12, 64)
        linear = None
        linear_1 = torch._C._nn.linear(
            normed_hidden_states,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states = linear_1.view(1, -1, 12, 64)
        linear_1 = None
        linear_2 = torch._C._nn.linear(
            normed_hidden_states,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states = None
        value_states = linear_2.view(1, -1, 12, 64)
        linear_2 = None
        linear_3 = torch._C._nn.linear(
            global_inputs_1,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states = linear_3.view(1, -1, 12, 64)
        linear_3 = None
        linear_4 = torch._C._nn.linear(
            global_inputs_1,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_1 = l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states = linear_4.view(1, -1, 12, 64)
        linear_4 = None
        x = torch._C._nn.pad(query_states, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0)
        query_states = None
        query_states_1 = x.reshape((1, 1, 128, 12, 64))
        x = None
        x_1 = torch._C._nn.pad(key_states, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0)
        key_states = None
        key_states_1 = x_1.reshape((1, 1, 128, 12, 64))
        x_1 = None
        x_2 = torch._C._nn.pad(value_states, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0)
        value_states = None
        value_states_1 = x_2.reshape((1, 1, 128, 12, 64))
        x_2 = None
        x_3 = torch._C._nn.pad(
            key_states_1, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_1 = None
        getitem_1 = x_3[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_2 = x_3[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_3 = x_3[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_3 = None
        key_states_2 = torch.cat([getitem_1, getitem_2, getitem_3], dim=2)
        getitem_1 = getitem_2 = getitem_3 = None
        x_4 = torch._C._nn.pad(
            value_states_1, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_1 = None
        getitem_4 = x_4[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_5 = x_4[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_6 = x_4[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_4 = None
        value_states_2 = torch.cat([getitem_4, getitem_5, getitem_6], dim=2)
        getitem_4 = getitem_5 = getitem_6 = None
        unsqueeze_1 = side_key_states.unsqueeze(1)
        side_key_states = None
        side_key_states_1 = unsqueeze_1.repeat([1, 1, 1, 1, 1])
        unsqueeze_1 = None
        unsqueeze_2 = side_value_states.unsqueeze(1)
        side_value_states = None
        side_value_states_1 = unsqueeze_2.repeat([1, 1, 1, 1, 1])
        unsqueeze_2 = None
        key_states_3 = torch.cat([key_states_2, side_key_states_1], dim=2)
        key_states_2 = side_key_states_1 = None
        value_states_3 = torch.cat([value_states_2, side_value_states_1], dim=2)
        value_states_2 = side_value_states_1 = None
        scores = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_1, key_states_3
        )
        query_states_1 = key_states_3 = None
        x_5 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask = x_5.reshape((1, 1, 128))
        x_5 = None
        x_6 = torch._C._nn.pad(
            _blocked_attention_mask, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_7 = x_6[(slice(0, None, None), slice(0, 1, None), slice(0, None, None))]
        getitem_8 = x_6[(slice(0, None, None), slice(1, 2, None), slice(0, None, None))]
        getitem_9 = x_6[(slice(0, None, None), slice(2, 3, None), slice(0, None, None))]
        x_6 = None
        _3blocked_attention_mask = torch.cat([getitem_7, getitem_8, getitem_9], dim=2)
        getitem_7 = getitem_8 = getitem_9 = None
        _blocked_attention_mask_1 = _blocked_attention_mask.unsqueeze(-1)
        _blocked_attention_mask = None
        _3blocked_attention_mask_1 = _3blocked_attention_mask.unsqueeze(-2)
        _3blocked_attention_mask = None
        local_attention_mask = torch.logical_and(
            _blocked_attention_mask_1, _3blocked_attention_mask_1
        )
        _blocked_attention_mask_1 = _3blocked_attention_mask_1 = None
        position_ids = torch.arange(384, dtype=torch.int32)
        center_position_ids = position_ids[slice(128, -128, None)]
        unsqueeze_5 = position_ids.unsqueeze(0)
        position_ids = None
        unsqueeze_6 = center_position_ids.unsqueeze(1)
        center_position_ids = None
        relative_position_ids = unsqueeze_5 - unsqueeze_6
        unsqueeze_5 = unsqueeze_6 = None
        abs_1 = torch.abs(relative_position_ids)
        relative_position_ids = None
        locality_mask = abs_1 < 128
        abs_1 = None
        locality_mask_1 = locality_mask[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask = None
        locality_mask_2 = locality_mask_1.to(device(type="cuda", index=0))
        locality_mask_1 = None
        local_attention_mask_1 = torch.logical_and(
            local_attention_mask, locality_mask_2
        )
        local_attention_mask = locality_mask_2 = None
        unsqueeze_7 = local_attention_mask_1.unsqueeze(1)
        local_attention_mask_1 = None
        local_attention_mask_2 = unsqueeze_7.to(device(type="cuda", index=0))
        unsqueeze_7 = None
        gt_1 = local_attention_mask_2 > 0
        local_attention_mask_2 = None
        local_attention_mask_3 = torch.where(gt_1, 0.0, -10000000000.0)
        gt_1 = None
        memory_position = torch.arange(
            384, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        context_position = memory_position[slice(128, -128, None)]
        getitem_13 = memory_position[(None, slice(None, None, None))]
        memory_position = None
        getitem_14 = context_position[(slice(None, None, None), None)]
        context_position = None
        relative_position = getitem_13 - getitem_14
        getitem_13 = getitem_14 = None
        gt_2 = relative_position > 0
        to_6 = gt_2.to(torch.int64)
        gt_2 = None
        mul_5 = to_6 * 16
        to_6 = None
        relative_buckets = 0 + mul_5
        mul_5 = None
        relative_position_1 = torch.abs(relative_position)
        relative_position = None
        is_small = relative_position_1 < 8
        float_1 = relative_position_1.float()
        truediv_1 = float_1 / 8
        float_1 = None
        log = torch.log(truediv_1)
        truediv_1 = None
        truediv_2 = log / 2.772588722239781
        log = None
        mul_6 = truediv_2 * 8
        truediv_2 = None
        to_7 = mul_6.to(torch.int64)
        mul_6 = None
        relative_position_if_large = 8 + to_7
        to_7 = None
        full_like = torch.full_like(relative_position_if_large, 15)
        relative_position_if_large_1 = torch.min(relative_position_if_large, full_like)
        relative_position_if_large = full_like = None
        where_6 = torch.where(
            is_small, relative_position_1, relative_position_if_large_1
        )
        is_small = relative_position_1 = relative_position_if_large_1 = None
        relative_buckets += where_6
        relative_buckets_1 = relative_buckets
        relative_buckets = where_6 = None
        values = torch.nn.functional.embedding(
            relative_buckets_1,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_relative_attention_bias_parameters_weight_,
            None,
            None,
            2.0,
            False,
            False,
        )
        relative_buckets_1 = l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_relative_attention_bias_parameters_weight_ = (None)
        permute = values.permute([2, 0, 1])
        values = None
        unsqueeze_8 = permute.unsqueeze(0)
        permute = None
        values_1 = unsqueeze_8.unsqueeze(0)
        unsqueeze_8 = None
        transpose = local_attention_mask_3.transpose(1, 2)
        local_attention_mask_3 = None
        position_bias = values_1 + transpose
        values_1 = transpose = None
        position_bias_1 = position_bias.type(torch.float32)
        position_bias = None
        getitem_15 = l_attention_mask_[(Ellipsis, None)]
        getitem_16 = global_segment_ids_3[
            (slice(None, None, None), None, slice(None, None, None))
        ]
        global_segment_ids_3 = None
        eq_1 = torch.eq(getitem_15, getitem_16)
        getitem_15 = getitem_16 = None
        side_attention_mask = eq_1[(slice(None, None, None), None, Ellipsis)]
        eq_1 = None
        gt_3 = side_attention_mask > 0
        side_attention_mask = None
        attention_side_bias = torch.where(gt_3, 0.0, -10000000000.0)
        gt_3 = None
        ones_like_1 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_2 = ones_like_1 / 16
        ones_like_1 = None
        cumsum_2 = torch.cumsum(fixed_block_mask_2, axis=1)
        fixed_block_mask_3 = cumsum_2 - fixed_block_mask_2
        cumsum_2 = fixed_block_mask_2 = None
        ne_1 = l_attention_mask_ != 0.0
        where_8 = torch.where(ne_1, 1.0, -1000.0)
        ne_1 = None
        mask_1 = where_8.type(torch.int64)
        where_8 = None
        add_7 = mask_1 + fixed_block_mask_3
        mask_1 = fixed_block_mask_3 = None
        sub_8 = add_7 - 1.0
        add_7 = None
        floor_1 = torch.floor(sub_8)
        sub_8 = None
        global_block_ids_3 = floor_1.type(torch.int64)
        floor_1 = None
        _global_block_ids_lower_bound_1 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_4 = global_block_ids_3 > _global_block_ids_lower_bound_1
        global_block_ids_4 = torch.where(
            gt_4, global_block_ids_3, _global_block_ids_lower_bound_1
        )
        gt_4 = global_block_ids_3 = _global_block_ids_lower_bound_1 = None
        mul_7 = global_block_ids_4 * l_attention_mask_
        global_block_ids_4 = None
        sub_9 = l_attention_mask_ - 1
        global_block_ids_5 = mul_7 + sub_9
        mul_7 = sub_9 = None
        arange_4 = torch.arange(12)
        mod_1 = arange_4 % 16
        arange_4 = None
        block_ends_2 = mod_1.__eq__(15)
        mod_1 = None
        block_ends_3 = block_ends_2.to(device(type="cuda", index=0))
        block_ends_2 = None
        ge_2 = global_block_ids_5 >= 0
        true_block_ends_1 = torch.logical_and(block_ends_3, ge_2)
        block_ends_3 = ge_2 = None
        sum_2 = true_block_ends_1.sum(-1)
        true_block_ends_1 = None
        unsqueeze_10 = sum_2.unsqueeze(-1)
        sum_2 = None
        type_11 = unsqueeze_10.type(torch.int64)
        unsqueeze_10 = None
        full_blocks_1 = type_11 - 1
        type_11 = None
        lt_3 = global_block_ids_5 < full_blocks_1
        block_ids_3 = torch.where(lt_3, global_block_ids_5, full_blocks_1)
        lt_3 = global_block_ids_5 = full_blocks_1 = None
        _sequence_block_ids_max_1 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_1 = torch.ones(1, 0)
        cumsum_3 = torch.cumsum(ones_1, dim=-1)
        ones_1 = None
        global_segment_ids_4 = cumsum_3 - 1
        cumsum_3 = None
        global_segment_ids_5 = global_segment_ids_4.to(device(type="cuda", index=0))
        global_segment_ids_4 = None
        le_1 = global_segment_ids_5 <= _sequence_block_ids_max_1
        global_segment_ids_5 = _sequence_block_ids_max_1 = None
        global_segment_ids_6 = torch.where(le_1, 1, 0)
        le_1 = None
        block_ids_4 = block_ids_3.type(torch.int32)
        block_ids_3 = None
        global_segment_ids_7 = global_segment_ids_6.type(torch.int32)
        global_segment_ids_6 = global_segment_ids_7 = None
        global_positions = torch.arange(0, device=device(type="cuda", index=0))
        getitem_18 = block_ids_4[(Ellipsis, None)]
        block_ids_4 = None
        side_relative_position = global_positions - getitem_18
        global_positions = getitem_18 = None
        side_relative_position_1 = side_relative_position.type(torch.int64)
        side_relative_position = None
        gt_5 = side_relative_position_1 > 0
        to_10 = gt_5.to(torch.int64)
        gt_5 = None
        mul_8 = to_10 * 16
        to_10 = None
        relative_buckets_2 = 0 + mul_8
        mul_8 = None
        relative_position_2 = torch.abs(side_relative_position_1)
        side_relative_position_1 = None
        is_small_1 = relative_position_2 < 8
        float_2 = relative_position_2.float()
        truediv_4 = float_2 / 8
        float_2 = None
        log_1 = torch.log(truediv_4)
        truediv_4 = None
        truediv_5 = log_1 / 2.772588722239781
        log_1 = None
        mul_9 = truediv_5 * 8
        truediv_5 = None
        to_11 = mul_9.to(torch.int64)
        mul_9 = None
        relative_position_if_large_2 = 8 + to_11
        to_11 = None
        full_like_1 = torch.full_like(relative_position_if_large_2, 15)
        relative_position_if_large_3 = torch.min(
            relative_position_if_large_2, full_like_1
        )
        relative_position_if_large_2 = full_like_1 = None
        where_12 = torch.where(
            is_small_1, relative_position_2, relative_position_if_large_3
        )
        is_small_1 = relative_position_2 = relative_position_if_large_3 = None
        relative_buckets_2 += where_12
        relative_buckets_3 = relative_buckets_2
        relative_buckets_2 = where_12 = None
        side_bias = torch.nn.functional.embedding(
            relative_buckets_3,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_relative_attention_bias_parameters_weight_,
            None,
            None,
            2.0,
            False,
            False,
        )
        relative_buckets_3 = l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_relative_attention_bias_parameters_weight_ = (None)
        side_bias_1 = side_bias.permute([0, 3, 1, 2])
        side_bias = None
        attention_side_bias_1 = attention_side_bias + side_bias_1
        attention_side_bias = side_bias_1 = attention_side_bias_1 = None
        x_7 = torch.zeros([1, 12, 128, 0], dtype=torch.float32)
        x_7 = None
        empty = torch.empty(
            (1, 12, 1, 128, 0), dtype=torch.float32, device=device(type="cpu")
        )
        side_position_bias = empty.transpose(1, 2)
        empty = None
        type_15 = side_position_bias.type(torch.float32)
        side_position_bias = None
        side_position_bias_1 = type_15.to(device(type="cuda", index=0))
        type_15 = None
        position_bias_2 = torch.cat([position_bias_1, side_position_bias_1], dim=-1)
        position_bias_1 = side_position_bias_1 = None
        scores += position_bias_2
        scores_1 = scores
        scores = None
        float_3 = scores_1.float()
        softmax = torch.nn.functional.softmax(float_3, dim=-1)
        float_3 = None
        attn_weights = softmax.type_as(scores_1)
        softmax = scores_1 = None
        attn_weights_1 = torch.nn.functional.dropout(
            attn_weights, p=0.1, training=False
        )
        attn_weights = None
        attn_weights_2 = attn_weights_1.type(torch.float32)
        attn_weights_1 = None
        einsum_2 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_2, value_states_3
        )
        attn_weights_2 = value_states_3 = None
        contiguous = einsum_2.contiguous()
        einsum_2 = None
        attn_output = contiguous.view(1, -1, 768)
        contiguous = None
        attn_output_1 = attn_output[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output = None
        attn_output_2 = torch._C._nn.linear(
            attn_output_1,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_1 = l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_2 = torch.nn.functional.dropout(attn_output_2, 0.1, False, False)
        attn_output_2 = None
        hidden_states_3 = hidden_states + dropout_2
        hidden_states = dropout_2 = None
        to_13 = hidden_states_3.to(torch.float32)
        pow_3 = to_13.pow(2)
        to_13 = None
        variance_2 = pow_3.mean(-1, keepdim=True)
        pow_3 = None
        add_13 = variance_2 + 1e-06
        variance_2 = None
        rsqrt_2 = torch.rsqrt(add_13)
        add_13 = None
        hidden_states_4 = hidden_states_3 * rsqrt_2
        rsqrt_2 = None
        forwarded_states = (
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_4
        )
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_4
        ) = None
        linear_6 = torch._C._nn.linear(
            forwarded_states,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_12 = 0.5 * linear_6
        pow_4 = torch.pow(linear_6, 3.0)
        mul_13 = 0.044715 * pow_4
        pow_4 = None
        add_14 = linear_6 + mul_13
        linear_6 = mul_13 = None
        mul_14 = 0.7978845608028654 * add_14
        add_14 = None
        tanh = torch.tanh(mul_14)
        mul_14 = None
        add_15 = 1.0 + tanh
        tanh = None
        hidden_gelu = mul_12 * add_15
        mul_12 = add_15 = None
        hidden_linear = torch._C._nn.linear(
            forwarded_states,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states = l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_5 = hidden_gelu * hidden_linear
        hidden_gelu = hidden_linear = None
        hidden_states_6 = torch.nn.functional.dropout(
            hidden_states_5, 0.1, False, False
        )
        hidden_states_5 = None
        hidden_states_7 = torch._C._nn.linear(
            hidden_states_6,
            l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_6 = l_self_modules_encoder_modules_block_modules_0_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_4 = torch.nn.functional.dropout(hidden_states_7, 0.1, False, False)
        hidden_states_7 = None
        hidden_states_8 = hidden_states_3 + dropout_4
        hidden_states_3 = dropout_4 = None
        to_14 = hidden_states_8.to(torch.float32)
        pow_5 = to_14.pow(2)
        to_14 = None
        variance_3 = pow_5.mean(-1, keepdim=True)
        pow_5 = None
        add_17 = variance_3 + 1e-06
        variance_3 = None
        rsqrt_3 = torch.rsqrt(add_17)
        add_17 = None
        hidden_states_9 = hidden_states_8 * rsqrt_3
        rsqrt_3 = None
        normed_hidden_states_1 = (
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_9
        )
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_9
        ) = None
        ones_like_2 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_4 = ones_like_2 / 16
        ones_like_2 = None
        cumsum_4 = torch.cumsum(fixed_block_mask_4, axis=1)
        fixed_block_mask_5 = cumsum_4 - fixed_block_mask_4
        cumsum_4 = fixed_block_mask_4 = None
        ne_2 = l_attention_mask_ != 0.0
        where_13 = torch.where(ne_2, 1.0, -1000.0)
        ne_2 = None
        mask_2 = where_13.type(torch.int64)
        where_13 = None
        add_18 = mask_2 + fixed_block_mask_5
        mask_2 = fixed_block_mask_5 = None
        sub_14 = add_18 - 1.0
        add_18 = None
        floor_2 = torch.floor(sub_14)
        sub_14 = None
        global_block_ids_6 = floor_2.type(torch.int64)
        floor_2 = None
        _global_block_ids_lower_bound_2 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_6 = global_block_ids_6 > _global_block_ids_lower_bound_2
        global_block_ids_7 = torch.where(
            gt_6, global_block_ids_6, _global_block_ids_lower_bound_2
        )
        gt_6 = global_block_ids_6 = _global_block_ids_lower_bound_2 = None
        mul_19 = global_block_ids_7 * l_attention_mask_
        global_block_ids_7 = None
        sub_15 = l_attention_mask_ - 1
        global_block_ids_8 = mul_19 + sub_15
        mul_19 = sub_15 = None
        arange_6 = torch.arange(12)
        mod_2 = arange_6 % 16
        arange_6 = None
        block_ends_4 = mod_2.__eq__(15)
        mod_2 = None
        block_ends_5 = block_ends_4.to(device(type="cuda", index=0))
        block_ends_4 = None
        ge_3 = global_block_ids_8 >= 0
        true_block_ends_2 = torch.logical_and(block_ends_5, ge_3)
        block_ends_5 = ge_3 = None
        sum_3 = true_block_ends_2.sum(-1)
        true_block_ends_2 = None
        unsqueeze_11 = sum_3.unsqueeze(-1)
        sum_3 = None
        type_19 = unsqueeze_11.type(torch.int64)
        unsqueeze_11 = None
        full_blocks_2 = type_19 - 1
        type_19 = None
        lt_5 = global_block_ids_8 < full_blocks_2
        block_ids_5 = torch.where(lt_5, global_block_ids_8, full_blocks_2)
        lt_5 = global_block_ids_8 = full_blocks_2 = None
        _sequence_block_ids_max_2 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_2 = torch.ones(1, 0)
        cumsum_5 = torch.cumsum(ones_2, dim=-1)
        ones_2 = None
        global_segment_ids_8 = cumsum_5 - 1
        cumsum_5 = None
        global_segment_ids_9 = global_segment_ids_8.to(device(type="cuda", index=0))
        global_segment_ids_8 = None
        le_2 = global_segment_ids_9 <= _sequence_block_ids_max_2
        global_segment_ids_9 = _sequence_block_ids_max_2 = None
        global_segment_ids_10 = torch.where(le_2, 1, 0)
        le_2 = None
        block_ids_6 = block_ids_5.type(torch.int32)
        block_ids_5 = None
        global_segment_ids_11 = global_segment_ids_10.type(torch.int32)
        global_segment_ids_10 = global_segment_ids_11 = None
        ge_4 = block_ids_6 >= 0
        tensor_4 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_7 = block_ids_6.where(ge_4, tensor_4)
        block_ids_6 = ge_4 = tensor_4 = None
        type_22 = block_ids_7.type(torch.int64)
        block_ids_7 = None
        one_hot_1 = torch._C._nn.one_hot(type_22, 1)
        type_22 = None
        one_hot_block_ids_1 = one_hot_1[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_1 = None
        type_23 = one_hot_block_ids_1.type(torch.float32)
        one_hot_block_ids_1 = None
        global_inputs_2 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_1, type_23
        )
        type_23 = None
        to_17 = global_inputs_2.to(torch.float32)
        pow_6 = to_17.pow(2)
        to_17 = None
        variance_4 = pow_6.mean(-1, keepdim=True)
        pow_6 = None
        add_20 = variance_4 + 1e-06
        variance_4 = None
        rsqrt_4 = torch.rsqrt(add_20)
        add_20 = None
        hidden_states_10 = global_inputs_2 * rsqrt_4
        global_inputs_2 = rsqrt_4 = None
        global_inputs_3 = (
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_10
        )
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_10
        ) = None
        linear_9 = torch._C._nn.linear(
            normed_hidden_states_1,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_2 = linear_9.view(1, -1, 12, 64)
        linear_9 = None
        linear_10 = torch._C._nn.linear(
            normed_hidden_states_1,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_4 = linear_10.view(1, -1, 12, 64)
        linear_10 = None
        linear_11 = torch._C._nn.linear(
            normed_hidden_states_1,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_1 = None
        value_states_4 = linear_11.view(1, -1, 12, 64)
        linear_11 = None
        linear_12 = torch._C._nn.linear(
            global_inputs_3,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_2 = linear_12.view(1, -1, 12, 64)
        linear_12 = None
        linear_13 = torch._C._nn.linear(
            global_inputs_3,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_3 = l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_2 = linear_13.view(1, -1, 12, 64)
        linear_13 = None
        x_8 = torch._C._nn.pad(
            query_states_2, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_2 = None
        query_states_3 = x_8.reshape((1, 1, 128, 12, 64))
        x_8 = None
        x_9 = torch._C._nn.pad(key_states_4, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0)
        key_states_4 = None
        key_states_5 = x_9.reshape((1, 1, 128, 12, 64))
        x_9 = None
        x_10 = torch._C._nn.pad(
            value_states_4, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_4 = None
        value_states_5 = x_10.reshape((1, 1, 128, 12, 64))
        x_10 = None
        x_11 = torch._C._nn.pad(
            key_states_5, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_5 = None
        getitem_21 = x_11[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_22 = x_11[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_23 = x_11[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_11 = None
        key_states_6 = torch.cat([getitem_21, getitem_22, getitem_23], dim=2)
        getitem_21 = getitem_22 = getitem_23 = None
        x_12 = torch._C._nn.pad(
            value_states_5, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_5 = None
        getitem_24 = x_12[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_25 = x_12[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_26 = x_12[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_12 = None
        value_states_6 = torch.cat([getitem_24, getitem_25, getitem_26], dim=2)
        getitem_24 = getitem_25 = getitem_26 = None
        unsqueeze_12 = side_key_states_2.unsqueeze(1)
        side_key_states_2 = None
        side_key_states_3 = unsqueeze_12.repeat([1, 1, 1, 1, 1])
        unsqueeze_12 = None
        unsqueeze_13 = side_value_states_2.unsqueeze(1)
        side_value_states_2 = None
        side_value_states_3 = unsqueeze_13.repeat([1, 1, 1, 1, 1])
        unsqueeze_13 = None
        key_states_7 = torch.cat([key_states_6, side_key_states_3], dim=2)
        key_states_6 = side_key_states_3 = None
        value_states_7 = torch.cat([value_states_6, side_value_states_3], dim=2)
        value_states_6 = side_value_states_3 = None
        scores_2 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_3, key_states_7
        )
        query_states_3 = key_states_7 = None
        x_13 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_2 = x_13.reshape((1, 1, 128))
        x_13 = None
        x_14 = torch._C._nn.pad(
            _blocked_attention_mask_2, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_27 = x_14[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_28 = x_14[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_29 = x_14[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_14 = None
        _3blocked_attention_mask_2 = torch.cat(
            [getitem_27, getitem_28, getitem_29], dim=2
        )
        getitem_27 = getitem_28 = getitem_29 = None
        _blocked_attention_mask_3 = _blocked_attention_mask_2.unsqueeze(-1)
        _blocked_attention_mask_2 = None
        _3blocked_attention_mask_3 = _3blocked_attention_mask_2.unsqueeze(-2)
        _3blocked_attention_mask_2 = None
        local_attention_mask_4 = torch.logical_and(
            _blocked_attention_mask_3, _3blocked_attention_mask_3
        )
        _blocked_attention_mask_3 = _3blocked_attention_mask_3 = None
        position_ids_1 = torch.arange(384, dtype=torch.int32)
        center_position_ids_1 = position_ids_1[slice(128, -128, None)]
        unsqueeze_16 = position_ids_1.unsqueeze(0)
        position_ids_1 = None
        unsqueeze_17 = center_position_ids_1.unsqueeze(1)
        center_position_ids_1 = None
        relative_position_ids_1 = unsqueeze_16 - unsqueeze_17
        unsqueeze_16 = unsqueeze_17 = None
        abs_4 = torch.abs(relative_position_ids_1)
        relative_position_ids_1 = None
        locality_mask_3 = abs_4 < 128
        abs_4 = None
        locality_mask_4 = locality_mask_3[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_3 = None
        locality_mask_5 = locality_mask_4.to(device(type="cuda", index=0))
        locality_mask_4 = None
        local_attention_mask_5 = torch.logical_and(
            local_attention_mask_4, locality_mask_5
        )
        local_attention_mask_4 = locality_mask_5 = None
        unsqueeze_18 = local_attention_mask_5.unsqueeze(1)
        local_attention_mask_5 = None
        local_attention_mask_6 = unsqueeze_18.to(device(type="cuda", index=0))
        unsqueeze_18 = None
        gt_7 = local_attention_mask_6 > 0
        local_attention_mask_6 = None
        local_attention_mask_7 = torch.where(gt_7, 0.0, -10000000000.0)
        gt_7 = local_attention_mask_7 = None
        scores_2 += position_bias_2
        scores_3 = scores_2
        scores_2 = None
        float_4 = scores_3.float()
        softmax_1 = torch.nn.functional.softmax(float_4, dim=-1)
        float_4 = None
        attn_weights_3 = softmax_1.type_as(scores_3)
        softmax_1 = scores_3 = None
        attn_weights_4 = torch.nn.functional.dropout(
            attn_weights_3, p=0.1, training=False
        )
        attn_weights_3 = None
        attn_weights_5 = attn_weights_4.type(torch.float32)
        attn_weights_4 = None
        einsum_5 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_5, value_states_7
        )
        attn_weights_5 = value_states_7 = None
        contiguous_1 = einsum_5.contiguous()
        einsum_5 = None
        attn_output_3 = contiguous_1.view(1, -1, 768)
        contiguous_1 = None
        attn_output_4 = attn_output_3[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_3 = None
        attn_output_5 = torch._C._nn.linear(
            attn_output_4,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_4 = l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_6 = torch.nn.functional.dropout(attn_output_5, 0.1, False, False)
        attn_output_5 = None
        hidden_states_11 = hidden_states_8 + dropout_6
        hidden_states_8 = dropout_6 = None
        to_20 = hidden_states_11.to(torch.float32)
        pow_7 = to_20.pow(2)
        to_20 = None
        variance_5 = pow_7.mean(-1, keepdim=True)
        pow_7 = None
        add_22 = variance_5 + 1e-06
        variance_5 = None
        rsqrt_5 = torch.rsqrt(add_22)
        add_22 = None
        hidden_states_12 = hidden_states_11 * rsqrt_5
        rsqrt_5 = None
        forwarded_states_1 = (
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_12
        )
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_12
        ) = None
        linear_15 = torch._C._nn.linear(
            forwarded_states_1,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_24 = 0.5 * linear_15
        pow_8 = torch.pow(linear_15, 3.0)
        mul_25 = 0.044715 * pow_8
        pow_8 = None
        add_23 = linear_15 + mul_25
        linear_15 = mul_25 = None
        mul_26 = 0.7978845608028654 * add_23
        add_23 = None
        tanh_1 = torch.tanh(mul_26)
        mul_26 = None
        add_24 = 1.0 + tanh_1
        tanh_1 = None
        hidden_gelu_1 = mul_24 * add_24
        mul_24 = add_24 = None
        hidden_linear_1 = torch._C._nn.linear(
            forwarded_states_1,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_1 = l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_13 = hidden_gelu_1 * hidden_linear_1
        hidden_gelu_1 = hidden_linear_1 = None
        hidden_states_14 = torch.nn.functional.dropout(
            hidden_states_13, 0.1, False, False
        )
        hidden_states_13 = None
        hidden_states_15 = torch._C._nn.linear(
            hidden_states_14,
            l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_14 = l_self_modules_encoder_modules_block_modules_1_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_8 = torch.nn.functional.dropout(hidden_states_15, 0.1, False, False)
        hidden_states_15 = None
        hidden_states_16 = hidden_states_11 + dropout_8
        hidden_states_11 = dropout_8 = None
        to_21 = hidden_states_16.to(torch.float32)
        pow_9 = to_21.pow(2)
        to_21 = None
        variance_6 = pow_9.mean(-1, keepdim=True)
        pow_9 = None
        add_26 = variance_6 + 1e-06
        variance_6 = None
        rsqrt_6 = torch.rsqrt(add_26)
        add_26 = None
        hidden_states_17 = hidden_states_16 * rsqrt_6
        rsqrt_6 = None
        normed_hidden_states_2 = (
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_17
        )
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_17
        ) = None
        ones_like_3 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_6 = ones_like_3 / 16
        ones_like_3 = None
        cumsum_6 = torch.cumsum(fixed_block_mask_6, axis=1)
        fixed_block_mask_7 = cumsum_6 - fixed_block_mask_6
        cumsum_6 = fixed_block_mask_6 = None
        ne_3 = l_attention_mask_ != 0.0
        where_19 = torch.where(ne_3, 1.0, -1000.0)
        ne_3 = None
        mask_3 = where_19.type(torch.int64)
        where_19 = None
        add_27 = mask_3 + fixed_block_mask_7
        mask_3 = fixed_block_mask_7 = None
        sub_20 = add_27 - 1.0
        add_27 = None
        floor_3 = torch.floor(sub_20)
        sub_20 = None
        global_block_ids_9 = floor_3.type(torch.int64)
        floor_3 = None
        _global_block_ids_lower_bound_3 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_8 = global_block_ids_9 > _global_block_ids_lower_bound_3
        global_block_ids_10 = torch.where(
            gt_8, global_block_ids_9, _global_block_ids_lower_bound_3
        )
        gt_8 = global_block_ids_9 = _global_block_ids_lower_bound_3 = None
        mul_31 = global_block_ids_10 * l_attention_mask_
        global_block_ids_10 = None
        sub_21 = l_attention_mask_ - 1
        global_block_ids_11 = mul_31 + sub_21
        mul_31 = sub_21 = None
        arange_8 = torch.arange(12)
        mod_3 = arange_8 % 16
        arange_8 = None
        block_ends_6 = mod_3.__eq__(15)
        mod_3 = None
        block_ends_7 = block_ends_6.to(device(type="cuda", index=0))
        block_ends_6 = None
        ge_5 = global_block_ids_11 >= 0
        true_block_ends_3 = torch.logical_and(block_ends_7, ge_5)
        block_ends_7 = ge_5 = None
        sum_4 = true_block_ends_3.sum(-1)
        true_block_ends_3 = None
        unsqueeze_19 = sum_4.unsqueeze(-1)
        sum_4 = None
        type_27 = unsqueeze_19.type(torch.int64)
        unsqueeze_19 = None
        full_blocks_3 = type_27 - 1
        type_27 = None
        lt_7 = global_block_ids_11 < full_blocks_3
        block_ids_8 = torch.where(lt_7, global_block_ids_11, full_blocks_3)
        lt_7 = global_block_ids_11 = full_blocks_3 = None
        _sequence_block_ids_max_3 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_3 = torch.ones(1, 0)
        cumsum_7 = torch.cumsum(ones_3, dim=-1)
        ones_3 = None
        global_segment_ids_12 = cumsum_7 - 1
        cumsum_7 = None
        global_segment_ids_13 = global_segment_ids_12.to(device(type="cuda", index=0))
        global_segment_ids_12 = None
        le_3 = global_segment_ids_13 <= _sequence_block_ids_max_3
        global_segment_ids_13 = _sequence_block_ids_max_3 = None
        global_segment_ids_14 = torch.where(le_3, 1, 0)
        le_3 = None
        block_ids_9 = block_ids_8.type(torch.int32)
        block_ids_8 = None
        global_segment_ids_15 = global_segment_ids_14.type(torch.int32)
        global_segment_ids_14 = global_segment_ids_15 = None
        ge_6 = block_ids_9 >= 0
        tensor_6 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_10 = block_ids_9.where(ge_6, tensor_6)
        block_ids_9 = ge_6 = tensor_6 = None
        type_30 = block_ids_10.type(torch.int64)
        block_ids_10 = None
        one_hot_2 = torch._C._nn.one_hot(type_30, 1)
        type_30 = None
        one_hot_block_ids_2 = one_hot_2[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_2 = None
        type_31 = one_hot_block_ids_2.type(torch.float32)
        one_hot_block_ids_2 = None
        global_inputs_4 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_2, type_31
        )
        type_31 = None
        to_24 = global_inputs_4.to(torch.float32)
        pow_10 = to_24.pow(2)
        to_24 = None
        variance_7 = pow_10.mean(-1, keepdim=True)
        pow_10 = None
        add_29 = variance_7 + 1e-06
        variance_7 = None
        rsqrt_7 = torch.rsqrt(add_29)
        add_29 = None
        hidden_states_18 = global_inputs_4 * rsqrt_7
        global_inputs_4 = rsqrt_7 = None
        global_inputs_5 = (
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_18
        )
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_18
        ) = None
        linear_18 = torch._C._nn.linear(
            normed_hidden_states_2,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_4 = linear_18.view(1, -1, 12, 64)
        linear_18 = None
        linear_19 = torch._C._nn.linear(
            normed_hidden_states_2,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_8 = linear_19.view(1, -1, 12, 64)
        linear_19 = None
        linear_20 = torch._C._nn.linear(
            normed_hidden_states_2,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_2 = None
        value_states_8 = linear_20.view(1, -1, 12, 64)
        linear_20 = None
        linear_21 = torch._C._nn.linear(
            global_inputs_5,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_4 = linear_21.view(1, -1, 12, 64)
        linear_21 = None
        linear_22 = torch._C._nn.linear(
            global_inputs_5,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_5 = l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_4 = linear_22.view(1, -1, 12, 64)
        linear_22 = None
        x_15 = torch._C._nn.pad(
            query_states_4, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_4 = None
        query_states_5 = x_15.reshape((1, 1, 128, 12, 64))
        x_15 = None
        x_16 = torch._C._nn.pad(key_states_8, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0)
        key_states_8 = None
        key_states_9 = x_16.reshape((1, 1, 128, 12, 64))
        x_16 = None
        x_17 = torch._C._nn.pad(
            value_states_8, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_8 = None
        value_states_9 = x_17.reshape((1, 1, 128, 12, 64))
        x_17 = None
        x_18 = torch._C._nn.pad(
            key_states_9, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_9 = None
        getitem_34 = x_18[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_35 = x_18[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_36 = x_18[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_18 = None
        key_states_10 = torch.cat([getitem_34, getitem_35, getitem_36], dim=2)
        getitem_34 = getitem_35 = getitem_36 = None
        x_19 = torch._C._nn.pad(
            value_states_9, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_9 = None
        getitem_37 = x_19[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_38 = x_19[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_39 = x_19[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_19 = None
        value_states_10 = torch.cat([getitem_37, getitem_38, getitem_39], dim=2)
        getitem_37 = getitem_38 = getitem_39 = None
        unsqueeze_20 = side_key_states_4.unsqueeze(1)
        side_key_states_4 = None
        side_key_states_5 = unsqueeze_20.repeat([1, 1, 1, 1, 1])
        unsqueeze_20 = None
        unsqueeze_21 = side_value_states_4.unsqueeze(1)
        side_value_states_4 = None
        side_value_states_5 = unsqueeze_21.repeat([1, 1, 1, 1, 1])
        unsqueeze_21 = None
        key_states_11 = torch.cat([key_states_10, side_key_states_5], dim=2)
        key_states_10 = side_key_states_5 = None
        value_states_11 = torch.cat([value_states_10, side_value_states_5], dim=2)
        value_states_10 = side_value_states_5 = None
        scores_4 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_5, key_states_11
        )
        query_states_5 = key_states_11 = None
        x_20 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_4 = x_20.reshape((1, 1, 128))
        x_20 = None
        x_21 = torch._C._nn.pad(
            _blocked_attention_mask_4, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_40 = x_21[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_41 = x_21[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_42 = x_21[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_21 = None
        _3blocked_attention_mask_4 = torch.cat(
            [getitem_40, getitem_41, getitem_42], dim=2
        )
        getitem_40 = getitem_41 = getitem_42 = None
        _blocked_attention_mask_5 = _blocked_attention_mask_4.unsqueeze(-1)
        _blocked_attention_mask_4 = None
        _3blocked_attention_mask_5 = _3blocked_attention_mask_4.unsqueeze(-2)
        _3blocked_attention_mask_4 = None
        local_attention_mask_8 = torch.logical_and(
            _blocked_attention_mask_5, _3blocked_attention_mask_5
        )
        _blocked_attention_mask_5 = _3blocked_attention_mask_5 = None
        position_ids_2 = torch.arange(384, dtype=torch.int32)
        center_position_ids_2 = position_ids_2[slice(128, -128, None)]
        unsqueeze_24 = position_ids_2.unsqueeze(0)
        position_ids_2 = None
        unsqueeze_25 = center_position_ids_2.unsqueeze(1)
        center_position_ids_2 = None
        relative_position_ids_2 = unsqueeze_24 - unsqueeze_25
        unsqueeze_24 = unsqueeze_25 = None
        abs_5 = torch.abs(relative_position_ids_2)
        relative_position_ids_2 = None
        locality_mask_6 = abs_5 < 128
        abs_5 = None
        locality_mask_7 = locality_mask_6[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_6 = None
        locality_mask_8 = locality_mask_7.to(device(type="cuda", index=0))
        locality_mask_7 = None
        local_attention_mask_9 = torch.logical_and(
            local_attention_mask_8, locality_mask_8
        )
        local_attention_mask_8 = locality_mask_8 = None
        unsqueeze_26 = local_attention_mask_9.unsqueeze(1)
        local_attention_mask_9 = None
        local_attention_mask_10 = unsqueeze_26.to(device(type="cuda", index=0))
        unsqueeze_26 = None
        gt_9 = local_attention_mask_10 > 0
        local_attention_mask_10 = None
        local_attention_mask_11 = torch.where(gt_9, 0.0, -10000000000.0)
        gt_9 = local_attention_mask_11 = None
        scores_4 += position_bias_2
        scores_5 = scores_4
        scores_4 = None
        float_5 = scores_5.float()
        softmax_2 = torch.nn.functional.softmax(float_5, dim=-1)
        float_5 = None
        attn_weights_6 = softmax_2.type_as(scores_5)
        softmax_2 = scores_5 = None
        attn_weights_7 = torch.nn.functional.dropout(
            attn_weights_6, p=0.1, training=False
        )
        attn_weights_6 = None
        attn_weights_8 = attn_weights_7.type(torch.float32)
        attn_weights_7 = None
        einsum_8 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_8, value_states_11
        )
        attn_weights_8 = value_states_11 = None
        contiguous_2 = einsum_8.contiguous()
        einsum_8 = None
        attn_output_6 = contiguous_2.view(1, -1, 768)
        contiguous_2 = None
        attn_output_7 = attn_output_6[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_6 = None
        attn_output_8 = torch._C._nn.linear(
            attn_output_7,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_7 = l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_10 = torch.nn.functional.dropout(attn_output_8, 0.1, False, False)
        attn_output_8 = None
        hidden_states_19 = hidden_states_16 + dropout_10
        hidden_states_16 = dropout_10 = None
        to_27 = hidden_states_19.to(torch.float32)
        pow_11 = to_27.pow(2)
        to_27 = None
        variance_8 = pow_11.mean(-1, keepdim=True)
        pow_11 = None
        add_31 = variance_8 + 1e-06
        variance_8 = None
        rsqrt_8 = torch.rsqrt(add_31)
        add_31 = None
        hidden_states_20 = hidden_states_19 * rsqrt_8
        rsqrt_8 = None
        forwarded_states_2 = (
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_20
        )
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_20
        ) = None
        linear_24 = torch._C._nn.linear(
            forwarded_states_2,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_36 = 0.5 * linear_24
        pow_12 = torch.pow(linear_24, 3.0)
        mul_37 = 0.044715 * pow_12
        pow_12 = None
        add_32 = linear_24 + mul_37
        linear_24 = mul_37 = None
        mul_38 = 0.7978845608028654 * add_32
        add_32 = None
        tanh_2 = torch.tanh(mul_38)
        mul_38 = None
        add_33 = 1.0 + tanh_2
        tanh_2 = None
        hidden_gelu_2 = mul_36 * add_33
        mul_36 = add_33 = None
        hidden_linear_2 = torch._C._nn.linear(
            forwarded_states_2,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_2 = l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_21 = hidden_gelu_2 * hidden_linear_2
        hidden_gelu_2 = hidden_linear_2 = None
        hidden_states_22 = torch.nn.functional.dropout(
            hidden_states_21, 0.1, False, False
        )
        hidden_states_21 = None
        hidden_states_23 = torch._C._nn.linear(
            hidden_states_22,
            l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_22 = l_self_modules_encoder_modules_block_modules_2_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_12 = torch.nn.functional.dropout(hidden_states_23, 0.1, False, False)
        hidden_states_23 = None
        hidden_states_24 = hidden_states_19 + dropout_12
        hidden_states_19 = dropout_12 = None
        to_28 = hidden_states_24.to(torch.float32)
        pow_13 = to_28.pow(2)
        to_28 = None
        variance_9 = pow_13.mean(-1, keepdim=True)
        pow_13 = None
        add_35 = variance_9 + 1e-06
        variance_9 = None
        rsqrt_9 = torch.rsqrt(add_35)
        add_35 = None
        hidden_states_25 = hidden_states_24 * rsqrt_9
        rsqrt_9 = None
        normed_hidden_states_3 = (
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_25
        )
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_25
        ) = None
        ones_like_4 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_8 = ones_like_4 / 16
        ones_like_4 = None
        cumsum_8 = torch.cumsum(fixed_block_mask_8, axis=1)
        fixed_block_mask_9 = cumsum_8 - fixed_block_mask_8
        cumsum_8 = fixed_block_mask_8 = None
        ne_4 = l_attention_mask_ != 0.0
        where_25 = torch.where(ne_4, 1.0, -1000.0)
        ne_4 = None
        mask_4 = where_25.type(torch.int64)
        where_25 = None
        add_36 = mask_4 + fixed_block_mask_9
        mask_4 = fixed_block_mask_9 = None
        sub_26 = add_36 - 1.0
        add_36 = None
        floor_4 = torch.floor(sub_26)
        sub_26 = None
        global_block_ids_12 = floor_4.type(torch.int64)
        floor_4 = None
        _global_block_ids_lower_bound_4 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_10 = global_block_ids_12 > _global_block_ids_lower_bound_4
        global_block_ids_13 = torch.where(
            gt_10, global_block_ids_12, _global_block_ids_lower_bound_4
        )
        gt_10 = global_block_ids_12 = _global_block_ids_lower_bound_4 = None
        mul_43 = global_block_ids_13 * l_attention_mask_
        global_block_ids_13 = None
        sub_27 = l_attention_mask_ - 1
        global_block_ids_14 = mul_43 + sub_27
        mul_43 = sub_27 = None
        arange_10 = torch.arange(12)
        mod_4 = arange_10 % 16
        arange_10 = None
        block_ends_8 = mod_4.__eq__(15)
        mod_4 = None
        block_ends_9 = block_ends_8.to(device(type="cuda", index=0))
        block_ends_8 = None
        ge_7 = global_block_ids_14 >= 0
        true_block_ends_4 = torch.logical_and(block_ends_9, ge_7)
        block_ends_9 = ge_7 = None
        sum_5 = true_block_ends_4.sum(-1)
        true_block_ends_4 = None
        unsqueeze_27 = sum_5.unsqueeze(-1)
        sum_5 = None
        type_35 = unsqueeze_27.type(torch.int64)
        unsqueeze_27 = None
        full_blocks_4 = type_35 - 1
        type_35 = None
        lt_9 = global_block_ids_14 < full_blocks_4
        block_ids_11 = torch.where(lt_9, global_block_ids_14, full_blocks_4)
        lt_9 = global_block_ids_14 = full_blocks_4 = None
        _sequence_block_ids_max_4 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_4 = torch.ones(1, 0)
        cumsum_9 = torch.cumsum(ones_4, dim=-1)
        ones_4 = None
        global_segment_ids_16 = cumsum_9 - 1
        cumsum_9 = None
        global_segment_ids_17 = global_segment_ids_16.to(device(type="cuda", index=0))
        global_segment_ids_16 = None
        le_4 = global_segment_ids_17 <= _sequence_block_ids_max_4
        global_segment_ids_17 = _sequence_block_ids_max_4 = None
        global_segment_ids_18 = torch.where(le_4, 1, 0)
        le_4 = None
        block_ids_12 = block_ids_11.type(torch.int32)
        block_ids_11 = None
        global_segment_ids_19 = global_segment_ids_18.type(torch.int32)
        global_segment_ids_18 = global_segment_ids_19 = None
        ge_8 = block_ids_12 >= 0
        tensor_8 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_13 = block_ids_12.where(ge_8, tensor_8)
        block_ids_12 = ge_8 = tensor_8 = None
        type_38 = block_ids_13.type(torch.int64)
        block_ids_13 = None
        one_hot_3 = torch._C._nn.one_hot(type_38, 1)
        type_38 = None
        one_hot_block_ids_3 = one_hot_3[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_3 = None
        type_39 = one_hot_block_ids_3.type(torch.float32)
        one_hot_block_ids_3 = None
        global_inputs_6 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_3, type_39
        )
        type_39 = None
        to_31 = global_inputs_6.to(torch.float32)
        pow_14 = to_31.pow(2)
        to_31 = None
        variance_10 = pow_14.mean(-1, keepdim=True)
        pow_14 = None
        add_38 = variance_10 + 1e-06
        variance_10 = None
        rsqrt_10 = torch.rsqrt(add_38)
        add_38 = None
        hidden_states_26 = global_inputs_6 * rsqrt_10
        global_inputs_6 = rsqrt_10 = None
        global_inputs_7 = (
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_26
        )
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_26
        ) = None
        linear_27 = torch._C._nn.linear(
            normed_hidden_states_3,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_6 = linear_27.view(1, -1, 12, 64)
        linear_27 = None
        linear_28 = torch._C._nn.linear(
            normed_hidden_states_3,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_12 = linear_28.view(1, -1, 12, 64)
        linear_28 = None
        linear_29 = torch._C._nn.linear(
            normed_hidden_states_3,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_3 = None
        value_states_12 = linear_29.view(1, -1, 12, 64)
        linear_29 = None
        linear_30 = torch._C._nn.linear(
            global_inputs_7,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_6 = linear_30.view(1, -1, 12, 64)
        linear_30 = None
        linear_31 = torch._C._nn.linear(
            global_inputs_7,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_7 = l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_6 = linear_31.view(1, -1, 12, 64)
        linear_31 = None
        x_22 = torch._C._nn.pad(
            query_states_6, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_6 = None
        query_states_7 = x_22.reshape((1, 1, 128, 12, 64))
        x_22 = None
        x_23 = torch._C._nn.pad(
            key_states_12, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_12 = None
        key_states_13 = x_23.reshape((1, 1, 128, 12, 64))
        x_23 = None
        x_24 = torch._C._nn.pad(
            value_states_12, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_12 = None
        value_states_13 = x_24.reshape((1, 1, 128, 12, 64))
        x_24 = None
        x_25 = torch._C._nn.pad(
            key_states_13, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_13 = None
        getitem_47 = x_25[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_48 = x_25[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_49 = x_25[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_25 = None
        key_states_14 = torch.cat([getitem_47, getitem_48, getitem_49], dim=2)
        getitem_47 = getitem_48 = getitem_49 = None
        x_26 = torch._C._nn.pad(
            value_states_13, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_13 = None
        getitem_50 = x_26[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_51 = x_26[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_52 = x_26[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_26 = None
        value_states_14 = torch.cat([getitem_50, getitem_51, getitem_52], dim=2)
        getitem_50 = getitem_51 = getitem_52 = None
        unsqueeze_28 = side_key_states_6.unsqueeze(1)
        side_key_states_6 = None
        side_key_states_7 = unsqueeze_28.repeat([1, 1, 1, 1, 1])
        unsqueeze_28 = None
        unsqueeze_29 = side_value_states_6.unsqueeze(1)
        side_value_states_6 = None
        side_value_states_7 = unsqueeze_29.repeat([1, 1, 1, 1, 1])
        unsqueeze_29 = None
        key_states_15 = torch.cat([key_states_14, side_key_states_7], dim=2)
        key_states_14 = side_key_states_7 = None
        value_states_15 = torch.cat([value_states_14, side_value_states_7], dim=2)
        value_states_14 = side_value_states_7 = None
        scores_6 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_7, key_states_15
        )
        query_states_7 = key_states_15 = None
        x_27 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_6 = x_27.reshape((1, 1, 128))
        x_27 = None
        x_28 = torch._C._nn.pad(
            _blocked_attention_mask_6, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_53 = x_28[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_54 = x_28[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_55 = x_28[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_28 = None
        _3blocked_attention_mask_6 = torch.cat(
            [getitem_53, getitem_54, getitem_55], dim=2
        )
        getitem_53 = getitem_54 = getitem_55 = None
        _blocked_attention_mask_7 = _blocked_attention_mask_6.unsqueeze(-1)
        _blocked_attention_mask_6 = None
        _3blocked_attention_mask_7 = _3blocked_attention_mask_6.unsqueeze(-2)
        _3blocked_attention_mask_6 = None
        local_attention_mask_12 = torch.logical_and(
            _blocked_attention_mask_7, _3blocked_attention_mask_7
        )
        _blocked_attention_mask_7 = _3blocked_attention_mask_7 = None
        position_ids_3 = torch.arange(384, dtype=torch.int32)
        center_position_ids_3 = position_ids_3[slice(128, -128, None)]
        unsqueeze_32 = position_ids_3.unsqueeze(0)
        position_ids_3 = None
        unsqueeze_33 = center_position_ids_3.unsqueeze(1)
        center_position_ids_3 = None
        relative_position_ids_3 = unsqueeze_32 - unsqueeze_33
        unsqueeze_32 = unsqueeze_33 = None
        abs_6 = torch.abs(relative_position_ids_3)
        relative_position_ids_3 = None
        locality_mask_9 = abs_6 < 128
        abs_6 = None
        locality_mask_10 = locality_mask_9[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_9 = None
        locality_mask_11 = locality_mask_10.to(device(type="cuda", index=0))
        locality_mask_10 = None
        local_attention_mask_13 = torch.logical_and(
            local_attention_mask_12, locality_mask_11
        )
        local_attention_mask_12 = locality_mask_11 = None
        unsqueeze_34 = local_attention_mask_13.unsqueeze(1)
        local_attention_mask_13 = None
        local_attention_mask_14 = unsqueeze_34.to(device(type="cuda", index=0))
        unsqueeze_34 = None
        gt_11 = local_attention_mask_14 > 0
        local_attention_mask_14 = None
        local_attention_mask_15 = torch.where(gt_11, 0.0, -10000000000.0)
        gt_11 = local_attention_mask_15 = None
        scores_6 += position_bias_2
        scores_7 = scores_6
        scores_6 = None
        float_6 = scores_7.float()
        softmax_3 = torch.nn.functional.softmax(float_6, dim=-1)
        float_6 = None
        attn_weights_9 = softmax_3.type_as(scores_7)
        softmax_3 = scores_7 = None
        attn_weights_10 = torch.nn.functional.dropout(
            attn_weights_9, p=0.1, training=False
        )
        attn_weights_9 = None
        attn_weights_11 = attn_weights_10.type(torch.float32)
        attn_weights_10 = None
        einsum_11 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_11, value_states_15
        )
        attn_weights_11 = value_states_15 = None
        contiguous_3 = einsum_11.contiguous()
        einsum_11 = None
        attn_output_9 = contiguous_3.view(1, -1, 768)
        contiguous_3 = None
        attn_output_10 = attn_output_9[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_9 = None
        attn_output_11 = torch._C._nn.linear(
            attn_output_10,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_10 = l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_14 = torch.nn.functional.dropout(attn_output_11, 0.1, False, False)
        attn_output_11 = None
        hidden_states_27 = hidden_states_24 + dropout_14
        hidden_states_24 = dropout_14 = None
        to_34 = hidden_states_27.to(torch.float32)
        pow_15 = to_34.pow(2)
        to_34 = None
        variance_11 = pow_15.mean(-1, keepdim=True)
        pow_15 = None
        add_40 = variance_11 + 1e-06
        variance_11 = None
        rsqrt_11 = torch.rsqrt(add_40)
        add_40 = None
        hidden_states_28 = hidden_states_27 * rsqrt_11
        rsqrt_11 = None
        forwarded_states_3 = (
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_28
        )
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_28
        ) = None
        linear_33 = torch._C._nn.linear(
            forwarded_states_3,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_48 = 0.5 * linear_33
        pow_16 = torch.pow(linear_33, 3.0)
        mul_49 = 0.044715 * pow_16
        pow_16 = None
        add_41 = linear_33 + mul_49
        linear_33 = mul_49 = None
        mul_50 = 0.7978845608028654 * add_41
        add_41 = None
        tanh_3 = torch.tanh(mul_50)
        mul_50 = None
        add_42 = 1.0 + tanh_3
        tanh_3 = None
        hidden_gelu_3 = mul_48 * add_42
        mul_48 = add_42 = None
        hidden_linear_3 = torch._C._nn.linear(
            forwarded_states_3,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_3 = l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_29 = hidden_gelu_3 * hidden_linear_3
        hidden_gelu_3 = hidden_linear_3 = None
        hidden_states_30 = torch.nn.functional.dropout(
            hidden_states_29, 0.1, False, False
        )
        hidden_states_29 = None
        hidden_states_31 = torch._C._nn.linear(
            hidden_states_30,
            l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_30 = l_self_modules_encoder_modules_block_modules_3_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_16 = torch.nn.functional.dropout(hidden_states_31, 0.1, False, False)
        hidden_states_31 = None
        hidden_states_32 = hidden_states_27 + dropout_16
        hidden_states_27 = dropout_16 = None
        to_35 = hidden_states_32.to(torch.float32)
        pow_17 = to_35.pow(2)
        to_35 = None
        variance_12 = pow_17.mean(-1, keepdim=True)
        pow_17 = None
        add_44 = variance_12 + 1e-06
        variance_12 = None
        rsqrt_12 = torch.rsqrt(add_44)
        add_44 = None
        hidden_states_33 = hidden_states_32 * rsqrt_12
        rsqrt_12 = None
        normed_hidden_states_4 = (
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_33
        )
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_33
        ) = None
        ones_like_5 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_10 = ones_like_5 / 16
        ones_like_5 = None
        cumsum_10 = torch.cumsum(fixed_block_mask_10, axis=1)
        fixed_block_mask_11 = cumsum_10 - fixed_block_mask_10
        cumsum_10 = fixed_block_mask_10 = None
        ne_5 = l_attention_mask_ != 0.0
        where_31 = torch.where(ne_5, 1.0, -1000.0)
        ne_5 = None
        mask_5 = where_31.type(torch.int64)
        where_31 = None
        add_45 = mask_5 + fixed_block_mask_11
        mask_5 = fixed_block_mask_11 = None
        sub_32 = add_45 - 1.0
        add_45 = None
        floor_5 = torch.floor(sub_32)
        sub_32 = None
        global_block_ids_15 = floor_5.type(torch.int64)
        floor_5 = None
        _global_block_ids_lower_bound_5 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_12 = global_block_ids_15 > _global_block_ids_lower_bound_5
        global_block_ids_16 = torch.where(
            gt_12, global_block_ids_15, _global_block_ids_lower_bound_5
        )
        gt_12 = global_block_ids_15 = _global_block_ids_lower_bound_5 = None
        mul_55 = global_block_ids_16 * l_attention_mask_
        global_block_ids_16 = None
        sub_33 = l_attention_mask_ - 1
        global_block_ids_17 = mul_55 + sub_33
        mul_55 = sub_33 = None
        arange_12 = torch.arange(12)
        mod_5 = arange_12 % 16
        arange_12 = None
        block_ends_10 = mod_5.__eq__(15)
        mod_5 = None
        block_ends_11 = block_ends_10.to(device(type="cuda", index=0))
        block_ends_10 = None
        ge_9 = global_block_ids_17 >= 0
        true_block_ends_5 = torch.logical_and(block_ends_11, ge_9)
        block_ends_11 = ge_9 = None
        sum_6 = true_block_ends_5.sum(-1)
        true_block_ends_5 = None
        unsqueeze_35 = sum_6.unsqueeze(-1)
        sum_6 = None
        type_43 = unsqueeze_35.type(torch.int64)
        unsqueeze_35 = None
        full_blocks_5 = type_43 - 1
        type_43 = None
        lt_11 = global_block_ids_17 < full_blocks_5
        block_ids_14 = torch.where(lt_11, global_block_ids_17, full_blocks_5)
        lt_11 = global_block_ids_17 = full_blocks_5 = None
        _sequence_block_ids_max_5 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_5 = torch.ones(1, 0)
        cumsum_11 = torch.cumsum(ones_5, dim=-1)
        ones_5 = None
        global_segment_ids_20 = cumsum_11 - 1
        cumsum_11 = None
        global_segment_ids_21 = global_segment_ids_20.to(device(type="cuda", index=0))
        global_segment_ids_20 = None
        le_5 = global_segment_ids_21 <= _sequence_block_ids_max_5
        global_segment_ids_21 = _sequence_block_ids_max_5 = None
        global_segment_ids_22 = torch.where(le_5, 1, 0)
        le_5 = None
        block_ids_15 = block_ids_14.type(torch.int32)
        block_ids_14 = None
        global_segment_ids_23 = global_segment_ids_22.type(torch.int32)
        global_segment_ids_22 = global_segment_ids_23 = None
        ge_10 = block_ids_15 >= 0
        tensor_10 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_16 = block_ids_15.where(ge_10, tensor_10)
        block_ids_15 = ge_10 = tensor_10 = None
        type_46 = block_ids_16.type(torch.int64)
        block_ids_16 = None
        one_hot_4 = torch._C._nn.one_hot(type_46, 1)
        type_46 = None
        one_hot_block_ids_4 = one_hot_4[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_4 = None
        type_47 = one_hot_block_ids_4.type(torch.float32)
        one_hot_block_ids_4 = None
        global_inputs_8 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_4, type_47
        )
        type_47 = None
        to_38 = global_inputs_8.to(torch.float32)
        pow_18 = to_38.pow(2)
        to_38 = None
        variance_13 = pow_18.mean(-1, keepdim=True)
        pow_18 = None
        add_47 = variance_13 + 1e-06
        variance_13 = None
        rsqrt_13 = torch.rsqrt(add_47)
        add_47 = None
        hidden_states_34 = global_inputs_8 * rsqrt_13
        global_inputs_8 = rsqrt_13 = None
        global_inputs_9 = (
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_34
        )
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_34
        ) = None
        linear_36 = torch._C._nn.linear(
            normed_hidden_states_4,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_8 = linear_36.view(1, -1, 12, 64)
        linear_36 = None
        linear_37 = torch._C._nn.linear(
            normed_hidden_states_4,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_16 = linear_37.view(1, -1, 12, 64)
        linear_37 = None
        linear_38 = torch._C._nn.linear(
            normed_hidden_states_4,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_4 = None
        value_states_16 = linear_38.view(1, -1, 12, 64)
        linear_38 = None
        linear_39 = torch._C._nn.linear(
            global_inputs_9,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_8 = linear_39.view(1, -1, 12, 64)
        linear_39 = None
        linear_40 = torch._C._nn.linear(
            global_inputs_9,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_9 = l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_8 = linear_40.view(1, -1, 12, 64)
        linear_40 = None
        x_29 = torch._C._nn.pad(
            query_states_8, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_8 = None
        query_states_9 = x_29.reshape((1, 1, 128, 12, 64))
        x_29 = None
        x_30 = torch._C._nn.pad(
            key_states_16, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_16 = None
        key_states_17 = x_30.reshape((1, 1, 128, 12, 64))
        x_30 = None
        x_31 = torch._C._nn.pad(
            value_states_16, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_16 = None
        value_states_17 = x_31.reshape((1, 1, 128, 12, 64))
        x_31 = None
        x_32 = torch._C._nn.pad(
            key_states_17, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_17 = None
        getitem_60 = x_32[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_61 = x_32[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_62 = x_32[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_32 = None
        key_states_18 = torch.cat([getitem_60, getitem_61, getitem_62], dim=2)
        getitem_60 = getitem_61 = getitem_62 = None
        x_33 = torch._C._nn.pad(
            value_states_17, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_17 = None
        getitem_63 = x_33[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_64 = x_33[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_65 = x_33[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_33 = None
        value_states_18 = torch.cat([getitem_63, getitem_64, getitem_65], dim=2)
        getitem_63 = getitem_64 = getitem_65 = None
        unsqueeze_36 = side_key_states_8.unsqueeze(1)
        side_key_states_8 = None
        side_key_states_9 = unsqueeze_36.repeat([1, 1, 1, 1, 1])
        unsqueeze_36 = None
        unsqueeze_37 = side_value_states_8.unsqueeze(1)
        side_value_states_8 = None
        side_value_states_9 = unsqueeze_37.repeat([1, 1, 1, 1, 1])
        unsqueeze_37 = None
        key_states_19 = torch.cat([key_states_18, side_key_states_9], dim=2)
        key_states_18 = side_key_states_9 = None
        value_states_19 = torch.cat([value_states_18, side_value_states_9], dim=2)
        value_states_18 = side_value_states_9 = None
        scores_8 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_9, key_states_19
        )
        query_states_9 = key_states_19 = None
        x_34 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_8 = x_34.reshape((1, 1, 128))
        x_34 = None
        x_35 = torch._C._nn.pad(
            _blocked_attention_mask_8, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_66 = x_35[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_67 = x_35[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_68 = x_35[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_35 = None
        _3blocked_attention_mask_8 = torch.cat(
            [getitem_66, getitem_67, getitem_68], dim=2
        )
        getitem_66 = getitem_67 = getitem_68 = None
        _blocked_attention_mask_9 = _blocked_attention_mask_8.unsqueeze(-1)
        _blocked_attention_mask_8 = None
        _3blocked_attention_mask_9 = _3blocked_attention_mask_8.unsqueeze(-2)
        _3blocked_attention_mask_8 = None
        local_attention_mask_16 = torch.logical_and(
            _blocked_attention_mask_9, _3blocked_attention_mask_9
        )
        _blocked_attention_mask_9 = _3blocked_attention_mask_9 = None
        position_ids_4 = torch.arange(384, dtype=torch.int32)
        center_position_ids_4 = position_ids_4[slice(128, -128, None)]
        unsqueeze_40 = position_ids_4.unsqueeze(0)
        position_ids_4 = None
        unsqueeze_41 = center_position_ids_4.unsqueeze(1)
        center_position_ids_4 = None
        relative_position_ids_4 = unsqueeze_40 - unsqueeze_41
        unsqueeze_40 = unsqueeze_41 = None
        abs_7 = torch.abs(relative_position_ids_4)
        relative_position_ids_4 = None
        locality_mask_12 = abs_7 < 128
        abs_7 = None
        locality_mask_13 = locality_mask_12[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_12 = None
        locality_mask_14 = locality_mask_13.to(device(type="cuda", index=0))
        locality_mask_13 = None
        local_attention_mask_17 = torch.logical_and(
            local_attention_mask_16, locality_mask_14
        )
        local_attention_mask_16 = locality_mask_14 = None
        unsqueeze_42 = local_attention_mask_17.unsqueeze(1)
        local_attention_mask_17 = None
        local_attention_mask_18 = unsqueeze_42.to(device(type="cuda", index=0))
        unsqueeze_42 = None
        gt_13 = local_attention_mask_18 > 0
        local_attention_mask_18 = None
        local_attention_mask_19 = torch.where(gt_13, 0.0, -10000000000.0)
        gt_13 = local_attention_mask_19 = None
        scores_8 += position_bias_2
        scores_9 = scores_8
        scores_8 = None
        float_7 = scores_9.float()
        softmax_4 = torch.nn.functional.softmax(float_7, dim=-1)
        float_7 = None
        attn_weights_12 = softmax_4.type_as(scores_9)
        softmax_4 = scores_9 = None
        attn_weights_13 = torch.nn.functional.dropout(
            attn_weights_12, p=0.1, training=False
        )
        attn_weights_12 = None
        attn_weights_14 = attn_weights_13.type(torch.float32)
        attn_weights_13 = None
        einsum_14 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_14, value_states_19
        )
        attn_weights_14 = value_states_19 = None
        contiguous_4 = einsum_14.contiguous()
        einsum_14 = None
        attn_output_12 = contiguous_4.view(1, -1, 768)
        contiguous_4 = None
        attn_output_13 = attn_output_12[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_12 = None
        attn_output_14 = torch._C._nn.linear(
            attn_output_13,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_13 = l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_18 = torch.nn.functional.dropout(attn_output_14, 0.1, False, False)
        attn_output_14 = None
        hidden_states_35 = hidden_states_32 + dropout_18
        hidden_states_32 = dropout_18 = None
        to_41 = hidden_states_35.to(torch.float32)
        pow_19 = to_41.pow(2)
        to_41 = None
        variance_14 = pow_19.mean(-1, keepdim=True)
        pow_19 = None
        add_49 = variance_14 + 1e-06
        variance_14 = None
        rsqrt_14 = torch.rsqrt(add_49)
        add_49 = None
        hidden_states_36 = hidden_states_35 * rsqrt_14
        rsqrt_14 = None
        forwarded_states_4 = (
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_36
        )
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_36
        ) = None
        linear_42 = torch._C._nn.linear(
            forwarded_states_4,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_60 = 0.5 * linear_42
        pow_20 = torch.pow(linear_42, 3.0)
        mul_61 = 0.044715 * pow_20
        pow_20 = None
        add_50 = linear_42 + mul_61
        linear_42 = mul_61 = None
        mul_62 = 0.7978845608028654 * add_50
        add_50 = None
        tanh_4 = torch.tanh(mul_62)
        mul_62 = None
        add_51 = 1.0 + tanh_4
        tanh_4 = None
        hidden_gelu_4 = mul_60 * add_51
        mul_60 = add_51 = None
        hidden_linear_4 = torch._C._nn.linear(
            forwarded_states_4,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_4 = l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_37 = hidden_gelu_4 * hidden_linear_4
        hidden_gelu_4 = hidden_linear_4 = None
        hidden_states_38 = torch.nn.functional.dropout(
            hidden_states_37, 0.1, False, False
        )
        hidden_states_37 = None
        hidden_states_39 = torch._C._nn.linear(
            hidden_states_38,
            l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_38 = l_self_modules_encoder_modules_block_modules_4_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_20 = torch.nn.functional.dropout(hidden_states_39, 0.1, False, False)
        hidden_states_39 = None
        hidden_states_40 = hidden_states_35 + dropout_20
        hidden_states_35 = dropout_20 = None
        to_42 = hidden_states_40.to(torch.float32)
        pow_21 = to_42.pow(2)
        to_42 = None
        variance_15 = pow_21.mean(-1, keepdim=True)
        pow_21 = None
        add_53 = variance_15 + 1e-06
        variance_15 = None
        rsqrt_15 = torch.rsqrt(add_53)
        add_53 = None
        hidden_states_41 = hidden_states_40 * rsqrt_15
        rsqrt_15 = None
        normed_hidden_states_5 = (
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_41
        )
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_41
        ) = None
        ones_like_6 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_12 = ones_like_6 / 16
        ones_like_6 = None
        cumsum_12 = torch.cumsum(fixed_block_mask_12, axis=1)
        fixed_block_mask_13 = cumsum_12 - fixed_block_mask_12
        cumsum_12 = fixed_block_mask_12 = None
        ne_6 = l_attention_mask_ != 0.0
        where_37 = torch.where(ne_6, 1.0, -1000.0)
        ne_6 = None
        mask_6 = where_37.type(torch.int64)
        where_37 = None
        add_54 = mask_6 + fixed_block_mask_13
        mask_6 = fixed_block_mask_13 = None
        sub_38 = add_54 - 1.0
        add_54 = None
        floor_6 = torch.floor(sub_38)
        sub_38 = None
        global_block_ids_18 = floor_6.type(torch.int64)
        floor_6 = None
        _global_block_ids_lower_bound_6 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_14 = global_block_ids_18 > _global_block_ids_lower_bound_6
        global_block_ids_19 = torch.where(
            gt_14, global_block_ids_18, _global_block_ids_lower_bound_6
        )
        gt_14 = global_block_ids_18 = _global_block_ids_lower_bound_6 = None
        mul_67 = global_block_ids_19 * l_attention_mask_
        global_block_ids_19 = None
        sub_39 = l_attention_mask_ - 1
        global_block_ids_20 = mul_67 + sub_39
        mul_67 = sub_39 = None
        arange_14 = torch.arange(12)
        mod_6 = arange_14 % 16
        arange_14 = None
        block_ends_12 = mod_6.__eq__(15)
        mod_6 = None
        block_ends_13 = block_ends_12.to(device(type="cuda", index=0))
        block_ends_12 = None
        ge_11 = global_block_ids_20 >= 0
        true_block_ends_6 = torch.logical_and(block_ends_13, ge_11)
        block_ends_13 = ge_11 = None
        sum_7 = true_block_ends_6.sum(-1)
        true_block_ends_6 = None
        unsqueeze_43 = sum_7.unsqueeze(-1)
        sum_7 = None
        type_51 = unsqueeze_43.type(torch.int64)
        unsqueeze_43 = None
        full_blocks_6 = type_51 - 1
        type_51 = None
        lt_13 = global_block_ids_20 < full_blocks_6
        block_ids_17 = torch.where(lt_13, global_block_ids_20, full_blocks_6)
        lt_13 = global_block_ids_20 = full_blocks_6 = None
        _sequence_block_ids_max_6 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_6 = torch.ones(1, 0)
        cumsum_13 = torch.cumsum(ones_6, dim=-1)
        ones_6 = None
        global_segment_ids_24 = cumsum_13 - 1
        cumsum_13 = None
        global_segment_ids_25 = global_segment_ids_24.to(device(type="cuda", index=0))
        global_segment_ids_24 = None
        le_6 = global_segment_ids_25 <= _sequence_block_ids_max_6
        global_segment_ids_25 = _sequence_block_ids_max_6 = None
        global_segment_ids_26 = torch.where(le_6, 1, 0)
        le_6 = None
        block_ids_18 = block_ids_17.type(torch.int32)
        block_ids_17 = None
        global_segment_ids_27 = global_segment_ids_26.type(torch.int32)
        global_segment_ids_26 = global_segment_ids_27 = None
        ge_12 = block_ids_18 >= 0
        tensor_12 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_19 = block_ids_18.where(ge_12, tensor_12)
        block_ids_18 = ge_12 = tensor_12 = None
        type_54 = block_ids_19.type(torch.int64)
        block_ids_19 = None
        one_hot_5 = torch._C._nn.one_hot(type_54, 1)
        type_54 = None
        one_hot_block_ids_5 = one_hot_5[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_5 = None
        type_55 = one_hot_block_ids_5.type(torch.float32)
        one_hot_block_ids_5 = None
        global_inputs_10 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_5, type_55
        )
        type_55 = None
        to_45 = global_inputs_10.to(torch.float32)
        pow_22 = to_45.pow(2)
        to_45 = None
        variance_16 = pow_22.mean(-1, keepdim=True)
        pow_22 = None
        add_56 = variance_16 + 1e-06
        variance_16 = None
        rsqrt_16 = torch.rsqrt(add_56)
        add_56 = None
        hidden_states_42 = global_inputs_10 * rsqrt_16
        global_inputs_10 = rsqrt_16 = None
        global_inputs_11 = (
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_42
        )
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_42
        ) = None
        linear_45 = torch._C._nn.linear(
            normed_hidden_states_5,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_10 = linear_45.view(1, -1, 12, 64)
        linear_45 = None
        linear_46 = torch._C._nn.linear(
            normed_hidden_states_5,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_20 = linear_46.view(1, -1, 12, 64)
        linear_46 = None
        linear_47 = torch._C._nn.linear(
            normed_hidden_states_5,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_5 = None
        value_states_20 = linear_47.view(1, -1, 12, 64)
        linear_47 = None
        linear_48 = torch._C._nn.linear(
            global_inputs_11,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_10 = linear_48.view(1, -1, 12, 64)
        linear_48 = None
        linear_49 = torch._C._nn.linear(
            global_inputs_11,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_11 = l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_10 = linear_49.view(1, -1, 12, 64)
        linear_49 = None
        x_36 = torch._C._nn.pad(
            query_states_10, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_10 = None
        query_states_11 = x_36.reshape((1, 1, 128, 12, 64))
        x_36 = None
        x_37 = torch._C._nn.pad(
            key_states_20, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_20 = None
        key_states_21 = x_37.reshape((1, 1, 128, 12, 64))
        x_37 = None
        x_38 = torch._C._nn.pad(
            value_states_20, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_20 = None
        value_states_21 = x_38.reshape((1, 1, 128, 12, 64))
        x_38 = None
        x_39 = torch._C._nn.pad(
            key_states_21, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_21 = None
        getitem_73 = x_39[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_74 = x_39[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_75 = x_39[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_39 = None
        key_states_22 = torch.cat([getitem_73, getitem_74, getitem_75], dim=2)
        getitem_73 = getitem_74 = getitem_75 = None
        x_40 = torch._C._nn.pad(
            value_states_21, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_21 = None
        getitem_76 = x_40[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_77 = x_40[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_78 = x_40[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_40 = None
        value_states_22 = torch.cat([getitem_76, getitem_77, getitem_78], dim=2)
        getitem_76 = getitem_77 = getitem_78 = None
        unsqueeze_44 = side_key_states_10.unsqueeze(1)
        side_key_states_10 = None
        side_key_states_11 = unsqueeze_44.repeat([1, 1, 1, 1, 1])
        unsqueeze_44 = None
        unsqueeze_45 = side_value_states_10.unsqueeze(1)
        side_value_states_10 = None
        side_value_states_11 = unsqueeze_45.repeat([1, 1, 1, 1, 1])
        unsqueeze_45 = None
        key_states_23 = torch.cat([key_states_22, side_key_states_11], dim=2)
        key_states_22 = side_key_states_11 = None
        value_states_23 = torch.cat([value_states_22, side_value_states_11], dim=2)
        value_states_22 = side_value_states_11 = None
        scores_10 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_11, key_states_23
        )
        query_states_11 = key_states_23 = None
        x_41 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_10 = x_41.reshape((1, 1, 128))
        x_41 = None
        x_42 = torch._C._nn.pad(
            _blocked_attention_mask_10, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_79 = x_42[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_80 = x_42[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_81 = x_42[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_42 = None
        _3blocked_attention_mask_10 = torch.cat(
            [getitem_79, getitem_80, getitem_81], dim=2
        )
        getitem_79 = getitem_80 = getitem_81 = None
        _blocked_attention_mask_11 = _blocked_attention_mask_10.unsqueeze(-1)
        _blocked_attention_mask_10 = None
        _3blocked_attention_mask_11 = _3blocked_attention_mask_10.unsqueeze(-2)
        _3blocked_attention_mask_10 = None
        local_attention_mask_20 = torch.logical_and(
            _blocked_attention_mask_11, _3blocked_attention_mask_11
        )
        _blocked_attention_mask_11 = _3blocked_attention_mask_11 = None
        position_ids_5 = torch.arange(384, dtype=torch.int32)
        center_position_ids_5 = position_ids_5[slice(128, -128, None)]
        unsqueeze_48 = position_ids_5.unsqueeze(0)
        position_ids_5 = None
        unsqueeze_49 = center_position_ids_5.unsqueeze(1)
        center_position_ids_5 = None
        relative_position_ids_5 = unsqueeze_48 - unsqueeze_49
        unsqueeze_48 = unsqueeze_49 = None
        abs_8 = torch.abs(relative_position_ids_5)
        relative_position_ids_5 = None
        locality_mask_15 = abs_8 < 128
        abs_8 = None
        locality_mask_16 = locality_mask_15[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_15 = None
        locality_mask_17 = locality_mask_16.to(device(type="cuda", index=0))
        locality_mask_16 = None
        local_attention_mask_21 = torch.logical_and(
            local_attention_mask_20, locality_mask_17
        )
        local_attention_mask_20 = locality_mask_17 = None
        unsqueeze_50 = local_attention_mask_21.unsqueeze(1)
        local_attention_mask_21 = None
        local_attention_mask_22 = unsqueeze_50.to(device(type="cuda", index=0))
        unsqueeze_50 = None
        gt_15 = local_attention_mask_22 > 0
        local_attention_mask_22 = None
        local_attention_mask_23 = torch.where(gt_15, 0.0, -10000000000.0)
        gt_15 = local_attention_mask_23 = None
        scores_10 += position_bias_2
        scores_11 = scores_10
        scores_10 = None
        float_8 = scores_11.float()
        softmax_5 = torch.nn.functional.softmax(float_8, dim=-1)
        float_8 = None
        attn_weights_15 = softmax_5.type_as(scores_11)
        softmax_5 = scores_11 = None
        attn_weights_16 = torch.nn.functional.dropout(
            attn_weights_15, p=0.1, training=False
        )
        attn_weights_15 = None
        attn_weights_17 = attn_weights_16.type(torch.float32)
        attn_weights_16 = None
        einsum_17 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_17, value_states_23
        )
        attn_weights_17 = value_states_23 = None
        contiguous_5 = einsum_17.contiguous()
        einsum_17 = None
        attn_output_15 = contiguous_5.view(1, -1, 768)
        contiguous_5 = None
        attn_output_16 = attn_output_15[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_15 = None
        attn_output_17 = torch._C._nn.linear(
            attn_output_16,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_16 = l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_22 = torch.nn.functional.dropout(attn_output_17, 0.1, False, False)
        attn_output_17 = None
        hidden_states_43 = hidden_states_40 + dropout_22
        hidden_states_40 = dropout_22 = None
        to_48 = hidden_states_43.to(torch.float32)
        pow_23 = to_48.pow(2)
        to_48 = None
        variance_17 = pow_23.mean(-1, keepdim=True)
        pow_23 = None
        add_58 = variance_17 + 1e-06
        variance_17 = None
        rsqrt_17 = torch.rsqrt(add_58)
        add_58 = None
        hidden_states_44 = hidden_states_43 * rsqrt_17
        rsqrt_17 = None
        forwarded_states_5 = (
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_44
        )
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_44
        ) = None
        linear_51 = torch._C._nn.linear(
            forwarded_states_5,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_72 = 0.5 * linear_51
        pow_24 = torch.pow(linear_51, 3.0)
        mul_73 = 0.044715 * pow_24
        pow_24 = None
        add_59 = linear_51 + mul_73
        linear_51 = mul_73 = None
        mul_74 = 0.7978845608028654 * add_59
        add_59 = None
        tanh_5 = torch.tanh(mul_74)
        mul_74 = None
        add_60 = 1.0 + tanh_5
        tanh_5 = None
        hidden_gelu_5 = mul_72 * add_60
        mul_72 = add_60 = None
        hidden_linear_5 = torch._C._nn.linear(
            forwarded_states_5,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_5 = l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_45 = hidden_gelu_5 * hidden_linear_5
        hidden_gelu_5 = hidden_linear_5 = None
        hidden_states_46 = torch.nn.functional.dropout(
            hidden_states_45, 0.1, False, False
        )
        hidden_states_45 = None
        hidden_states_47 = torch._C._nn.linear(
            hidden_states_46,
            l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_46 = l_self_modules_encoder_modules_block_modules_5_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_24 = torch.nn.functional.dropout(hidden_states_47, 0.1, False, False)
        hidden_states_47 = None
        hidden_states_48 = hidden_states_43 + dropout_24
        hidden_states_43 = dropout_24 = None
        to_49 = hidden_states_48.to(torch.float32)
        pow_25 = to_49.pow(2)
        to_49 = None
        variance_18 = pow_25.mean(-1, keepdim=True)
        pow_25 = None
        add_62 = variance_18 + 1e-06
        variance_18 = None
        rsqrt_18 = torch.rsqrt(add_62)
        add_62 = None
        hidden_states_49 = hidden_states_48 * rsqrt_18
        rsqrt_18 = None
        normed_hidden_states_6 = (
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_49
        )
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_49
        ) = None
        ones_like_7 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_14 = ones_like_7 / 16
        ones_like_7 = None
        cumsum_14 = torch.cumsum(fixed_block_mask_14, axis=1)
        fixed_block_mask_15 = cumsum_14 - fixed_block_mask_14
        cumsum_14 = fixed_block_mask_14 = None
        ne_7 = l_attention_mask_ != 0.0
        where_43 = torch.where(ne_7, 1.0, -1000.0)
        ne_7 = None
        mask_7 = where_43.type(torch.int64)
        where_43 = None
        add_63 = mask_7 + fixed_block_mask_15
        mask_7 = fixed_block_mask_15 = None
        sub_44 = add_63 - 1.0
        add_63 = None
        floor_7 = torch.floor(sub_44)
        sub_44 = None
        global_block_ids_21 = floor_7.type(torch.int64)
        floor_7 = None
        _global_block_ids_lower_bound_7 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_16 = global_block_ids_21 > _global_block_ids_lower_bound_7
        global_block_ids_22 = torch.where(
            gt_16, global_block_ids_21, _global_block_ids_lower_bound_7
        )
        gt_16 = global_block_ids_21 = _global_block_ids_lower_bound_7 = None
        mul_79 = global_block_ids_22 * l_attention_mask_
        global_block_ids_22 = None
        sub_45 = l_attention_mask_ - 1
        global_block_ids_23 = mul_79 + sub_45
        mul_79 = sub_45 = None
        arange_16 = torch.arange(12)
        mod_7 = arange_16 % 16
        arange_16 = None
        block_ends_14 = mod_7.__eq__(15)
        mod_7 = None
        block_ends_15 = block_ends_14.to(device(type="cuda", index=0))
        block_ends_14 = None
        ge_13 = global_block_ids_23 >= 0
        true_block_ends_7 = torch.logical_and(block_ends_15, ge_13)
        block_ends_15 = ge_13 = None
        sum_8 = true_block_ends_7.sum(-1)
        true_block_ends_7 = None
        unsqueeze_51 = sum_8.unsqueeze(-1)
        sum_8 = None
        type_59 = unsqueeze_51.type(torch.int64)
        unsqueeze_51 = None
        full_blocks_7 = type_59 - 1
        type_59 = None
        lt_15 = global_block_ids_23 < full_blocks_7
        block_ids_20 = torch.where(lt_15, global_block_ids_23, full_blocks_7)
        lt_15 = global_block_ids_23 = full_blocks_7 = None
        _sequence_block_ids_max_7 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_7 = torch.ones(1, 0)
        cumsum_15 = torch.cumsum(ones_7, dim=-1)
        ones_7 = None
        global_segment_ids_28 = cumsum_15 - 1
        cumsum_15 = None
        global_segment_ids_29 = global_segment_ids_28.to(device(type="cuda", index=0))
        global_segment_ids_28 = None
        le_7 = global_segment_ids_29 <= _sequence_block_ids_max_7
        global_segment_ids_29 = _sequence_block_ids_max_7 = None
        global_segment_ids_30 = torch.where(le_7, 1, 0)
        le_7 = None
        block_ids_21 = block_ids_20.type(torch.int32)
        block_ids_20 = None
        global_segment_ids_31 = global_segment_ids_30.type(torch.int32)
        global_segment_ids_30 = global_segment_ids_31 = None
        ge_14 = block_ids_21 >= 0
        tensor_14 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_22 = block_ids_21.where(ge_14, tensor_14)
        block_ids_21 = ge_14 = tensor_14 = None
        type_62 = block_ids_22.type(torch.int64)
        block_ids_22 = None
        one_hot_6 = torch._C._nn.one_hot(type_62, 1)
        type_62 = None
        one_hot_block_ids_6 = one_hot_6[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_6 = None
        type_63 = one_hot_block_ids_6.type(torch.float32)
        one_hot_block_ids_6 = None
        global_inputs_12 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_6, type_63
        )
        type_63 = None
        to_52 = global_inputs_12.to(torch.float32)
        pow_26 = to_52.pow(2)
        to_52 = None
        variance_19 = pow_26.mean(-1, keepdim=True)
        pow_26 = None
        add_65 = variance_19 + 1e-06
        variance_19 = None
        rsqrt_19 = torch.rsqrt(add_65)
        add_65 = None
        hidden_states_50 = global_inputs_12 * rsqrt_19
        global_inputs_12 = rsqrt_19 = None
        global_inputs_13 = (
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_50
        )
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_50
        ) = None
        linear_54 = torch._C._nn.linear(
            normed_hidden_states_6,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_12 = linear_54.view(1, -1, 12, 64)
        linear_54 = None
        linear_55 = torch._C._nn.linear(
            normed_hidden_states_6,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_24 = linear_55.view(1, -1, 12, 64)
        linear_55 = None
        linear_56 = torch._C._nn.linear(
            normed_hidden_states_6,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_6 = None
        value_states_24 = linear_56.view(1, -1, 12, 64)
        linear_56 = None
        linear_57 = torch._C._nn.linear(
            global_inputs_13,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_12 = linear_57.view(1, -1, 12, 64)
        linear_57 = None
        linear_58 = torch._C._nn.linear(
            global_inputs_13,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_13 = l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_12 = linear_58.view(1, -1, 12, 64)
        linear_58 = None
        x_43 = torch._C._nn.pad(
            query_states_12, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_12 = None
        query_states_13 = x_43.reshape((1, 1, 128, 12, 64))
        x_43 = None
        x_44 = torch._C._nn.pad(
            key_states_24, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_24 = None
        key_states_25 = x_44.reshape((1, 1, 128, 12, 64))
        x_44 = None
        x_45 = torch._C._nn.pad(
            value_states_24, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_24 = None
        value_states_25 = x_45.reshape((1, 1, 128, 12, 64))
        x_45 = None
        x_46 = torch._C._nn.pad(
            key_states_25, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_25 = None
        getitem_86 = x_46[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_87 = x_46[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_88 = x_46[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_46 = None
        key_states_26 = torch.cat([getitem_86, getitem_87, getitem_88], dim=2)
        getitem_86 = getitem_87 = getitem_88 = None
        x_47 = torch._C._nn.pad(
            value_states_25, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_25 = None
        getitem_89 = x_47[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_90 = x_47[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_91 = x_47[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_47 = None
        value_states_26 = torch.cat([getitem_89, getitem_90, getitem_91], dim=2)
        getitem_89 = getitem_90 = getitem_91 = None
        unsqueeze_52 = side_key_states_12.unsqueeze(1)
        side_key_states_12 = None
        side_key_states_13 = unsqueeze_52.repeat([1, 1, 1, 1, 1])
        unsqueeze_52 = None
        unsqueeze_53 = side_value_states_12.unsqueeze(1)
        side_value_states_12 = None
        side_value_states_13 = unsqueeze_53.repeat([1, 1, 1, 1, 1])
        unsqueeze_53 = None
        key_states_27 = torch.cat([key_states_26, side_key_states_13], dim=2)
        key_states_26 = side_key_states_13 = None
        value_states_27 = torch.cat([value_states_26, side_value_states_13], dim=2)
        value_states_26 = side_value_states_13 = None
        scores_12 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_13, key_states_27
        )
        query_states_13 = key_states_27 = None
        x_48 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_12 = x_48.reshape((1, 1, 128))
        x_48 = None
        x_49 = torch._C._nn.pad(
            _blocked_attention_mask_12, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_92 = x_49[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_93 = x_49[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_94 = x_49[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_49 = None
        _3blocked_attention_mask_12 = torch.cat(
            [getitem_92, getitem_93, getitem_94], dim=2
        )
        getitem_92 = getitem_93 = getitem_94 = None
        _blocked_attention_mask_13 = _blocked_attention_mask_12.unsqueeze(-1)
        _blocked_attention_mask_12 = None
        _3blocked_attention_mask_13 = _3blocked_attention_mask_12.unsqueeze(-2)
        _3blocked_attention_mask_12 = None
        local_attention_mask_24 = torch.logical_and(
            _blocked_attention_mask_13, _3blocked_attention_mask_13
        )
        _blocked_attention_mask_13 = _3blocked_attention_mask_13 = None
        position_ids_6 = torch.arange(384, dtype=torch.int32)
        center_position_ids_6 = position_ids_6[slice(128, -128, None)]
        unsqueeze_56 = position_ids_6.unsqueeze(0)
        position_ids_6 = None
        unsqueeze_57 = center_position_ids_6.unsqueeze(1)
        center_position_ids_6 = None
        relative_position_ids_6 = unsqueeze_56 - unsqueeze_57
        unsqueeze_56 = unsqueeze_57 = None
        abs_9 = torch.abs(relative_position_ids_6)
        relative_position_ids_6 = None
        locality_mask_18 = abs_9 < 128
        abs_9 = None
        locality_mask_19 = locality_mask_18[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_18 = None
        locality_mask_20 = locality_mask_19.to(device(type="cuda", index=0))
        locality_mask_19 = None
        local_attention_mask_25 = torch.logical_and(
            local_attention_mask_24, locality_mask_20
        )
        local_attention_mask_24 = locality_mask_20 = None
        unsqueeze_58 = local_attention_mask_25.unsqueeze(1)
        local_attention_mask_25 = None
        local_attention_mask_26 = unsqueeze_58.to(device(type="cuda", index=0))
        unsqueeze_58 = None
        gt_17 = local_attention_mask_26 > 0
        local_attention_mask_26 = None
        local_attention_mask_27 = torch.where(gt_17, 0.0, -10000000000.0)
        gt_17 = local_attention_mask_27 = None
        scores_12 += position_bias_2
        scores_13 = scores_12
        scores_12 = None
        float_9 = scores_13.float()
        softmax_6 = torch.nn.functional.softmax(float_9, dim=-1)
        float_9 = None
        attn_weights_18 = softmax_6.type_as(scores_13)
        softmax_6 = scores_13 = None
        attn_weights_19 = torch.nn.functional.dropout(
            attn_weights_18, p=0.1, training=False
        )
        attn_weights_18 = None
        attn_weights_20 = attn_weights_19.type(torch.float32)
        attn_weights_19 = None
        einsum_20 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_20, value_states_27
        )
        attn_weights_20 = value_states_27 = None
        contiguous_6 = einsum_20.contiguous()
        einsum_20 = None
        attn_output_18 = contiguous_6.view(1, -1, 768)
        contiguous_6 = None
        attn_output_19 = attn_output_18[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_18 = None
        attn_output_20 = torch._C._nn.linear(
            attn_output_19,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_19 = l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_26 = torch.nn.functional.dropout(attn_output_20, 0.1, False, False)
        attn_output_20 = None
        hidden_states_51 = hidden_states_48 + dropout_26
        hidden_states_48 = dropout_26 = None
        to_55 = hidden_states_51.to(torch.float32)
        pow_27 = to_55.pow(2)
        to_55 = None
        variance_20 = pow_27.mean(-1, keepdim=True)
        pow_27 = None
        add_67 = variance_20 + 1e-06
        variance_20 = None
        rsqrt_20 = torch.rsqrt(add_67)
        add_67 = None
        hidden_states_52 = hidden_states_51 * rsqrt_20
        rsqrt_20 = None
        forwarded_states_6 = (
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_52
        )
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_52
        ) = None
        linear_60 = torch._C._nn.linear(
            forwarded_states_6,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_84 = 0.5 * linear_60
        pow_28 = torch.pow(linear_60, 3.0)
        mul_85 = 0.044715 * pow_28
        pow_28 = None
        add_68 = linear_60 + mul_85
        linear_60 = mul_85 = None
        mul_86 = 0.7978845608028654 * add_68
        add_68 = None
        tanh_6 = torch.tanh(mul_86)
        mul_86 = None
        add_69 = 1.0 + tanh_6
        tanh_6 = None
        hidden_gelu_6 = mul_84 * add_69
        mul_84 = add_69 = None
        hidden_linear_6 = torch._C._nn.linear(
            forwarded_states_6,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_6 = l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_53 = hidden_gelu_6 * hidden_linear_6
        hidden_gelu_6 = hidden_linear_6 = None
        hidden_states_54 = torch.nn.functional.dropout(
            hidden_states_53, 0.1, False, False
        )
        hidden_states_53 = None
        hidden_states_55 = torch._C._nn.linear(
            hidden_states_54,
            l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_54 = l_self_modules_encoder_modules_block_modules_6_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_28 = torch.nn.functional.dropout(hidden_states_55, 0.1, False, False)
        hidden_states_55 = None
        hidden_states_56 = hidden_states_51 + dropout_28
        hidden_states_51 = dropout_28 = None
        to_56 = hidden_states_56.to(torch.float32)
        pow_29 = to_56.pow(2)
        to_56 = None
        variance_21 = pow_29.mean(-1, keepdim=True)
        pow_29 = None
        add_71 = variance_21 + 1e-06
        variance_21 = None
        rsqrt_21 = torch.rsqrt(add_71)
        add_71 = None
        hidden_states_57 = hidden_states_56 * rsqrt_21
        rsqrt_21 = None
        normed_hidden_states_7 = (
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_57
        )
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_57
        ) = None
        ones_like_8 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_16 = ones_like_8 / 16
        ones_like_8 = None
        cumsum_16 = torch.cumsum(fixed_block_mask_16, axis=1)
        fixed_block_mask_17 = cumsum_16 - fixed_block_mask_16
        cumsum_16 = fixed_block_mask_16 = None
        ne_8 = l_attention_mask_ != 0.0
        where_49 = torch.where(ne_8, 1.0, -1000.0)
        ne_8 = None
        mask_8 = where_49.type(torch.int64)
        where_49 = None
        add_72 = mask_8 + fixed_block_mask_17
        mask_8 = fixed_block_mask_17 = None
        sub_50 = add_72 - 1.0
        add_72 = None
        floor_8 = torch.floor(sub_50)
        sub_50 = None
        global_block_ids_24 = floor_8.type(torch.int64)
        floor_8 = None
        _global_block_ids_lower_bound_8 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_18 = global_block_ids_24 > _global_block_ids_lower_bound_8
        global_block_ids_25 = torch.where(
            gt_18, global_block_ids_24, _global_block_ids_lower_bound_8
        )
        gt_18 = global_block_ids_24 = _global_block_ids_lower_bound_8 = None
        mul_91 = global_block_ids_25 * l_attention_mask_
        global_block_ids_25 = None
        sub_51 = l_attention_mask_ - 1
        global_block_ids_26 = mul_91 + sub_51
        mul_91 = sub_51 = None
        arange_18 = torch.arange(12)
        mod_8 = arange_18 % 16
        arange_18 = None
        block_ends_16 = mod_8.__eq__(15)
        mod_8 = None
        block_ends_17 = block_ends_16.to(device(type="cuda", index=0))
        block_ends_16 = None
        ge_15 = global_block_ids_26 >= 0
        true_block_ends_8 = torch.logical_and(block_ends_17, ge_15)
        block_ends_17 = ge_15 = None
        sum_9 = true_block_ends_8.sum(-1)
        true_block_ends_8 = None
        unsqueeze_59 = sum_9.unsqueeze(-1)
        sum_9 = None
        type_67 = unsqueeze_59.type(torch.int64)
        unsqueeze_59 = None
        full_blocks_8 = type_67 - 1
        type_67 = None
        lt_17 = global_block_ids_26 < full_blocks_8
        block_ids_23 = torch.where(lt_17, global_block_ids_26, full_blocks_8)
        lt_17 = global_block_ids_26 = full_blocks_8 = None
        _sequence_block_ids_max_8 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_8 = torch.ones(1, 0)
        cumsum_17 = torch.cumsum(ones_8, dim=-1)
        ones_8 = None
        global_segment_ids_32 = cumsum_17 - 1
        cumsum_17 = None
        global_segment_ids_33 = global_segment_ids_32.to(device(type="cuda", index=0))
        global_segment_ids_32 = None
        le_8 = global_segment_ids_33 <= _sequence_block_ids_max_8
        global_segment_ids_33 = _sequence_block_ids_max_8 = None
        global_segment_ids_34 = torch.where(le_8, 1, 0)
        le_8 = None
        block_ids_24 = block_ids_23.type(torch.int32)
        block_ids_23 = None
        global_segment_ids_35 = global_segment_ids_34.type(torch.int32)
        global_segment_ids_34 = global_segment_ids_35 = None
        ge_16 = block_ids_24 >= 0
        tensor_16 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_25 = block_ids_24.where(ge_16, tensor_16)
        block_ids_24 = ge_16 = tensor_16 = None
        type_70 = block_ids_25.type(torch.int64)
        block_ids_25 = None
        one_hot_7 = torch._C._nn.one_hot(type_70, 1)
        type_70 = None
        one_hot_block_ids_7 = one_hot_7[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_7 = None
        type_71 = one_hot_block_ids_7.type(torch.float32)
        one_hot_block_ids_7 = None
        global_inputs_14 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_7, type_71
        )
        type_71 = None
        to_59 = global_inputs_14.to(torch.float32)
        pow_30 = to_59.pow(2)
        to_59 = None
        variance_22 = pow_30.mean(-1, keepdim=True)
        pow_30 = None
        add_74 = variance_22 + 1e-06
        variance_22 = None
        rsqrt_22 = torch.rsqrt(add_74)
        add_74 = None
        hidden_states_58 = global_inputs_14 * rsqrt_22
        global_inputs_14 = rsqrt_22 = None
        global_inputs_15 = (
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_58
        )
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_58
        ) = None
        linear_63 = torch._C._nn.linear(
            normed_hidden_states_7,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_14 = linear_63.view(1, -1, 12, 64)
        linear_63 = None
        linear_64 = torch._C._nn.linear(
            normed_hidden_states_7,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_28 = linear_64.view(1, -1, 12, 64)
        linear_64 = None
        linear_65 = torch._C._nn.linear(
            normed_hidden_states_7,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_7 = None
        value_states_28 = linear_65.view(1, -1, 12, 64)
        linear_65 = None
        linear_66 = torch._C._nn.linear(
            global_inputs_15,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_14 = linear_66.view(1, -1, 12, 64)
        linear_66 = None
        linear_67 = torch._C._nn.linear(
            global_inputs_15,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_15 = l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_14 = linear_67.view(1, -1, 12, 64)
        linear_67 = None
        x_50 = torch._C._nn.pad(
            query_states_14, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_14 = None
        query_states_15 = x_50.reshape((1, 1, 128, 12, 64))
        x_50 = None
        x_51 = torch._C._nn.pad(
            key_states_28, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_28 = None
        key_states_29 = x_51.reshape((1, 1, 128, 12, 64))
        x_51 = None
        x_52 = torch._C._nn.pad(
            value_states_28, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_28 = None
        value_states_29 = x_52.reshape((1, 1, 128, 12, 64))
        x_52 = None
        x_53 = torch._C._nn.pad(
            key_states_29, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_29 = None
        getitem_99 = x_53[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_100 = x_53[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_101 = x_53[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_53 = None
        key_states_30 = torch.cat([getitem_99, getitem_100, getitem_101], dim=2)
        getitem_99 = getitem_100 = getitem_101 = None
        x_54 = torch._C._nn.pad(
            value_states_29, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_29 = None
        getitem_102 = x_54[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_103 = x_54[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_104 = x_54[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_54 = None
        value_states_30 = torch.cat([getitem_102, getitem_103, getitem_104], dim=2)
        getitem_102 = getitem_103 = getitem_104 = None
        unsqueeze_60 = side_key_states_14.unsqueeze(1)
        side_key_states_14 = None
        side_key_states_15 = unsqueeze_60.repeat([1, 1, 1, 1, 1])
        unsqueeze_60 = None
        unsqueeze_61 = side_value_states_14.unsqueeze(1)
        side_value_states_14 = None
        side_value_states_15 = unsqueeze_61.repeat([1, 1, 1, 1, 1])
        unsqueeze_61 = None
        key_states_31 = torch.cat([key_states_30, side_key_states_15], dim=2)
        key_states_30 = side_key_states_15 = None
        value_states_31 = torch.cat([value_states_30, side_value_states_15], dim=2)
        value_states_30 = side_value_states_15 = None
        scores_14 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_15, key_states_31
        )
        query_states_15 = key_states_31 = None
        x_55 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_14 = x_55.reshape((1, 1, 128))
        x_55 = None
        x_56 = torch._C._nn.pad(
            _blocked_attention_mask_14, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_105 = x_56[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_106 = x_56[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_107 = x_56[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_56 = None
        _3blocked_attention_mask_14 = torch.cat(
            [getitem_105, getitem_106, getitem_107], dim=2
        )
        getitem_105 = getitem_106 = getitem_107 = None
        _blocked_attention_mask_15 = _blocked_attention_mask_14.unsqueeze(-1)
        _blocked_attention_mask_14 = None
        _3blocked_attention_mask_15 = _3blocked_attention_mask_14.unsqueeze(-2)
        _3blocked_attention_mask_14 = None
        local_attention_mask_28 = torch.logical_and(
            _blocked_attention_mask_15, _3blocked_attention_mask_15
        )
        _blocked_attention_mask_15 = _3blocked_attention_mask_15 = None
        position_ids_7 = torch.arange(384, dtype=torch.int32)
        center_position_ids_7 = position_ids_7[slice(128, -128, None)]
        unsqueeze_64 = position_ids_7.unsqueeze(0)
        position_ids_7 = None
        unsqueeze_65 = center_position_ids_7.unsqueeze(1)
        center_position_ids_7 = None
        relative_position_ids_7 = unsqueeze_64 - unsqueeze_65
        unsqueeze_64 = unsqueeze_65 = None
        abs_10 = torch.abs(relative_position_ids_7)
        relative_position_ids_7 = None
        locality_mask_21 = abs_10 < 128
        abs_10 = None
        locality_mask_22 = locality_mask_21[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_21 = None
        locality_mask_23 = locality_mask_22.to(device(type="cuda", index=0))
        locality_mask_22 = None
        local_attention_mask_29 = torch.logical_and(
            local_attention_mask_28, locality_mask_23
        )
        local_attention_mask_28 = locality_mask_23 = None
        unsqueeze_66 = local_attention_mask_29.unsqueeze(1)
        local_attention_mask_29 = None
        local_attention_mask_30 = unsqueeze_66.to(device(type="cuda", index=0))
        unsqueeze_66 = None
        gt_19 = local_attention_mask_30 > 0
        local_attention_mask_30 = None
        local_attention_mask_31 = torch.where(gt_19, 0.0, -10000000000.0)
        gt_19 = local_attention_mask_31 = None
        scores_14 += position_bias_2
        scores_15 = scores_14
        scores_14 = None
        float_10 = scores_15.float()
        softmax_7 = torch.nn.functional.softmax(float_10, dim=-1)
        float_10 = None
        attn_weights_21 = softmax_7.type_as(scores_15)
        softmax_7 = scores_15 = None
        attn_weights_22 = torch.nn.functional.dropout(
            attn_weights_21, p=0.1, training=False
        )
        attn_weights_21 = None
        attn_weights_23 = attn_weights_22.type(torch.float32)
        attn_weights_22 = None
        einsum_23 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_23, value_states_31
        )
        attn_weights_23 = value_states_31 = None
        contiguous_7 = einsum_23.contiguous()
        einsum_23 = None
        attn_output_21 = contiguous_7.view(1, -1, 768)
        contiguous_7 = None
        attn_output_22 = attn_output_21[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_21 = None
        attn_output_23 = torch._C._nn.linear(
            attn_output_22,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_22 = l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_30 = torch.nn.functional.dropout(attn_output_23, 0.1, False, False)
        attn_output_23 = None
        hidden_states_59 = hidden_states_56 + dropout_30
        hidden_states_56 = dropout_30 = None
        to_62 = hidden_states_59.to(torch.float32)
        pow_31 = to_62.pow(2)
        to_62 = None
        variance_23 = pow_31.mean(-1, keepdim=True)
        pow_31 = None
        add_76 = variance_23 + 1e-06
        variance_23 = None
        rsqrt_23 = torch.rsqrt(add_76)
        add_76 = None
        hidden_states_60 = hidden_states_59 * rsqrt_23
        rsqrt_23 = None
        forwarded_states_7 = (
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_60
        )
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_60
        ) = None
        linear_69 = torch._C._nn.linear(
            forwarded_states_7,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_96 = 0.5 * linear_69
        pow_32 = torch.pow(linear_69, 3.0)
        mul_97 = 0.044715 * pow_32
        pow_32 = None
        add_77 = linear_69 + mul_97
        linear_69 = mul_97 = None
        mul_98 = 0.7978845608028654 * add_77
        add_77 = None
        tanh_7 = torch.tanh(mul_98)
        mul_98 = None
        add_78 = 1.0 + tanh_7
        tanh_7 = None
        hidden_gelu_7 = mul_96 * add_78
        mul_96 = add_78 = None
        hidden_linear_7 = torch._C._nn.linear(
            forwarded_states_7,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_7 = l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_61 = hidden_gelu_7 * hidden_linear_7
        hidden_gelu_7 = hidden_linear_7 = None
        hidden_states_62 = torch.nn.functional.dropout(
            hidden_states_61, 0.1, False, False
        )
        hidden_states_61 = None
        hidden_states_63 = torch._C._nn.linear(
            hidden_states_62,
            l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_62 = l_self_modules_encoder_modules_block_modules_7_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_32 = torch.nn.functional.dropout(hidden_states_63, 0.1, False, False)
        hidden_states_63 = None
        hidden_states_64 = hidden_states_59 + dropout_32
        hidden_states_59 = dropout_32 = None
        to_63 = hidden_states_64.to(torch.float32)
        pow_33 = to_63.pow(2)
        to_63 = None
        variance_24 = pow_33.mean(-1, keepdim=True)
        pow_33 = None
        add_80 = variance_24 + 1e-06
        variance_24 = None
        rsqrt_24 = torch.rsqrt(add_80)
        add_80 = None
        hidden_states_65 = hidden_states_64 * rsqrt_24
        rsqrt_24 = None
        normed_hidden_states_8 = (
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_65
        )
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_65
        ) = None
        ones_like_9 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_18 = ones_like_9 / 16
        ones_like_9 = None
        cumsum_18 = torch.cumsum(fixed_block_mask_18, axis=1)
        fixed_block_mask_19 = cumsum_18 - fixed_block_mask_18
        cumsum_18 = fixed_block_mask_18 = None
        ne_9 = l_attention_mask_ != 0.0
        where_55 = torch.where(ne_9, 1.0, -1000.0)
        ne_9 = None
        mask_9 = where_55.type(torch.int64)
        where_55 = None
        add_81 = mask_9 + fixed_block_mask_19
        mask_9 = fixed_block_mask_19 = None
        sub_56 = add_81 - 1.0
        add_81 = None
        floor_9 = torch.floor(sub_56)
        sub_56 = None
        global_block_ids_27 = floor_9.type(torch.int64)
        floor_9 = None
        _global_block_ids_lower_bound_9 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_20 = global_block_ids_27 > _global_block_ids_lower_bound_9
        global_block_ids_28 = torch.where(
            gt_20, global_block_ids_27, _global_block_ids_lower_bound_9
        )
        gt_20 = global_block_ids_27 = _global_block_ids_lower_bound_9 = None
        mul_103 = global_block_ids_28 * l_attention_mask_
        global_block_ids_28 = None
        sub_57 = l_attention_mask_ - 1
        global_block_ids_29 = mul_103 + sub_57
        mul_103 = sub_57 = None
        arange_20 = torch.arange(12)
        mod_9 = arange_20 % 16
        arange_20 = None
        block_ends_18 = mod_9.__eq__(15)
        mod_9 = None
        block_ends_19 = block_ends_18.to(device(type="cuda", index=0))
        block_ends_18 = None
        ge_17 = global_block_ids_29 >= 0
        true_block_ends_9 = torch.logical_and(block_ends_19, ge_17)
        block_ends_19 = ge_17 = None
        sum_10 = true_block_ends_9.sum(-1)
        true_block_ends_9 = None
        unsqueeze_67 = sum_10.unsqueeze(-1)
        sum_10 = None
        type_75 = unsqueeze_67.type(torch.int64)
        unsqueeze_67 = None
        full_blocks_9 = type_75 - 1
        type_75 = None
        lt_19 = global_block_ids_29 < full_blocks_9
        block_ids_26 = torch.where(lt_19, global_block_ids_29, full_blocks_9)
        lt_19 = global_block_ids_29 = full_blocks_9 = None
        _sequence_block_ids_max_9 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_9 = torch.ones(1, 0)
        cumsum_19 = torch.cumsum(ones_9, dim=-1)
        ones_9 = None
        global_segment_ids_36 = cumsum_19 - 1
        cumsum_19 = None
        global_segment_ids_37 = global_segment_ids_36.to(device(type="cuda", index=0))
        global_segment_ids_36 = None
        le_9 = global_segment_ids_37 <= _sequence_block_ids_max_9
        global_segment_ids_37 = _sequence_block_ids_max_9 = None
        global_segment_ids_38 = torch.where(le_9, 1, 0)
        le_9 = None
        block_ids_27 = block_ids_26.type(torch.int32)
        block_ids_26 = None
        global_segment_ids_39 = global_segment_ids_38.type(torch.int32)
        global_segment_ids_38 = global_segment_ids_39 = None
        ge_18 = block_ids_27 >= 0
        tensor_18 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_28 = block_ids_27.where(ge_18, tensor_18)
        block_ids_27 = ge_18 = tensor_18 = None
        type_78 = block_ids_28.type(torch.int64)
        block_ids_28 = None
        one_hot_8 = torch._C._nn.one_hot(type_78, 1)
        type_78 = None
        one_hot_block_ids_8 = one_hot_8[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_8 = None
        type_79 = one_hot_block_ids_8.type(torch.float32)
        one_hot_block_ids_8 = None
        global_inputs_16 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_8, type_79
        )
        type_79 = None
        to_66 = global_inputs_16.to(torch.float32)
        pow_34 = to_66.pow(2)
        to_66 = None
        variance_25 = pow_34.mean(-1, keepdim=True)
        pow_34 = None
        add_83 = variance_25 + 1e-06
        variance_25 = None
        rsqrt_25 = torch.rsqrt(add_83)
        add_83 = None
        hidden_states_66 = global_inputs_16 * rsqrt_25
        global_inputs_16 = rsqrt_25 = None
        global_inputs_17 = (
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_66
        )
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_66
        ) = None
        linear_72 = torch._C._nn.linear(
            normed_hidden_states_8,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_16 = linear_72.view(1, -1, 12, 64)
        linear_72 = None
        linear_73 = torch._C._nn.linear(
            normed_hidden_states_8,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_32 = linear_73.view(1, -1, 12, 64)
        linear_73 = None
        linear_74 = torch._C._nn.linear(
            normed_hidden_states_8,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_8 = None
        value_states_32 = linear_74.view(1, -1, 12, 64)
        linear_74 = None
        linear_75 = torch._C._nn.linear(
            global_inputs_17,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_16 = linear_75.view(1, -1, 12, 64)
        linear_75 = None
        linear_76 = torch._C._nn.linear(
            global_inputs_17,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_17 = l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_16 = linear_76.view(1, -1, 12, 64)
        linear_76 = None
        x_57 = torch._C._nn.pad(
            query_states_16, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_16 = None
        query_states_17 = x_57.reshape((1, 1, 128, 12, 64))
        x_57 = None
        x_58 = torch._C._nn.pad(
            key_states_32, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_32 = None
        key_states_33 = x_58.reshape((1, 1, 128, 12, 64))
        x_58 = None
        x_59 = torch._C._nn.pad(
            value_states_32, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_32 = None
        value_states_33 = x_59.reshape((1, 1, 128, 12, 64))
        x_59 = None
        x_60 = torch._C._nn.pad(
            key_states_33, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_33 = None
        getitem_112 = x_60[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_113 = x_60[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_114 = x_60[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_60 = None
        key_states_34 = torch.cat([getitem_112, getitem_113, getitem_114], dim=2)
        getitem_112 = getitem_113 = getitem_114 = None
        x_61 = torch._C._nn.pad(
            value_states_33, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_33 = None
        getitem_115 = x_61[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_116 = x_61[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_117 = x_61[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_61 = None
        value_states_34 = torch.cat([getitem_115, getitem_116, getitem_117], dim=2)
        getitem_115 = getitem_116 = getitem_117 = None
        unsqueeze_68 = side_key_states_16.unsqueeze(1)
        side_key_states_16 = None
        side_key_states_17 = unsqueeze_68.repeat([1, 1, 1, 1, 1])
        unsqueeze_68 = None
        unsqueeze_69 = side_value_states_16.unsqueeze(1)
        side_value_states_16 = None
        side_value_states_17 = unsqueeze_69.repeat([1, 1, 1, 1, 1])
        unsqueeze_69 = None
        key_states_35 = torch.cat([key_states_34, side_key_states_17], dim=2)
        key_states_34 = side_key_states_17 = None
        value_states_35 = torch.cat([value_states_34, side_value_states_17], dim=2)
        value_states_34 = side_value_states_17 = None
        scores_16 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_17, key_states_35
        )
        query_states_17 = key_states_35 = None
        x_62 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_16 = x_62.reshape((1, 1, 128))
        x_62 = None
        x_63 = torch._C._nn.pad(
            _blocked_attention_mask_16, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_118 = x_63[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_119 = x_63[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_120 = x_63[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_63 = None
        _3blocked_attention_mask_16 = torch.cat(
            [getitem_118, getitem_119, getitem_120], dim=2
        )
        getitem_118 = getitem_119 = getitem_120 = None
        _blocked_attention_mask_17 = _blocked_attention_mask_16.unsqueeze(-1)
        _blocked_attention_mask_16 = None
        _3blocked_attention_mask_17 = _3blocked_attention_mask_16.unsqueeze(-2)
        _3blocked_attention_mask_16 = None
        local_attention_mask_32 = torch.logical_and(
            _blocked_attention_mask_17, _3blocked_attention_mask_17
        )
        _blocked_attention_mask_17 = _3blocked_attention_mask_17 = None
        position_ids_8 = torch.arange(384, dtype=torch.int32)
        center_position_ids_8 = position_ids_8[slice(128, -128, None)]
        unsqueeze_72 = position_ids_8.unsqueeze(0)
        position_ids_8 = None
        unsqueeze_73 = center_position_ids_8.unsqueeze(1)
        center_position_ids_8 = None
        relative_position_ids_8 = unsqueeze_72 - unsqueeze_73
        unsqueeze_72 = unsqueeze_73 = None
        abs_11 = torch.abs(relative_position_ids_8)
        relative_position_ids_8 = None
        locality_mask_24 = abs_11 < 128
        abs_11 = None
        locality_mask_25 = locality_mask_24[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_24 = None
        locality_mask_26 = locality_mask_25.to(device(type="cuda", index=0))
        locality_mask_25 = None
        local_attention_mask_33 = torch.logical_and(
            local_attention_mask_32, locality_mask_26
        )
        local_attention_mask_32 = locality_mask_26 = None
        unsqueeze_74 = local_attention_mask_33.unsqueeze(1)
        local_attention_mask_33 = None
        local_attention_mask_34 = unsqueeze_74.to(device(type="cuda", index=0))
        unsqueeze_74 = None
        gt_21 = local_attention_mask_34 > 0
        local_attention_mask_34 = None
        local_attention_mask_35 = torch.where(gt_21, 0.0, -10000000000.0)
        gt_21 = local_attention_mask_35 = None
        scores_16 += position_bias_2
        scores_17 = scores_16
        scores_16 = None
        float_11 = scores_17.float()
        softmax_8 = torch.nn.functional.softmax(float_11, dim=-1)
        float_11 = None
        attn_weights_24 = softmax_8.type_as(scores_17)
        softmax_8 = scores_17 = None
        attn_weights_25 = torch.nn.functional.dropout(
            attn_weights_24, p=0.1, training=False
        )
        attn_weights_24 = None
        attn_weights_26 = attn_weights_25.type(torch.float32)
        attn_weights_25 = None
        einsum_26 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_26, value_states_35
        )
        attn_weights_26 = value_states_35 = None
        contiguous_8 = einsum_26.contiguous()
        einsum_26 = None
        attn_output_24 = contiguous_8.view(1, -1, 768)
        contiguous_8 = None
        attn_output_25 = attn_output_24[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_24 = None
        attn_output_26 = torch._C._nn.linear(
            attn_output_25,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_25 = l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_34 = torch.nn.functional.dropout(attn_output_26, 0.1, False, False)
        attn_output_26 = None
        hidden_states_67 = hidden_states_64 + dropout_34
        hidden_states_64 = dropout_34 = None
        to_69 = hidden_states_67.to(torch.float32)
        pow_35 = to_69.pow(2)
        to_69 = None
        variance_26 = pow_35.mean(-1, keepdim=True)
        pow_35 = None
        add_85 = variance_26 + 1e-06
        variance_26 = None
        rsqrt_26 = torch.rsqrt(add_85)
        add_85 = None
        hidden_states_68 = hidden_states_67 * rsqrt_26
        rsqrt_26 = None
        forwarded_states_8 = (
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_68
        )
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_68
        ) = None
        linear_78 = torch._C._nn.linear(
            forwarded_states_8,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_108 = 0.5 * linear_78
        pow_36 = torch.pow(linear_78, 3.0)
        mul_109 = 0.044715 * pow_36
        pow_36 = None
        add_86 = linear_78 + mul_109
        linear_78 = mul_109 = None
        mul_110 = 0.7978845608028654 * add_86
        add_86 = None
        tanh_8 = torch.tanh(mul_110)
        mul_110 = None
        add_87 = 1.0 + tanh_8
        tanh_8 = None
        hidden_gelu_8 = mul_108 * add_87
        mul_108 = add_87 = None
        hidden_linear_8 = torch._C._nn.linear(
            forwarded_states_8,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_8 = l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_69 = hidden_gelu_8 * hidden_linear_8
        hidden_gelu_8 = hidden_linear_8 = None
        hidden_states_70 = torch.nn.functional.dropout(
            hidden_states_69, 0.1, False, False
        )
        hidden_states_69 = None
        hidden_states_71 = torch._C._nn.linear(
            hidden_states_70,
            l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_70 = l_self_modules_encoder_modules_block_modules_8_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_36 = torch.nn.functional.dropout(hidden_states_71, 0.1, False, False)
        hidden_states_71 = None
        hidden_states_72 = hidden_states_67 + dropout_36
        hidden_states_67 = dropout_36 = None
        to_70 = hidden_states_72.to(torch.float32)
        pow_37 = to_70.pow(2)
        to_70 = None
        variance_27 = pow_37.mean(-1, keepdim=True)
        pow_37 = None
        add_89 = variance_27 + 1e-06
        variance_27 = None
        rsqrt_27 = torch.rsqrt(add_89)
        add_89 = None
        hidden_states_73 = hidden_states_72 * rsqrt_27
        rsqrt_27 = None
        normed_hidden_states_9 = (
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_73
        )
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_73
        ) = None
        ones_like_10 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_20 = ones_like_10 / 16
        ones_like_10 = None
        cumsum_20 = torch.cumsum(fixed_block_mask_20, axis=1)
        fixed_block_mask_21 = cumsum_20 - fixed_block_mask_20
        cumsum_20 = fixed_block_mask_20 = None
        ne_10 = l_attention_mask_ != 0.0
        where_61 = torch.where(ne_10, 1.0, -1000.0)
        ne_10 = None
        mask_10 = where_61.type(torch.int64)
        where_61 = None
        add_90 = mask_10 + fixed_block_mask_21
        mask_10 = fixed_block_mask_21 = None
        sub_62 = add_90 - 1.0
        add_90 = None
        floor_10 = torch.floor(sub_62)
        sub_62 = None
        global_block_ids_30 = floor_10.type(torch.int64)
        floor_10 = None
        _global_block_ids_lower_bound_10 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_22 = global_block_ids_30 > _global_block_ids_lower_bound_10
        global_block_ids_31 = torch.where(
            gt_22, global_block_ids_30, _global_block_ids_lower_bound_10
        )
        gt_22 = global_block_ids_30 = _global_block_ids_lower_bound_10 = None
        mul_115 = global_block_ids_31 * l_attention_mask_
        global_block_ids_31 = None
        sub_63 = l_attention_mask_ - 1
        global_block_ids_32 = mul_115 + sub_63
        mul_115 = sub_63 = None
        arange_22 = torch.arange(12)
        mod_10 = arange_22 % 16
        arange_22 = None
        block_ends_20 = mod_10.__eq__(15)
        mod_10 = None
        block_ends_21 = block_ends_20.to(device(type="cuda", index=0))
        block_ends_20 = None
        ge_19 = global_block_ids_32 >= 0
        true_block_ends_10 = torch.logical_and(block_ends_21, ge_19)
        block_ends_21 = ge_19 = None
        sum_11 = true_block_ends_10.sum(-1)
        true_block_ends_10 = None
        unsqueeze_75 = sum_11.unsqueeze(-1)
        sum_11 = None
        type_83 = unsqueeze_75.type(torch.int64)
        unsqueeze_75 = None
        full_blocks_10 = type_83 - 1
        type_83 = None
        lt_21 = global_block_ids_32 < full_blocks_10
        block_ids_29 = torch.where(lt_21, global_block_ids_32, full_blocks_10)
        lt_21 = global_block_ids_32 = full_blocks_10 = None
        _sequence_block_ids_max_10 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_10 = torch.ones(1, 0)
        cumsum_21 = torch.cumsum(ones_10, dim=-1)
        ones_10 = None
        global_segment_ids_40 = cumsum_21 - 1
        cumsum_21 = None
        global_segment_ids_41 = global_segment_ids_40.to(device(type="cuda", index=0))
        global_segment_ids_40 = None
        le_10 = global_segment_ids_41 <= _sequence_block_ids_max_10
        global_segment_ids_41 = _sequence_block_ids_max_10 = None
        global_segment_ids_42 = torch.where(le_10, 1, 0)
        le_10 = None
        block_ids_30 = block_ids_29.type(torch.int32)
        block_ids_29 = None
        global_segment_ids_43 = global_segment_ids_42.type(torch.int32)
        global_segment_ids_42 = global_segment_ids_43 = None
        ge_20 = block_ids_30 >= 0
        tensor_20 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_31 = block_ids_30.where(ge_20, tensor_20)
        block_ids_30 = ge_20 = tensor_20 = None
        type_86 = block_ids_31.type(torch.int64)
        block_ids_31 = None
        one_hot_9 = torch._C._nn.one_hot(type_86, 1)
        type_86 = None
        one_hot_block_ids_9 = one_hot_9[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_9 = None
        type_87 = one_hot_block_ids_9.type(torch.float32)
        one_hot_block_ids_9 = None
        global_inputs_18 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_9, type_87
        )
        type_87 = None
        to_73 = global_inputs_18.to(torch.float32)
        pow_38 = to_73.pow(2)
        to_73 = None
        variance_28 = pow_38.mean(-1, keepdim=True)
        pow_38 = None
        add_92 = variance_28 + 1e-06
        variance_28 = None
        rsqrt_28 = torch.rsqrt(add_92)
        add_92 = None
        hidden_states_74 = global_inputs_18 * rsqrt_28
        global_inputs_18 = rsqrt_28 = None
        global_inputs_19 = (
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_74
        )
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_74
        ) = None
        linear_81 = torch._C._nn.linear(
            normed_hidden_states_9,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_18 = linear_81.view(1, -1, 12, 64)
        linear_81 = None
        linear_82 = torch._C._nn.linear(
            normed_hidden_states_9,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_36 = linear_82.view(1, -1, 12, 64)
        linear_82 = None
        linear_83 = torch._C._nn.linear(
            normed_hidden_states_9,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_9 = None
        value_states_36 = linear_83.view(1, -1, 12, 64)
        linear_83 = None
        linear_84 = torch._C._nn.linear(
            global_inputs_19,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_18 = linear_84.view(1, -1, 12, 64)
        linear_84 = None
        linear_85 = torch._C._nn.linear(
            global_inputs_19,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_19 = l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_18 = linear_85.view(1, -1, 12, 64)
        linear_85 = None
        x_64 = torch._C._nn.pad(
            query_states_18, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_18 = None
        query_states_19 = x_64.reshape((1, 1, 128, 12, 64))
        x_64 = None
        x_65 = torch._C._nn.pad(
            key_states_36, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_36 = None
        key_states_37 = x_65.reshape((1, 1, 128, 12, 64))
        x_65 = None
        x_66 = torch._C._nn.pad(
            value_states_36, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_36 = None
        value_states_37 = x_66.reshape((1, 1, 128, 12, 64))
        x_66 = None
        x_67 = torch._C._nn.pad(
            key_states_37, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_37 = None
        getitem_125 = x_67[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_126 = x_67[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_127 = x_67[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_67 = None
        key_states_38 = torch.cat([getitem_125, getitem_126, getitem_127], dim=2)
        getitem_125 = getitem_126 = getitem_127 = None
        x_68 = torch._C._nn.pad(
            value_states_37, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_37 = None
        getitem_128 = x_68[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_129 = x_68[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_130 = x_68[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_68 = None
        value_states_38 = torch.cat([getitem_128, getitem_129, getitem_130], dim=2)
        getitem_128 = getitem_129 = getitem_130 = None
        unsqueeze_76 = side_key_states_18.unsqueeze(1)
        side_key_states_18 = None
        side_key_states_19 = unsqueeze_76.repeat([1, 1, 1, 1, 1])
        unsqueeze_76 = None
        unsqueeze_77 = side_value_states_18.unsqueeze(1)
        side_value_states_18 = None
        side_value_states_19 = unsqueeze_77.repeat([1, 1, 1, 1, 1])
        unsqueeze_77 = None
        key_states_39 = torch.cat([key_states_38, side_key_states_19], dim=2)
        key_states_38 = side_key_states_19 = None
        value_states_39 = torch.cat([value_states_38, side_value_states_19], dim=2)
        value_states_38 = side_value_states_19 = None
        scores_18 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_19, key_states_39
        )
        query_states_19 = key_states_39 = None
        x_69 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_18 = x_69.reshape((1, 1, 128))
        x_69 = None
        x_70 = torch._C._nn.pad(
            _blocked_attention_mask_18, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_131 = x_70[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_132 = x_70[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_133 = x_70[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_70 = None
        _3blocked_attention_mask_18 = torch.cat(
            [getitem_131, getitem_132, getitem_133], dim=2
        )
        getitem_131 = getitem_132 = getitem_133 = None
        _blocked_attention_mask_19 = _blocked_attention_mask_18.unsqueeze(-1)
        _blocked_attention_mask_18 = None
        _3blocked_attention_mask_19 = _3blocked_attention_mask_18.unsqueeze(-2)
        _3blocked_attention_mask_18 = None
        local_attention_mask_36 = torch.logical_and(
            _blocked_attention_mask_19, _3blocked_attention_mask_19
        )
        _blocked_attention_mask_19 = _3blocked_attention_mask_19 = None
        position_ids_9 = torch.arange(384, dtype=torch.int32)
        center_position_ids_9 = position_ids_9[slice(128, -128, None)]
        unsqueeze_80 = position_ids_9.unsqueeze(0)
        position_ids_9 = None
        unsqueeze_81 = center_position_ids_9.unsqueeze(1)
        center_position_ids_9 = None
        relative_position_ids_9 = unsqueeze_80 - unsqueeze_81
        unsqueeze_80 = unsqueeze_81 = None
        abs_12 = torch.abs(relative_position_ids_9)
        relative_position_ids_9 = None
        locality_mask_27 = abs_12 < 128
        abs_12 = None
        locality_mask_28 = locality_mask_27[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_27 = None
        locality_mask_29 = locality_mask_28.to(device(type="cuda", index=0))
        locality_mask_28 = None
        local_attention_mask_37 = torch.logical_and(
            local_attention_mask_36, locality_mask_29
        )
        local_attention_mask_36 = locality_mask_29 = None
        unsqueeze_82 = local_attention_mask_37.unsqueeze(1)
        local_attention_mask_37 = None
        local_attention_mask_38 = unsqueeze_82.to(device(type="cuda", index=0))
        unsqueeze_82 = None
        gt_23 = local_attention_mask_38 > 0
        local_attention_mask_38 = None
        local_attention_mask_39 = torch.where(gt_23, 0.0, -10000000000.0)
        gt_23 = local_attention_mask_39 = None
        scores_18 += position_bias_2
        scores_19 = scores_18
        scores_18 = None
        float_12 = scores_19.float()
        softmax_9 = torch.nn.functional.softmax(float_12, dim=-1)
        float_12 = None
        attn_weights_27 = softmax_9.type_as(scores_19)
        softmax_9 = scores_19 = None
        attn_weights_28 = torch.nn.functional.dropout(
            attn_weights_27, p=0.1, training=False
        )
        attn_weights_27 = None
        attn_weights_29 = attn_weights_28.type(torch.float32)
        attn_weights_28 = None
        einsum_29 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_29, value_states_39
        )
        attn_weights_29 = value_states_39 = None
        contiguous_9 = einsum_29.contiguous()
        einsum_29 = None
        attn_output_27 = contiguous_9.view(1, -1, 768)
        contiguous_9 = None
        attn_output_28 = attn_output_27[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_27 = None
        attn_output_29 = torch._C._nn.linear(
            attn_output_28,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_28 = l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_38 = torch.nn.functional.dropout(attn_output_29, 0.1, False, False)
        attn_output_29 = None
        hidden_states_75 = hidden_states_72 + dropout_38
        hidden_states_72 = dropout_38 = None
        to_76 = hidden_states_75.to(torch.float32)
        pow_39 = to_76.pow(2)
        to_76 = None
        variance_29 = pow_39.mean(-1, keepdim=True)
        pow_39 = None
        add_94 = variance_29 + 1e-06
        variance_29 = None
        rsqrt_29 = torch.rsqrt(add_94)
        add_94 = None
        hidden_states_76 = hidden_states_75 * rsqrt_29
        rsqrt_29 = None
        forwarded_states_9 = (
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_76
        )
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_76
        ) = None
        linear_87 = torch._C._nn.linear(
            forwarded_states_9,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_120 = 0.5 * linear_87
        pow_40 = torch.pow(linear_87, 3.0)
        mul_121 = 0.044715 * pow_40
        pow_40 = None
        add_95 = linear_87 + mul_121
        linear_87 = mul_121 = None
        mul_122 = 0.7978845608028654 * add_95
        add_95 = None
        tanh_9 = torch.tanh(mul_122)
        mul_122 = None
        add_96 = 1.0 + tanh_9
        tanh_9 = None
        hidden_gelu_9 = mul_120 * add_96
        mul_120 = add_96 = None
        hidden_linear_9 = torch._C._nn.linear(
            forwarded_states_9,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_9 = l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_77 = hidden_gelu_9 * hidden_linear_9
        hidden_gelu_9 = hidden_linear_9 = None
        hidden_states_78 = torch.nn.functional.dropout(
            hidden_states_77, 0.1, False, False
        )
        hidden_states_77 = None
        hidden_states_79 = torch._C._nn.linear(
            hidden_states_78,
            l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_78 = l_self_modules_encoder_modules_block_modules_9_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_40 = torch.nn.functional.dropout(hidden_states_79, 0.1, False, False)
        hidden_states_79 = None
        hidden_states_80 = hidden_states_75 + dropout_40
        hidden_states_75 = dropout_40 = None
        to_77 = hidden_states_80.to(torch.float32)
        pow_41 = to_77.pow(2)
        to_77 = None
        variance_30 = pow_41.mean(-1, keepdim=True)
        pow_41 = None
        add_98 = variance_30 + 1e-06
        variance_30 = None
        rsqrt_30 = torch.rsqrt(add_98)
        add_98 = None
        hidden_states_81 = hidden_states_80 * rsqrt_30
        rsqrt_30 = None
        normed_hidden_states_10 = (
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_81
        )
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_81
        ) = None
        ones_like_11 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_22 = ones_like_11 / 16
        ones_like_11 = None
        cumsum_22 = torch.cumsum(fixed_block_mask_22, axis=1)
        fixed_block_mask_23 = cumsum_22 - fixed_block_mask_22
        cumsum_22 = fixed_block_mask_22 = None
        ne_11 = l_attention_mask_ != 0.0
        where_67 = torch.where(ne_11, 1.0, -1000.0)
        ne_11 = None
        mask_11 = where_67.type(torch.int64)
        where_67 = None
        add_99 = mask_11 + fixed_block_mask_23
        mask_11 = fixed_block_mask_23 = None
        sub_68 = add_99 - 1.0
        add_99 = None
        floor_11 = torch.floor(sub_68)
        sub_68 = None
        global_block_ids_33 = floor_11.type(torch.int64)
        floor_11 = None
        _global_block_ids_lower_bound_11 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_24 = global_block_ids_33 > _global_block_ids_lower_bound_11
        global_block_ids_34 = torch.where(
            gt_24, global_block_ids_33, _global_block_ids_lower_bound_11
        )
        gt_24 = global_block_ids_33 = _global_block_ids_lower_bound_11 = None
        mul_127 = global_block_ids_34 * l_attention_mask_
        global_block_ids_34 = None
        sub_69 = l_attention_mask_ - 1
        global_block_ids_35 = mul_127 + sub_69
        mul_127 = sub_69 = None
        arange_24 = torch.arange(12)
        mod_11 = arange_24 % 16
        arange_24 = None
        block_ends_22 = mod_11.__eq__(15)
        mod_11 = None
        block_ends_23 = block_ends_22.to(device(type="cuda", index=0))
        block_ends_22 = None
        ge_21 = global_block_ids_35 >= 0
        true_block_ends_11 = torch.logical_and(block_ends_23, ge_21)
        block_ends_23 = ge_21 = None
        sum_12 = true_block_ends_11.sum(-1)
        true_block_ends_11 = None
        unsqueeze_83 = sum_12.unsqueeze(-1)
        sum_12 = None
        type_91 = unsqueeze_83.type(torch.int64)
        unsqueeze_83 = None
        full_blocks_11 = type_91 - 1
        type_91 = None
        lt_23 = global_block_ids_35 < full_blocks_11
        block_ids_32 = torch.where(lt_23, global_block_ids_35, full_blocks_11)
        lt_23 = global_block_ids_35 = full_blocks_11 = None
        _sequence_block_ids_max_11 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_11 = torch.ones(1, 0)
        cumsum_23 = torch.cumsum(ones_11, dim=-1)
        ones_11 = None
        global_segment_ids_44 = cumsum_23 - 1
        cumsum_23 = None
        global_segment_ids_45 = global_segment_ids_44.to(device(type="cuda", index=0))
        global_segment_ids_44 = None
        le_11 = global_segment_ids_45 <= _sequence_block_ids_max_11
        global_segment_ids_45 = _sequence_block_ids_max_11 = None
        global_segment_ids_46 = torch.where(le_11, 1, 0)
        le_11 = None
        block_ids_33 = block_ids_32.type(torch.int32)
        block_ids_32 = None
        global_segment_ids_47 = global_segment_ids_46.type(torch.int32)
        global_segment_ids_46 = global_segment_ids_47 = None
        ge_22 = block_ids_33 >= 0
        tensor_22 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_34 = block_ids_33.where(ge_22, tensor_22)
        block_ids_33 = ge_22 = tensor_22 = None
        type_94 = block_ids_34.type(torch.int64)
        block_ids_34 = None
        one_hot_10 = torch._C._nn.one_hot(type_94, 1)
        type_94 = None
        one_hot_block_ids_10 = one_hot_10[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_10 = None
        type_95 = one_hot_block_ids_10.type(torch.float32)
        one_hot_block_ids_10 = None
        global_inputs_20 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_10, type_95
        )
        type_95 = None
        to_80 = global_inputs_20.to(torch.float32)
        pow_42 = to_80.pow(2)
        to_80 = None
        variance_31 = pow_42.mean(-1, keepdim=True)
        pow_42 = None
        add_101 = variance_31 + 1e-06
        variance_31 = None
        rsqrt_31 = torch.rsqrt(add_101)
        add_101 = None
        hidden_states_82 = global_inputs_20 * rsqrt_31
        global_inputs_20 = rsqrt_31 = None
        global_inputs_21 = (
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_82
        )
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_82
        ) = None
        linear_90 = torch._C._nn.linear(
            normed_hidden_states_10,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_20 = linear_90.view(1, -1, 12, 64)
        linear_90 = None
        linear_91 = torch._C._nn.linear(
            normed_hidden_states_10,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_40 = linear_91.view(1, -1, 12, 64)
        linear_91 = None
        linear_92 = torch._C._nn.linear(
            normed_hidden_states_10,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_10 = None
        value_states_40 = linear_92.view(1, -1, 12, 64)
        linear_92 = None
        linear_93 = torch._C._nn.linear(
            global_inputs_21,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_20 = linear_93.view(1, -1, 12, 64)
        linear_93 = None
        linear_94 = torch._C._nn.linear(
            global_inputs_21,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_21 = l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_20 = linear_94.view(1, -1, 12, 64)
        linear_94 = None
        x_71 = torch._C._nn.pad(
            query_states_20, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_20 = None
        query_states_21 = x_71.reshape((1, 1, 128, 12, 64))
        x_71 = None
        x_72 = torch._C._nn.pad(
            key_states_40, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_40 = None
        key_states_41 = x_72.reshape((1, 1, 128, 12, 64))
        x_72 = None
        x_73 = torch._C._nn.pad(
            value_states_40, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_40 = None
        value_states_41 = x_73.reshape((1, 1, 128, 12, 64))
        x_73 = None
        x_74 = torch._C._nn.pad(
            key_states_41, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_41 = None
        getitem_138 = x_74[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_139 = x_74[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_140 = x_74[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_74 = None
        key_states_42 = torch.cat([getitem_138, getitem_139, getitem_140], dim=2)
        getitem_138 = getitem_139 = getitem_140 = None
        x_75 = torch._C._nn.pad(
            value_states_41, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_41 = None
        getitem_141 = x_75[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_142 = x_75[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_143 = x_75[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_75 = None
        value_states_42 = torch.cat([getitem_141, getitem_142, getitem_143], dim=2)
        getitem_141 = getitem_142 = getitem_143 = None
        unsqueeze_84 = side_key_states_20.unsqueeze(1)
        side_key_states_20 = None
        side_key_states_21 = unsqueeze_84.repeat([1, 1, 1, 1, 1])
        unsqueeze_84 = None
        unsqueeze_85 = side_value_states_20.unsqueeze(1)
        side_value_states_20 = None
        side_value_states_21 = unsqueeze_85.repeat([1, 1, 1, 1, 1])
        unsqueeze_85 = None
        key_states_43 = torch.cat([key_states_42, side_key_states_21], dim=2)
        key_states_42 = side_key_states_21 = None
        value_states_43 = torch.cat([value_states_42, side_value_states_21], dim=2)
        value_states_42 = side_value_states_21 = None
        scores_20 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_21, key_states_43
        )
        query_states_21 = key_states_43 = None
        x_76 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_20 = x_76.reshape((1, 1, 128))
        x_76 = None
        x_77 = torch._C._nn.pad(
            _blocked_attention_mask_20, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_144 = x_77[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_145 = x_77[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_146 = x_77[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_77 = None
        _3blocked_attention_mask_20 = torch.cat(
            [getitem_144, getitem_145, getitem_146], dim=2
        )
        getitem_144 = getitem_145 = getitem_146 = None
        _blocked_attention_mask_21 = _blocked_attention_mask_20.unsqueeze(-1)
        _blocked_attention_mask_20 = None
        _3blocked_attention_mask_21 = _3blocked_attention_mask_20.unsqueeze(-2)
        _3blocked_attention_mask_20 = None
        local_attention_mask_40 = torch.logical_and(
            _blocked_attention_mask_21, _3blocked_attention_mask_21
        )
        _blocked_attention_mask_21 = _3blocked_attention_mask_21 = None
        position_ids_10 = torch.arange(384, dtype=torch.int32)
        center_position_ids_10 = position_ids_10[slice(128, -128, None)]
        unsqueeze_88 = position_ids_10.unsqueeze(0)
        position_ids_10 = None
        unsqueeze_89 = center_position_ids_10.unsqueeze(1)
        center_position_ids_10 = None
        relative_position_ids_10 = unsqueeze_88 - unsqueeze_89
        unsqueeze_88 = unsqueeze_89 = None
        abs_13 = torch.abs(relative_position_ids_10)
        relative_position_ids_10 = None
        locality_mask_30 = abs_13 < 128
        abs_13 = None
        locality_mask_31 = locality_mask_30[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_30 = None
        locality_mask_32 = locality_mask_31.to(device(type="cuda", index=0))
        locality_mask_31 = None
        local_attention_mask_41 = torch.logical_and(
            local_attention_mask_40, locality_mask_32
        )
        local_attention_mask_40 = locality_mask_32 = None
        unsqueeze_90 = local_attention_mask_41.unsqueeze(1)
        local_attention_mask_41 = None
        local_attention_mask_42 = unsqueeze_90.to(device(type="cuda", index=0))
        unsqueeze_90 = None
        gt_25 = local_attention_mask_42 > 0
        local_attention_mask_42 = None
        local_attention_mask_43 = torch.where(gt_25, 0.0, -10000000000.0)
        gt_25 = local_attention_mask_43 = None
        scores_20 += position_bias_2
        scores_21 = scores_20
        scores_20 = None
        float_13 = scores_21.float()
        softmax_10 = torch.nn.functional.softmax(float_13, dim=-1)
        float_13 = None
        attn_weights_30 = softmax_10.type_as(scores_21)
        softmax_10 = scores_21 = None
        attn_weights_31 = torch.nn.functional.dropout(
            attn_weights_30, p=0.1, training=False
        )
        attn_weights_30 = None
        attn_weights_32 = attn_weights_31.type(torch.float32)
        attn_weights_31 = None
        einsum_32 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_32, value_states_43
        )
        attn_weights_32 = value_states_43 = None
        contiguous_10 = einsum_32.contiguous()
        einsum_32 = None
        attn_output_30 = contiguous_10.view(1, -1, 768)
        contiguous_10 = None
        attn_output_31 = attn_output_30[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_30 = None
        attn_output_32 = torch._C._nn.linear(
            attn_output_31,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_31 = l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_42 = torch.nn.functional.dropout(attn_output_32, 0.1, False, False)
        attn_output_32 = None
        hidden_states_83 = hidden_states_80 + dropout_42
        hidden_states_80 = dropout_42 = None
        to_83 = hidden_states_83.to(torch.float32)
        pow_43 = to_83.pow(2)
        to_83 = None
        variance_32 = pow_43.mean(-1, keepdim=True)
        pow_43 = None
        add_103 = variance_32 + 1e-06
        variance_32 = None
        rsqrt_32 = torch.rsqrt(add_103)
        add_103 = None
        hidden_states_84 = hidden_states_83 * rsqrt_32
        rsqrt_32 = None
        forwarded_states_10 = (
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_84
        )
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_84
        ) = None
        linear_96 = torch._C._nn.linear(
            forwarded_states_10,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_132 = 0.5 * linear_96
        pow_44 = torch.pow(linear_96, 3.0)
        mul_133 = 0.044715 * pow_44
        pow_44 = None
        add_104 = linear_96 + mul_133
        linear_96 = mul_133 = None
        mul_134 = 0.7978845608028654 * add_104
        add_104 = None
        tanh_10 = torch.tanh(mul_134)
        mul_134 = None
        add_105 = 1.0 + tanh_10
        tanh_10 = None
        hidden_gelu_10 = mul_132 * add_105
        mul_132 = add_105 = None
        hidden_linear_10 = torch._C._nn.linear(
            forwarded_states_10,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_10 = l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_85 = hidden_gelu_10 * hidden_linear_10
        hidden_gelu_10 = hidden_linear_10 = None
        hidden_states_86 = torch.nn.functional.dropout(
            hidden_states_85, 0.1, False, False
        )
        hidden_states_85 = None
        hidden_states_87 = torch._C._nn.linear(
            hidden_states_86,
            l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_86 = l_self_modules_encoder_modules_block_modules_10_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_44 = torch.nn.functional.dropout(hidden_states_87, 0.1, False, False)
        hidden_states_87 = None
        hidden_states_88 = hidden_states_83 + dropout_44
        hidden_states_83 = dropout_44 = None
        to_84 = hidden_states_88.to(torch.float32)
        pow_45 = to_84.pow(2)
        to_84 = None
        variance_33 = pow_45.mean(-1, keepdim=True)
        pow_45 = None
        add_107 = variance_33 + 1e-06
        variance_33 = None
        rsqrt_33 = torch.rsqrt(add_107)
        add_107 = None
        hidden_states_89 = hidden_states_88 * rsqrt_33
        rsqrt_33 = None
        normed_hidden_states_11 = (
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_89
        )
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_89
        ) = None
        ones_like_12 = torch.ones_like(
            l_attention_mask_, device=device(type="cuda", index=0)
        )
        fixed_block_mask_24 = ones_like_12 / 16
        ones_like_12 = None
        cumsum_24 = torch.cumsum(fixed_block_mask_24, axis=1)
        fixed_block_mask_25 = cumsum_24 - fixed_block_mask_24
        cumsum_24 = fixed_block_mask_24 = None
        ne_12 = l_attention_mask_ != 0.0
        where_73 = torch.where(ne_12, 1.0, -1000.0)
        ne_12 = None
        mask_12 = where_73.type(torch.int64)
        where_73 = None
        add_108 = mask_12 + fixed_block_mask_25
        mask_12 = fixed_block_mask_25 = None
        sub_74 = add_108 - 1.0
        add_108 = None
        floor_12 = torch.floor(sub_74)
        sub_74 = None
        global_block_ids_36 = floor_12.type(torch.int64)
        floor_12 = None
        _global_block_ids_lower_bound_12 = torch.tensor(
            -1, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        gt_26 = global_block_ids_36 > _global_block_ids_lower_bound_12
        global_block_ids_37 = torch.where(
            gt_26, global_block_ids_36, _global_block_ids_lower_bound_12
        )
        gt_26 = global_block_ids_36 = _global_block_ids_lower_bound_12 = None
        mul_139 = global_block_ids_37 * l_attention_mask_
        global_block_ids_37 = None
        sub_75 = l_attention_mask_ - 1
        global_block_ids_38 = mul_139 + sub_75
        mul_139 = sub_75 = None
        arange_26 = torch.arange(12)
        mod_12 = arange_26 % 16
        arange_26 = None
        block_ends_24 = mod_12.__eq__(15)
        mod_12 = None
        block_ends_25 = block_ends_24.to(device(type="cuda", index=0))
        block_ends_24 = None
        ge_23 = global_block_ids_38 >= 0
        true_block_ends_12 = torch.logical_and(block_ends_25, ge_23)
        block_ends_25 = ge_23 = None
        sum_13 = true_block_ends_12.sum(-1)
        true_block_ends_12 = None
        unsqueeze_91 = sum_13.unsqueeze(-1)
        sum_13 = None
        type_99 = unsqueeze_91.type(torch.int64)
        unsqueeze_91 = None
        full_blocks_12 = type_99 - 1
        type_99 = None
        lt_25 = global_block_ids_38 < full_blocks_12
        block_ids_35 = torch.where(lt_25, global_block_ids_38, full_blocks_12)
        lt_25 = global_block_ids_38 = full_blocks_12 = None
        _sequence_block_ids_max_12 = torch.zeros(
            1, 0, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        ones_12 = torch.ones(1, 0)
        cumsum_25 = torch.cumsum(ones_12, dim=-1)
        ones_12 = None
        global_segment_ids_48 = cumsum_25 - 1
        cumsum_25 = None
        global_segment_ids_49 = global_segment_ids_48.to(device(type="cuda", index=0))
        global_segment_ids_48 = None
        le_12 = global_segment_ids_49 <= _sequence_block_ids_max_12
        global_segment_ids_49 = _sequence_block_ids_max_12 = None
        global_segment_ids_50 = torch.where(le_12, 1, 0)
        le_12 = None
        block_ids_36 = block_ids_35.type(torch.int32)
        block_ids_35 = None
        global_segment_ids_51 = global_segment_ids_50.type(torch.int32)
        global_segment_ids_50 = global_segment_ids_51 = None
        ge_24 = block_ids_36 >= 0
        tensor_24 = torch.tensor(
            0, dtype=torch.int32, device=device(type="cuda", index=0)
        )
        block_ids_37 = block_ids_36.where(ge_24, tensor_24)
        block_ids_36 = ge_24 = tensor_24 = None
        type_102 = block_ids_37.type(torch.int64)
        block_ids_37 = None
        one_hot_11 = torch._C._nn.one_hot(type_102, 1)
        type_102 = None
        one_hot_block_ids_11 = one_hot_11[
            (slice(None, None, None), slice(None, None, None), slice(None, -1, None))
        ]
        one_hot_11 = None
        type_103 = one_hot_block_ids_11.type(torch.float32)
        one_hot_block_ids_11 = None
        global_inputs_22 = torch.functional.einsum(
            "...nd,...ng->...gd", normed_hidden_states_11, type_103
        )
        type_103 = None
        to_87 = global_inputs_22.to(torch.float32)
        pow_46 = to_87.pow(2)
        to_87 = None
        variance_34 = pow_46.mean(-1, keepdim=True)
        pow_46 = None
        add_110 = variance_34 + 1e-06
        variance_34 = None
        rsqrt_34 = torch.rsqrt(add_110)
        add_110 = None
        hidden_states_90 = global_inputs_22 * rsqrt_34
        global_inputs_22 = rsqrt_34 = None
        global_inputs_23 = (
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_
            * hidden_states_90
        )
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_global_input_layer_norm_parameters_weight_ = (
            hidden_states_90
        ) = None
        linear_99 = torch._C._nn.linear(
            normed_hidden_states_11,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_q_parameters_weight_ = (
            None
        )
        query_states_22 = linear_99.view(1, -1, 12, 64)
        linear_99 = None
        linear_100 = torch._C._nn.linear(
            normed_hidden_states_11,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        key_states_44 = linear_100.view(1, -1, 12, 64)
        linear_100 = None
        linear_101 = torch._C._nn.linear(
            normed_hidden_states_11,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_11 = None
        value_states_44 = linear_101.view(1, -1, 12, 64)
        linear_101 = None
        linear_102 = torch._C._nn.linear(
            global_inputs_23,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_k_parameters_weight_ = (
            None
        )
        side_key_states_22 = linear_102.view(1, -1, 12, 64)
        linear_102 = None
        linear_103 = torch._C._nn.linear(
            global_inputs_23,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_,
            None,
        )
        global_inputs_23 = l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_v_parameters_weight_ = (None)
        side_value_states_22 = linear_103.view(1, -1, 12, 64)
        linear_103 = None
        x_78 = torch._C._nn.pad(
            query_states_22, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        query_states_22 = None
        query_states_23 = x_78.reshape((1, 1, 128, 12, 64))
        x_78 = None
        x_79 = torch._C._nn.pad(
            key_states_44, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        key_states_44 = None
        key_states_45 = x_79.reshape((1, 1, 128, 12, 64))
        x_79 = None
        x_80 = torch._C._nn.pad(
            value_states_44, (0, 0, 0, 0, 0, 116, 0, 0), "constant", 0
        )
        value_states_44 = None
        value_states_45 = x_80.reshape((1, 1, 128, 12, 64))
        x_80 = None
        x_81 = torch._C._nn.pad(
            key_states_45, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        key_states_45 = None
        getitem_151 = x_81[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_152 = x_81[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_153 = x_81[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_81 = None
        key_states_46 = torch.cat([getitem_151, getitem_152, getitem_153], dim=2)
        getitem_151 = getitem_152 = getitem_153 = None
        x_82 = torch._C._nn.pad(
            value_states_45, (0, 0, 0, 0, 0, 0, 1, 1, 0, 0), "constant", 0
        )
        value_states_45 = None
        getitem_154 = x_82[
            (
                slice(0, None, None),
                slice(0, 1, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_155 = x_82[
            (
                slice(0, None, None),
                slice(1, 2, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        getitem_156 = x_82[
            (
                slice(0, None, None),
                slice(2, 3, None),
                slice(0, None, None),
                slice(0, None, None),
                slice(0, None, None),
            )
        ]
        x_82 = None
        value_states_46 = torch.cat([getitem_154, getitem_155, getitem_156], dim=2)
        getitem_154 = getitem_155 = getitem_156 = None
        unsqueeze_92 = side_key_states_22.unsqueeze(1)
        side_key_states_22 = None
        side_key_states_23 = unsqueeze_92.repeat([1, 1, 1, 1, 1])
        unsqueeze_92 = None
        unsqueeze_93 = side_value_states_22.unsqueeze(1)
        side_value_states_22 = None
        side_value_states_23 = unsqueeze_93.repeat([1, 1, 1, 1, 1])
        unsqueeze_93 = None
        key_states_47 = torch.cat([key_states_46, side_key_states_23], dim=2)
        key_states_46 = side_key_states_23 = None
        value_states_47 = torch.cat([value_states_46, side_value_states_23], dim=2)
        value_states_46 = side_value_states_23 = None
        scores_22 = torch.functional.einsum(
            "...qhd,...khd->...hqk", query_states_23, key_states_47
        )
        query_states_23 = key_states_47 = None
        x_83 = torch._C._nn.pad(l_attention_mask_, (0, 116, 0, 0), "constant", 0)
        _blocked_attention_mask_22 = x_83.reshape((1, 1, 128))
        x_83 = None
        x_84 = torch._C._nn.pad(
            _blocked_attention_mask_22, (0, 0, 1, 1, 0, 0), "constant", 0
        )
        getitem_157 = x_84[
            (slice(0, None, None), slice(0, 1, None), slice(0, None, None))
        ]
        getitem_158 = x_84[
            (slice(0, None, None), slice(1, 2, None), slice(0, None, None))
        ]
        getitem_159 = x_84[
            (slice(0, None, None), slice(2, 3, None), slice(0, None, None))
        ]
        x_84 = None
        _3blocked_attention_mask_22 = torch.cat(
            [getitem_157, getitem_158, getitem_159], dim=2
        )
        getitem_157 = getitem_158 = getitem_159 = None
        _blocked_attention_mask_23 = _blocked_attention_mask_22.unsqueeze(-1)
        _blocked_attention_mask_22 = None
        _3blocked_attention_mask_23 = _3blocked_attention_mask_22.unsqueeze(-2)
        _3blocked_attention_mask_22 = None
        local_attention_mask_44 = torch.logical_and(
            _blocked_attention_mask_23, _3blocked_attention_mask_23
        )
        _blocked_attention_mask_23 = _3blocked_attention_mask_23 = None
        position_ids_11 = torch.arange(384, dtype=torch.int32)
        center_position_ids_11 = position_ids_11[slice(128, -128, None)]
        unsqueeze_96 = position_ids_11.unsqueeze(0)
        position_ids_11 = None
        unsqueeze_97 = center_position_ids_11.unsqueeze(1)
        center_position_ids_11 = None
        relative_position_ids_11 = unsqueeze_96 - unsqueeze_97
        unsqueeze_96 = unsqueeze_97 = None
        abs_14 = torch.abs(relative_position_ids_11)
        relative_position_ids_11 = None
        locality_mask_33 = abs_14 < 128
        abs_14 = None
        locality_mask_34 = locality_mask_33[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        locality_mask_33 = None
        locality_mask_35 = locality_mask_34.to(device(type="cuda", index=0))
        locality_mask_34 = None
        local_attention_mask_45 = torch.logical_and(
            local_attention_mask_44, locality_mask_35
        )
        local_attention_mask_44 = locality_mask_35 = None
        unsqueeze_98 = local_attention_mask_45.unsqueeze(1)
        local_attention_mask_45 = None
        local_attention_mask_46 = unsqueeze_98.to(device(type="cuda", index=0))
        unsqueeze_98 = None
        gt_27 = local_attention_mask_46 > 0
        local_attention_mask_46 = None
        local_attention_mask_47 = torch.where(gt_27, 0.0, -10000000000.0)
        gt_27 = local_attention_mask_47 = None
        scores_22 += position_bias_2
        scores_23 = scores_22
        scores_22 = position_bias_2 = None
        float_14 = scores_23.float()
        softmax_11 = torch.nn.functional.softmax(float_14, dim=-1)
        float_14 = None
        attn_weights_33 = softmax_11.type_as(scores_23)
        softmax_11 = scores_23 = None
        attn_weights_34 = torch.nn.functional.dropout(
            attn_weights_33, p=0.1, training=False
        )
        attn_weights_33 = None
        attn_weights_35 = attn_weights_34.type(torch.float32)
        attn_weights_34 = None
        einsum_35 = torch.functional.einsum(
            "...hqk,...khd->...qhd", attn_weights_35, value_states_47
        )
        attn_weights_35 = value_states_47 = None
        contiguous_11 = einsum_35.contiguous()
        einsum_35 = None
        attn_output_33 = contiguous_11.view(1, -1, 768)
        contiguous_11 = None
        attn_output_34 = attn_output_33[
            (slice(None, None, None), slice(None, 12, None), slice(None, None, None))
        ]
        attn_output_33 = None
        attn_output_35 = torch._C._nn.linear(
            attn_output_34,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_34 = l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_0_modules_transient_global_self_attention_modules_o_parameters_weight_ = (None)
        dropout_46 = torch.nn.functional.dropout(attn_output_35, 0.1, False, False)
        attn_output_35 = None
        hidden_states_91 = hidden_states_88 + dropout_46
        hidden_states_88 = dropout_46 = None
        to_90 = hidden_states_91.to(torch.float32)
        pow_47 = to_90.pow(2)
        to_90 = None
        variance_35 = pow_47.mean(-1, keepdim=True)
        pow_47 = None
        add_112 = variance_35 + 1e-06
        variance_35 = None
        rsqrt_35 = torch.rsqrt(add_112)
        add_112 = None
        hidden_states_92 = hidden_states_91 * rsqrt_35
        rsqrt_35 = None
        forwarded_states_11 = (
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_92
        )
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_92
        ) = None
        linear_105 = torch._C._nn.linear(
            forwarded_states_11,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_144 = 0.5 * linear_105
        pow_48 = torch.pow(linear_105, 3.0)
        mul_145 = 0.044715 * pow_48
        pow_48 = None
        add_113 = linear_105 + mul_145
        linear_105 = mul_145 = None
        mul_146 = 0.7978845608028654 * add_113
        add_113 = None
        tanh_11 = torch.tanh(mul_146)
        mul_146 = None
        add_114 = 1.0 + tanh_11
        tanh_11 = None
        hidden_gelu_11 = mul_144 * add_114
        mul_144 = add_114 = None
        hidden_linear_11 = torch._C._nn.linear(
            forwarded_states_11,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_11 = l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_93 = hidden_gelu_11 * hidden_linear_11
        hidden_gelu_11 = hidden_linear_11 = None
        hidden_states_94 = torch.nn.functional.dropout(
            hidden_states_93, 0.1, False, False
        )
        hidden_states_93 = None
        hidden_states_95 = torch._C._nn.linear(
            hidden_states_94,
            l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_94 = l_self_modules_encoder_modules_block_modules_11_modules_layer_modules_1_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_48 = torch.nn.functional.dropout(hidden_states_95, 0.1, False, False)
        hidden_states_95 = None
        hidden_states_96 = hidden_states_91 + dropout_48
        hidden_states_91 = dropout_48 = None
        to_91 = hidden_states_96.to(torch.float32)
        pow_49 = to_91.pow(2)
        to_91 = None
        variance_36 = pow_49.mean(-1, keepdim=True)
        pow_49 = None
        add_116 = variance_36 + 1e-06
        variance_36 = None
        rsqrt_36 = torch.rsqrt(add_116)
        add_116 = None
        hidden_states_97 = hidden_states_96 * rsqrt_36
        hidden_states_96 = rsqrt_36 = None
        hidden_states_98 = (
            l_self_modules_encoder_modules_final_layer_norm_parameters_weight_
            * hidden_states_97
        )
        l_self_modules_encoder_modules_final_layer_norm_parameters_weight_ = (
            hidden_states_97
        ) = None
        hidden_states_99 = torch.nn.functional.dropout(
            hidden_states_98, 0.1, False, False
        )
        hidden_states_98 = None
        input_ids_1 = l_decoder_input_ids_.view(-1, 12)
        l_decoder_input_ids_ = None
        inputs_embeds_1 = torch.nn.functional.embedding(
            input_ids_1,
            l_self_modules_encoder_modules_embed_tokens_parameters_weight_,
            None,
            None,
            2.0,
            False,
            False,
        )
        input_ids_1 = (
            l_self_modules_encoder_modules_embed_tokens_parameters_weight_
        ) = None
        cache_position_1 = torch.arange(0, 12, device=device(type="cuda", index=0))
        causal_mask = torch.full(
            (12, 13),
            fill_value=-3.4028234663852886e38,
            dtype=torch.float32,
            device=device(type="cuda", index=0),
        )
        causal_mask_1 = torch.triu(causal_mask, diagonal=1)
        causal_mask = None
        arange_29 = torch.arange(13, device=device(type="cuda", index=0))
        reshape_48 = cache_position_1.reshape(-1, 1)
        gt_28 = arange_29 > reshape_48
        arange_29 = reshape_48 = None
        causal_mask_1 *= gt_28
        causal_mask_2 = causal_mask_1
        causal_mask_1 = gt_28 = None
        getitem_163 = causal_mask_2[
            (None, None, slice(None, None, None), slice(None, None, None))
        ]
        causal_mask_2 = None
        causal_mask_3 = getitem_163.expand(1, 1, -1, -1)
        getitem_163 = None
        encoder_extended_attention_mask = l_attention_mask_[
            (slice(None, None, None), None, None, slice(None, None, None))
        ]
        l_attention_mask_ = None
        encoder_extended_attention_mask_1 = encoder_extended_attention_mask.to(
            dtype=torch.float32
        )
        encoder_extended_attention_mask = None
        sub_79 = 1.0 - encoder_extended_attention_mask_1
        encoder_extended_attention_mask_1 = None
        encoder_extended_attention_mask_2 = sub_79 * -3.4028234663852886e38
        sub_79 = None
        hidden_states_100 = torch.nn.functional.dropout(
            inputs_embeds_1, 0.1, False, False
        )
        inputs_embeds_1 = None
        to_93 = hidden_states_100.to(torch.float32)
        pow_50 = to_93.pow(2)
        to_93 = None
        variance_37 = pow_50.mean(-1, keepdim=True)
        pow_50 = None
        add_117 = variance_37 + 1e-06
        variance_37 = None
        rsqrt_37 = torch.rsqrt(add_117)
        add_117 = None
        hidden_states_101 = hidden_states_100 * rsqrt_37
        rsqrt_37 = None
        normed_hidden_states_12 = (
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_101
        )
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_101
        ) = None
        query_states_24 = torch._C._nn.linear(
            normed_hidden_states_12,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_74 = query_states_24.view(1, -1, 12, 64)
        query_states_24 = None
        query_states_25 = view_74.transpose(1, 2)
        view_74 = None
        key_states_48 = torch._C._nn.linear(
            normed_hidden_states_12,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_48 = torch._C._nn.linear(
            normed_hidden_states_12,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_12 = l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_75 = key_states_48.view(1, -1, 12, 64)
        key_states_48 = None
        key_states_49 = view_75.transpose(1, 2)
        view_75 = None
        view_76 = value_states_48.view(1, -1, 12, 64)
        value_states_48 = None
        value_states_49 = view_76.transpose(1, 2)
        view_76 = None
        transpose_5 = key_states_49.transpose(3, 2)
        key_states_49 = None
        scores_24 = torch.matmul(query_states_25, transpose_5)
        query_states_25 = transpose_5 = None
        getitem_165 = cache_position_1[-1]
        real_seq_length = getitem_165 + 1
        getitem_165 = real_seq_length = None
        getitem_166 = cache_position_1[(slice(None, None, None), None)]
        context_position_1 = getitem_166.to(device(type="cuda", index=0))
        getitem_166 = None
        arange_30 = torch.arange(
            12, dtype=torch.int64, device=device(type="cuda", index=0)
        )
        memory_position_1 = arange_30[(None, slice(None, None, None))]
        arange_30 = None
        relative_position_3 = memory_position_1 - context_position_1
        memory_position_1 = context_position_1 = None
        zeros_like = torch.zeros_like(relative_position_3)
        min_3 = torch.min(relative_position_3, zeros_like)
        relative_position_3 = zeros_like = None
        relative_position_4 = -min_3
        min_3 = None
        is_small_2 = relative_position_4 < 16
        float_15 = relative_position_4.float()
        truediv_17 = float_15 / 16
        float_15 = None
        log_2 = torch.log(truediv_17)
        truediv_17 = None
        truediv_18 = log_2 / 2.0794415416798357
        log_2 = None
        mul_154 = truediv_18 * 16
        truediv_18 = None
        to_95 = mul_154.to(torch.int64)
        mul_154 = None
        relative_position_if_large_4 = 16 + to_95
        to_95 = None
        full_like_2 = torch.full_like(relative_position_if_large_4, 31)
        relative_position_if_large_5 = torch.min(
            relative_position_if_large_4, full_like_2
        )
        relative_position_if_large_4 = full_like_2 = None
        where_79 = torch.where(
            is_small_2, relative_position_4, relative_position_if_large_5
        )
        is_small_2 = relative_position_4 = relative_position_if_large_5 = None
        relative_buckets_4 = 0 + where_79
        where_79 = None
        values_2 = torch.nn.functional.embedding(
            relative_buckets_4,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_relative_attention_bias_parameters_weight_,
            None,
            None,
            2.0,
            False,
            False,
        )
        relative_buckets_4 = l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_relative_attention_bias_parameters_weight_ = (None)
        permute_2 = values_2.permute([2, 0, 1])
        values_2 = None
        values_3 = permute_2.unsqueeze(0)
        permute_2 = None
        position_bias_3 = values_3[
            (
                slice(None, None, None),
                slice(None, None, None),
                slice(-12, None, None),
                slice(None, None, None),
            )
        ]
        values_3 = None
        causal_mask_4 = causal_mask_3[
            (
                slice(None, None, None),
                slice(None, None, None),
                slice(None, None, None),
                slice(None, 12, None),
            )
        ]
        causal_mask_3 = None
        position_bias_4 = position_bias_3 + causal_mask_4
        position_bias_3 = causal_mask_4 = None
        scores_24 += position_bias_4
        scores_25 = scores_24
        scores_24 = None
        float_16 = scores_25.float()
        softmax_12 = torch.nn.functional.softmax(float_16, dim=-1)
        float_16 = None
        attn_weights_36 = softmax_12.type_as(scores_25)
        softmax_12 = scores_25 = None
        attn_weights_37 = torch.nn.functional.dropout(
            attn_weights_36, p=0.1, training=False
        )
        attn_weights_36 = None
        attn_output_36 = torch.matmul(attn_weights_37, value_states_49)
        attn_weights_37 = value_states_49 = None
        transpose_6 = attn_output_36.transpose(1, 2)
        attn_output_36 = None
        attn_output_37 = transpose_6.contiguous()
        transpose_6 = None
        attn_output_38 = attn_output_37.view(1, -1, 768)
        attn_output_37 = None
        attn_output_39 = torch._C._nn.linear(
            attn_output_38,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_38 = l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_52 = torch.nn.functional.dropout(attn_output_39, 0.1, False, False)
        attn_output_39 = None
        hidden_states_102 = hidden_states_100 + dropout_52
        hidden_states_100 = dropout_52 = None
        getitem_170 = cache_position_1[-1]
        real_seq_length_1 = getitem_170 + 1
        getitem_170 = real_seq_length_1 = None
        to_96 = hidden_states_102.to(torch.float32)
        pow_51 = to_96.pow(2)
        to_96 = None
        variance_38 = pow_51.mean(-1, keepdim=True)
        pow_51 = None
        add_124 = variance_38 + 1e-06
        variance_38 = None
        rsqrt_38 = torch.rsqrt(add_124)
        add_124 = None
        hidden_states_103 = hidden_states_102 * rsqrt_38
        rsqrt_38 = None
        normed_hidden_states_13 = (
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_103
        )
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_103
        ) = None
        query_states_26 = torch._C._nn.linear(
            normed_hidden_states_13,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_13 = l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_78 = query_states_26.view(1, -1, 12, 64)
        query_states_26 = None
        query_states_27 = view_78.transpose(1, 2)
        view_78 = None
        key_states_50 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_50 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_79 = key_states_50.view(1, -1, 12, 64)
        key_states_50 = None
        key_states_51 = view_79.transpose(1, 2)
        view_79 = None
        view_80 = value_states_50.view(1, -1, 12, 64)
        value_states_50 = None
        value_states_51 = view_80.transpose(1, 2)
        view_80 = None
        transpose_10 = key_states_51.transpose(3, 2)
        key_states_51 = None
        scores_26 = torch.matmul(query_states_27, transpose_10)
        query_states_27 = transpose_10 = None
        position_bias_5 = torch.zeros(
            (1, 12, 12, 12), device=device(type="cuda", index=0), dtype=torch.float32
        )
        causal_mask_5 = encoder_extended_attention_mask_2[
            (
                slice(None, None, None),
                slice(None, None, None),
                slice(None, None, None),
                slice(None, 12, None),
            )
        ]
        encoder_extended_attention_mask_2 = None
        position_bias_6 = position_bias_5 + causal_mask_5
        position_bias_5 = causal_mask_5 = None
        scores_26 += position_bias_6
        scores_27 = scores_26
        scores_26 = None
        float_17 = scores_27.float()
        softmax_13 = torch.nn.functional.softmax(float_17, dim=-1)
        float_17 = None
        attn_weights_38 = softmax_13.type_as(scores_27)
        softmax_13 = scores_27 = None
        attn_weights_39 = torch.nn.functional.dropout(
            attn_weights_38, p=0.1, training=False
        )
        attn_weights_38 = None
        attn_output_40 = torch.matmul(attn_weights_39, value_states_51)
        attn_weights_39 = value_states_51 = None
        transpose_11 = attn_output_40.transpose(1, 2)
        attn_output_40 = None
        attn_output_41 = transpose_11.contiguous()
        transpose_11 = None
        attn_output_42 = attn_output_41.view(1, -1, 768)
        attn_output_41 = None
        attn_output_43 = torch._C._nn.linear(
            attn_output_42,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_42 = l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_54 = torch.nn.functional.dropout(attn_output_43, 0.1, False, False)
        attn_output_43 = None
        layer_output = hidden_states_102 + dropout_54
        hidden_states_102 = dropout_54 = None
        to_97 = layer_output.to(torch.float32)
        pow_52 = to_97.pow(2)
        to_97 = None
        variance_39 = pow_52.mean(-1, keepdim=True)
        pow_52 = None
        add_127 = variance_39 + 1e-06
        variance_39 = None
        rsqrt_39 = torch.rsqrt(add_127)
        add_127 = None
        hidden_states_104 = layer_output * rsqrt_39
        rsqrt_39 = None
        forwarded_states_12 = (
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_104
        )
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_104
        ) = None
        linear_116 = torch._C._nn.linear(
            forwarded_states_12,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_159 = 0.5 * linear_116
        pow_53 = torch.pow(linear_116, 3.0)
        mul_160 = 0.044715 * pow_53
        pow_53 = None
        add_128 = linear_116 + mul_160
        linear_116 = mul_160 = None
        mul_161 = 0.7978845608028654 * add_128
        add_128 = None
        tanh_12 = torch.tanh(mul_161)
        mul_161 = None
        add_129 = 1.0 + tanh_12
        tanh_12 = None
        hidden_gelu_12 = mul_159 * add_129
        mul_159 = add_129 = None
        hidden_linear_12 = torch._C._nn.linear(
            forwarded_states_12,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_12 = l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_105 = hidden_gelu_12 * hidden_linear_12
        hidden_gelu_12 = hidden_linear_12 = None
        hidden_states_106 = torch.nn.functional.dropout(
            hidden_states_105, 0.1, False, False
        )
        hidden_states_105 = None
        hidden_states_107 = torch._C._nn.linear(
            hidden_states_106,
            l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_106 = l_self_modules_decoder_modules_block_modules_0_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_56 = torch.nn.functional.dropout(hidden_states_107, 0.1, False, False)
        hidden_states_107 = None
        hidden_states_108 = layer_output + dropout_56
        layer_output = dropout_56 = None
        to_98 = hidden_states_108.to(torch.float32)
        pow_54 = to_98.pow(2)
        to_98 = None
        variance_40 = pow_54.mean(-1, keepdim=True)
        pow_54 = None
        add_131 = variance_40 + 1e-06
        variance_40 = None
        rsqrt_40 = torch.rsqrt(add_131)
        add_131 = None
        hidden_states_109 = hidden_states_108 * rsqrt_40
        rsqrt_40 = None
        normed_hidden_states_14 = (
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_109
        )
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_109
        ) = None
        query_states_28 = torch._C._nn.linear(
            normed_hidden_states_14,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_82 = query_states_28.view(1, -1, 12, 64)
        query_states_28 = None
        query_states_29 = view_82.transpose(1, 2)
        view_82 = None
        key_states_52 = torch._C._nn.linear(
            normed_hidden_states_14,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_52 = torch._C._nn.linear(
            normed_hidden_states_14,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_14 = l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_83 = key_states_52.view(1, -1, 12, 64)
        key_states_52 = None
        key_states_53 = view_83.transpose(1, 2)
        view_83 = None
        view_84 = value_states_52.view(1, -1, 12, 64)
        value_states_52 = None
        value_states_53 = view_84.transpose(1, 2)
        view_84 = None
        transpose_15 = key_states_53.transpose(3, 2)
        key_states_53 = None
        scores_28 = torch.matmul(query_states_29, transpose_15)
        query_states_29 = transpose_15 = None
        scores_28 += position_bias_4
        scores_29 = scores_28
        scores_28 = None
        float_18 = scores_29.float()
        softmax_14 = torch.nn.functional.softmax(float_18, dim=-1)
        float_18 = None
        attn_weights_40 = softmax_14.type_as(scores_29)
        softmax_14 = scores_29 = None
        attn_weights_41 = torch.nn.functional.dropout(
            attn_weights_40, p=0.1, training=False
        )
        attn_weights_40 = None
        attn_output_44 = torch.matmul(attn_weights_41, value_states_53)
        attn_weights_41 = value_states_53 = None
        transpose_16 = attn_output_44.transpose(1, 2)
        attn_output_44 = None
        attn_output_45 = transpose_16.contiguous()
        transpose_16 = None
        attn_output_46 = attn_output_45.view(1, -1, 768)
        attn_output_45 = None
        attn_output_47 = torch._C._nn.linear(
            attn_output_46,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_46 = l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_58 = torch.nn.functional.dropout(attn_output_47, 0.1, False, False)
        attn_output_47 = None
        hidden_states_110 = hidden_states_108 + dropout_58
        hidden_states_108 = dropout_58 = None
        getitem_172 = cache_position_1[-1]
        add_133 = getitem_172 + 1
        getitem_172 = add_133 = None
        to_99 = hidden_states_110.to(torch.float32)
        pow_55 = to_99.pow(2)
        to_99 = None
        variance_41 = pow_55.mean(-1, keepdim=True)
        pow_55 = None
        add_134 = variance_41 + 1e-06
        variance_41 = None
        rsqrt_41 = torch.rsqrt(add_134)
        add_134 = None
        hidden_states_111 = hidden_states_110 * rsqrt_41
        rsqrt_41 = None
        normed_hidden_states_15 = (
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_111
        )
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_111
        ) = None
        query_states_30 = torch._C._nn.linear(
            normed_hidden_states_15,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_15 = l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_86 = query_states_30.view(1, -1, 12, 64)
        query_states_30 = None
        query_states_31 = view_86.transpose(1, 2)
        view_86 = None
        key_states_54 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_54 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_87 = key_states_54.view(1, -1, 12, 64)
        key_states_54 = None
        key_states_55 = view_87.transpose(1, 2)
        view_87 = None
        view_88 = value_states_54.view(1, -1, 12, 64)
        value_states_54 = None
        value_states_55 = view_88.transpose(1, 2)
        view_88 = None
        transpose_20 = key_states_55.transpose(3, 2)
        key_states_55 = None
        scores_30 = torch.matmul(query_states_31, transpose_20)
        query_states_31 = transpose_20 = None
        scores_30 += position_bias_6
        scores_31 = scores_30
        scores_30 = None
        float_19 = scores_31.float()
        softmax_15 = torch.nn.functional.softmax(float_19, dim=-1)
        float_19 = None
        attn_weights_42 = softmax_15.type_as(scores_31)
        softmax_15 = scores_31 = None
        attn_weights_43 = torch.nn.functional.dropout(
            attn_weights_42, p=0.1, training=False
        )
        attn_weights_42 = None
        attn_output_48 = torch.matmul(attn_weights_43, value_states_55)
        attn_weights_43 = value_states_55 = None
        transpose_21 = attn_output_48.transpose(1, 2)
        attn_output_48 = None
        attn_output_49 = transpose_21.contiguous()
        transpose_21 = None
        attn_output_50 = attn_output_49.view(1, -1, 768)
        attn_output_49 = None
        attn_output_51 = torch._C._nn.linear(
            attn_output_50,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_50 = l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_60 = torch.nn.functional.dropout(attn_output_51, 0.1, False, False)
        attn_output_51 = None
        layer_output_1 = hidden_states_110 + dropout_60
        hidden_states_110 = dropout_60 = None
        to_100 = layer_output_1.to(torch.float32)
        pow_56 = to_100.pow(2)
        to_100 = None
        variance_42 = pow_56.mean(-1, keepdim=True)
        pow_56 = None
        add_136 = variance_42 + 1e-06
        variance_42 = None
        rsqrt_42 = torch.rsqrt(add_136)
        add_136 = None
        hidden_states_112 = layer_output_1 * rsqrt_42
        rsqrt_42 = None
        forwarded_states_13 = (
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_112
        )
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_112
        ) = None
        linear_127 = torch._C._nn.linear(
            forwarded_states_13,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_170 = 0.5 * linear_127
        pow_57 = torch.pow(linear_127, 3.0)
        mul_171 = 0.044715 * pow_57
        pow_57 = None
        add_137 = linear_127 + mul_171
        linear_127 = mul_171 = None
        mul_172 = 0.7978845608028654 * add_137
        add_137 = None
        tanh_13 = torch.tanh(mul_172)
        mul_172 = None
        add_138 = 1.0 + tanh_13
        tanh_13 = None
        hidden_gelu_13 = mul_170 * add_138
        mul_170 = add_138 = None
        hidden_linear_13 = torch._C._nn.linear(
            forwarded_states_13,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_13 = l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_113 = hidden_gelu_13 * hidden_linear_13
        hidden_gelu_13 = hidden_linear_13 = None
        hidden_states_114 = torch.nn.functional.dropout(
            hidden_states_113, 0.1, False, False
        )
        hidden_states_113 = None
        hidden_states_115 = torch._C._nn.linear(
            hidden_states_114,
            l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_114 = l_self_modules_decoder_modules_block_modules_1_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_62 = torch.nn.functional.dropout(hidden_states_115, 0.1, False, False)
        hidden_states_115 = None
        hidden_states_116 = layer_output_1 + dropout_62
        layer_output_1 = dropout_62 = None
        to_101 = hidden_states_116.to(torch.float32)
        pow_58 = to_101.pow(2)
        to_101 = None
        variance_43 = pow_58.mean(-1, keepdim=True)
        pow_58 = None
        add_140 = variance_43 + 1e-06
        variance_43 = None
        rsqrt_43 = torch.rsqrt(add_140)
        add_140 = None
        hidden_states_117 = hidden_states_116 * rsqrt_43
        rsqrt_43 = None
        normed_hidden_states_16 = (
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_117
        )
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_117
        ) = None
        query_states_32 = torch._C._nn.linear(
            normed_hidden_states_16,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_90 = query_states_32.view(1, -1, 12, 64)
        query_states_32 = None
        query_states_33 = view_90.transpose(1, 2)
        view_90 = None
        key_states_56 = torch._C._nn.linear(
            normed_hidden_states_16,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_56 = torch._C._nn.linear(
            normed_hidden_states_16,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_16 = l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_91 = key_states_56.view(1, -1, 12, 64)
        key_states_56 = None
        key_states_57 = view_91.transpose(1, 2)
        view_91 = None
        view_92 = value_states_56.view(1, -1, 12, 64)
        value_states_56 = None
        value_states_57 = view_92.transpose(1, 2)
        view_92 = None
        transpose_25 = key_states_57.transpose(3, 2)
        key_states_57 = None
        scores_32 = torch.matmul(query_states_33, transpose_25)
        query_states_33 = transpose_25 = None
        scores_32 += position_bias_4
        scores_33 = scores_32
        scores_32 = None
        float_20 = scores_33.float()
        softmax_16 = torch.nn.functional.softmax(float_20, dim=-1)
        float_20 = None
        attn_weights_44 = softmax_16.type_as(scores_33)
        softmax_16 = scores_33 = None
        attn_weights_45 = torch.nn.functional.dropout(
            attn_weights_44, p=0.1, training=False
        )
        attn_weights_44 = None
        attn_output_52 = torch.matmul(attn_weights_45, value_states_57)
        attn_weights_45 = value_states_57 = None
        transpose_26 = attn_output_52.transpose(1, 2)
        attn_output_52 = None
        attn_output_53 = transpose_26.contiguous()
        transpose_26 = None
        attn_output_54 = attn_output_53.view(1, -1, 768)
        attn_output_53 = None
        attn_output_55 = torch._C._nn.linear(
            attn_output_54,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_54 = l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_64 = torch.nn.functional.dropout(attn_output_55, 0.1, False, False)
        attn_output_55 = None
        hidden_states_118 = hidden_states_116 + dropout_64
        hidden_states_116 = dropout_64 = None
        getitem_173 = cache_position_1[-1]
        add_142 = getitem_173 + 1
        getitem_173 = add_142 = None
        to_102 = hidden_states_118.to(torch.float32)
        pow_59 = to_102.pow(2)
        to_102 = None
        variance_44 = pow_59.mean(-1, keepdim=True)
        pow_59 = None
        add_143 = variance_44 + 1e-06
        variance_44 = None
        rsqrt_44 = torch.rsqrt(add_143)
        add_143 = None
        hidden_states_119 = hidden_states_118 * rsqrt_44
        rsqrt_44 = None
        normed_hidden_states_17 = (
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_119
        )
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_119
        ) = None
        query_states_34 = torch._C._nn.linear(
            normed_hidden_states_17,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_17 = l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_94 = query_states_34.view(1, -1, 12, 64)
        query_states_34 = None
        query_states_35 = view_94.transpose(1, 2)
        view_94 = None
        key_states_58 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_58 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_95 = key_states_58.view(1, -1, 12, 64)
        key_states_58 = None
        key_states_59 = view_95.transpose(1, 2)
        view_95 = None
        view_96 = value_states_58.view(1, -1, 12, 64)
        value_states_58 = None
        value_states_59 = view_96.transpose(1, 2)
        view_96 = None
        transpose_30 = key_states_59.transpose(3, 2)
        key_states_59 = None
        scores_34 = torch.matmul(query_states_35, transpose_30)
        query_states_35 = transpose_30 = None
        scores_34 += position_bias_6
        scores_35 = scores_34
        scores_34 = None
        float_21 = scores_35.float()
        softmax_17 = torch.nn.functional.softmax(float_21, dim=-1)
        float_21 = None
        attn_weights_46 = softmax_17.type_as(scores_35)
        softmax_17 = scores_35 = None
        attn_weights_47 = torch.nn.functional.dropout(
            attn_weights_46, p=0.1, training=False
        )
        attn_weights_46 = None
        attn_output_56 = torch.matmul(attn_weights_47, value_states_59)
        attn_weights_47 = value_states_59 = None
        transpose_31 = attn_output_56.transpose(1, 2)
        attn_output_56 = None
        attn_output_57 = transpose_31.contiguous()
        transpose_31 = None
        attn_output_58 = attn_output_57.view(1, -1, 768)
        attn_output_57 = None
        attn_output_59 = torch._C._nn.linear(
            attn_output_58,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_58 = l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_66 = torch.nn.functional.dropout(attn_output_59, 0.1, False, False)
        attn_output_59 = None
        layer_output_2 = hidden_states_118 + dropout_66
        hidden_states_118 = dropout_66 = None
        to_103 = layer_output_2.to(torch.float32)
        pow_60 = to_103.pow(2)
        to_103 = None
        variance_45 = pow_60.mean(-1, keepdim=True)
        pow_60 = None
        add_145 = variance_45 + 1e-06
        variance_45 = None
        rsqrt_45 = torch.rsqrt(add_145)
        add_145 = None
        hidden_states_120 = layer_output_2 * rsqrt_45
        rsqrt_45 = None
        forwarded_states_14 = (
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_120
        )
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_120
        ) = None
        linear_138 = torch._C._nn.linear(
            forwarded_states_14,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_181 = 0.5 * linear_138
        pow_61 = torch.pow(linear_138, 3.0)
        mul_182 = 0.044715 * pow_61
        pow_61 = None
        add_146 = linear_138 + mul_182
        linear_138 = mul_182 = None
        mul_183 = 0.7978845608028654 * add_146
        add_146 = None
        tanh_14 = torch.tanh(mul_183)
        mul_183 = None
        add_147 = 1.0 + tanh_14
        tanh_14 = None
        hidden_gelu_14 = mul_181 * add_147
        mul_181 = add_147 = None
        hidden_linear_14 = torch._C._nn.linear(
            forwarded_states_14,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_14 = l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_121 = hidden_gelu_14 * hidden_linear_14
        hidden_gelu_14 = hidden_linear_14 = None
        hidden_states_122 = torch.nn.functional.dropout(
            hidden_states_121, 0.1, False, False
        )
        hidden_states_121 = None
        hidden_states_123 = torch._C._nn.linear(
            hidden_states_122,
            l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_122 = l_self_modules_decoder_modules_block_modules_2_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_68 = torch.nn.functional.dropout(hidden_states_123, 0.1, False, False)
        hidden_states_123 = None
        hidden_states_124 = layer_output_2 + dropout_68
        layer_output_2 = dropout_68 = None
        to_104 = hidden_states_124.to(torch.float32)
        pow_62 = to_104.pow(2)
        to_104 = None
        variance_46 = pow_62.mean(-1, keepdim=True)
        pow_62 = None
        add_149 = variance_46 + 1e-06
        variance_46 = None
        rsqrt_46 = torch.rsqrt(add_149)
        add_149 = None
        hidden_states_125 = hidden_states_124 * rsqrt_46
        rsqrt_46 = None
        normed_hidden_states_18 = (
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_125
        )
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_125
        ) = None
        query_states_36 = torch._C._nn.linear(
            normed_hidden_states_18,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_98 = query_states_36.view(1, -1, 12, 64)
        query_states_36 = None
        query_states_37 = view_98.transpose(1, 2)
        view_98 = None
        key_states_60 = torch._C._nn.linear(
            normed_hidden_states_18,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_60 = torch._C._nn.linear(
            normed_hidden_states_18,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_18 = l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_99 = key_states_60.view(1, -1, 12, 64)
        key_states_60 = None
        key_states_61 = view_99.transpose(1, 2)
        view_99 = None
        view_100 = value_states_60.view(1, -1, 12, 64)
        value_states_60 = None
        value_states_61 = view_100.transpose(1, 2)
        view_100 = None
        transpose_35 = key_states_61.transpose(3, 2)
        key_states_61 = None
        scores_36 = torch.matmul(query_states_37, transpose_35)
        query_states_37 = transpose_35 = None
        scores_36 += position_bias_4
        scores_37 = scores_36
        scores_36 = None
        float_22 = scores_37.float()
        softmax_18 = torch.nn.functional.softmax(float_22, dim=-1)
        float_22 = None
        attn_weights_48 = softmax_18.type_as(scores_37)
        softmax_18 = scores_37 = None
        attn_weights_49 = torch.nn.functional.dropout(
            attn_weights_48, p=0.1, training=False
        )
        attn_weights_48 = None
        attn_output_60 = torch.matmul(attn_weights_49, value_states_61)
        attn_weights_49 = value_states_61 = None
        transpose_36 = attn_output_60.transpose(1, 2)
        attn_output_60 = None
        attn_output_61 = transpose_36.contiguous()
        transpose_36 = None
        attn_output_62 = attn_output_61.view(1, -1, 768)
        attn_output_61 = None
        attn_output_63 = torch._C._nn.linear(
            attn_output_62,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_62 = l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_70 = torch.nn.functional.dropout(attn_output_63, 0.1, False, False)
        attn_output_63 = None
        hidden_states_126 = hidden_states_124 + dropout_70
        hidden_states_124 = dropout_70 = None
        getitem_174 = cache_position_1[-1]
        add_151 = getitem_174 + 1
        getitem_174 = add_151 = None
        to_105 = hidden_states_126.to(torch.float32)
        pow_63 = to_105.pow(2)
        to_105 = None
        variance_47 = pow_63.mean(-1, keepdim=True)
        pow_63 = None
        add_152 = variance_47 + 1e-06
        variance_47 = None
        rsqrt_47 = torch.rsqrt(add_152)
        add_152 = None
        hidden_states_127 = hidden_states_126 * rsqrt_47
        rsqrt_47 = None
        normed_hidden_states_19 = (
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_127
        )
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_127
        ) = None
        query_states_38 = torch._C._nn.linear(
            normed_hidden_states_19,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_19 = l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_102 = query_states_38.view(1, -1, 12, 64)
        query_states_38 = None
        query_states_39 = view_102.transpose(1, 2)
        view_102 = None
        key_states_62 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_62 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_103 = key_states_62.view(1, -1, 12, 64)
        key_states_62 = None
        key_states_63 = view_103.transpose(1, 2)
        view_103 = None
        view_104 = value_states_62.view(1, -1, 12, 64)
        value_states_62 = None
        value_states_63 = view_104.transpose(1, 2)
        view_104 = None
        transpose_40 = key_states_63.transpose(3, 2)
        key_states_63 = None
        scores_38 = torch.matmul(query_states_39, transpose_40)
        query_states_39 = transpose_40 = None
        scores_38 += position_bias_6
        scores_39 = scores_38
        scores_38 = None
        float_23 = scores_39.float()
        softmax_19 = torch.nn.functional.softmax(float_23, dim=-1)
        float_23 = None
        attn_weights_50 = softmax_19.type_as(scores_39)
        softmax_19 = scores_39 = None
        attn_weights_51 = torch.nn.functional.dropout(
            attn_weights_50, p=0.1, training=False
        )
        attn_weights_50 = None
        attn_output_64 = torch.matmul(attn_weights_51, value_states_63)
        attn_weights_51 = value_states_63 = None
        transpose_41 = attn_output_64.transpose(1, 2)
        attn_output_64 = None
        attn_output_65 = transpose_41.contiguous()
        transpose_41 = None
        attn_output_66 = attn_output_65.view(1, -1, 768)
        attn_output_65 = None
        attn_output_67 = torch._C._nn.linear(
            attn_output_66,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_66 = l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_72 = torch.nn.functional.dropout(attn_output_67, 0.1, False, False)
        attn_output_67 = None
        layer_output_3 = hidden_states_126 + dropout_72
        hidden_states_126 = dropout_72 = None
        to_106 = layer_output_3.to(torch.float32)
        pow_64 = to_106.pow(2)
        to_106 = None
        variance_48 = pow_64.mean(-1, keepdim=True)
        pow_64 = None
        add_154 = variance_48 + 1e-06
        variance_48 = None
        rsqrt_48 = torch.rsqrt(add_154)
        add_154 = None
        hidden_states_128 = layer_output_3 * rsqrt_48
        rsqrt_48 = None
        forwarded_states_15 = (
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_128
        )
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_128
        ) = None
        linear_149 = torch._C._nn.linear(
            forwarded_states_15,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_192 = 0.5 * linear_149
        pow_65 = torch.pow(linear_149, 3.0)
        mul_193 = 0.044715 * pow_65
        pow_65 = None
        add_155 = linear_149 + mul_193
        linear_149 = mul_193 = None
        mul_194 = 0.7978845608028654 * add_155
        add_155 = None
        tanh_15 = torch.tanh(mul_194)
        mul_194 = None
        add_156 = 1.0 + tanh_15
        tanh_15 = None
        hidden_gelu_15 = mul_192 * add_156
        mul_192 = add_156 = None
        hidden_linear_15 = torch._C._nn.linear(
            forwarded_states_15,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_15 = l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_129 = hidden_gelu_15 * hidden_linear_15
        hidden_gelu_15 = hidden_linear_15 = None
        hidden_states_130 = torch.nn.functional.dropout(
            hidden_states_129, 0.1, False, False
        )
        hidden_states_129 = None
        hidden_states_131 = torch._C._nn.linear(
            hidden_states_130,
            l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_130 = l_self_modules_decoder_modules_block_modules_3_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_74 = torch.nn.functional.dropout(hidden_states_131, 0.1, False, False)
        hidden_states_131 = None
        hidden_states_132 = layer_output_3 + dropout_74
        layer_output_3 = dropout_74 = None
        to_107 = hidden_states_132.to(torch.float32)
        pow_66 = to_107.pow(2)
        to_107 = None
        variance_49 = pow_66.mean(-1, keepdim=True)
        pow_66 = None
        add_158 = variance_49 + 1e-06
        variance_49 = None
        rsqrt_49 = torch.rsqrt(add_158)
        add_158 = None
        hidden_states_133 = hidden_states_132 * rsqrt_49
        rsqrt_49 = None
        normed_hidden_states_20 = (
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_133
        )
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_133
        ) = None
        query_states_40 = torch._C._nn.linear(
            normed_hidden_states_20,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_106 = query_states_40.view(1, -1, 12, 64)
        query_states_40 = None
        query_states_41 = view_106.transpose(1, 2)
        view_106 = None
        key_states_64 = torch._C._nn.linear(
            normed_hidden_states_20,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_64 = torch._C._nn.linear(
            normed_hidden_states_20,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_20 = l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_107 = key_states_64.view(1, -1, 12, 64)
        key_states_64 = None
        key_states_65 = view_107.transpose(1, 2)
        view_107 = None
        view_108 = value_states_64.view(1, -1, 12, 64)
        value_states_64 = None
        value_states_65 = view_108.transpose(1, 2)
        view_108 = None
        transpose_45 = key_states_65.transpose(3, 2)
        key_states_65 = None
        scores_40 = torch.matmul(query_states_41, transpose_45)
        query_states_41 = transpose_45 = None
        scores_40 += position_bias_4
        scores_41 = scores_40
        scores_40 = None
        float_24 = scores_41.float()
        softmax_20 = torch.nn.functional.softmax(float_24, dim=-1)
        float_24 = None
        attn_weights_52 = softmax_20.type_as(scores_41)
        softmax_20 = scores_41 = None
        attn_weights_53 = torch.nn.functional.dropout(
            attn_weights_52, p=0.1, training=False
        )
        attn_weights_52 = None
        attn_output_68 = torch.matmul(attn_weights_53, value_states_65)
        attn_weights_53 = value_states_65 = None
        transpose_46 = attn_output_68.transpose(1, 2)
        attn_output_68 = None
        attn_output_69 = transpose_46.contiguous()
        transpose_46 = None
        attn_output_70 = attn_output_69.view(1, -1, 768)
        attn_output_69 = None
        attn_output_71 = torch._C._nn.linear(
            attn_output_70,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_70 = l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_76 = torch.nn.functional.dropout(attn_output_71, 0.1, False, False)
        attn_output_71 = None
        hidden_states_134 = hidden_states_132 + dropout_76
        hidden_states_132 = dropout_76 = None
        getitem_175 = cache_position_1[-1]
        add_160 = getitem_175 + 1
        getitem_175 = add_160 = None
        to_108 = hidden_states_134.to(torch.float32)
        pow_67 = to_108.pow(2)
        to_108 = None
        variance_50 = pow_67.mean(-1, keepdim=True)
        pow_67 = None
        add_161 = variance_50 + 1e-06
        variance_50 = None
        rsqrt_50 = torch.rsqrt(add_161)
        add_161 = None
        hidden_states_135 = hidden_states_134 * rsqrt_50
        rsqrt_50 = None
        normed_hidden_states_21 = (
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_135
        )
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_135
        ) = None
        query_states_42 = torch._C._nn.linear(
            normed_hidden_states_21,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_21 = l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_110 = query_states_42.view(1, -1, 12, 64)
        query_states_42 = None
        query_states_43 = view_110.transpose(1, 2)
        view_110 = None
        key_states_66 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_66 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_111 = key_states_66.view(1, -1, 12, 64)
        key_states_66 = None
        key_states_67 = view_111.transpose(1, 2)
        view_111 = None
        view_112 = value_states_66.view(1, -1, 12, 64)
        value_states_66 = None
        value_states_67 = view_112.transpose(1, 2)
        view_112 = None
        transpose_50 = key_states_67.transpose(3, 2)
        key_states_67 = None
        scores_42 = torch.matmul(query_states_43, transpose_50)
        query_states_43 = transpose_50 = None
        scores_42 += position_bias_6
        scores_43 = scores_42
        scores_42 = None
        float_25 = scores_43.float()
        softmax_21 = torch.nn.functional.softmax(float_25, dim=-1)
        float_25 = None
        attn_weights_54 = softmax_21.type_as(scores_43)
        softmax_21 = scores_43 = None
        attn_weights_55 = torch.nn.functional.dropout(
            attn_weights_54, p=0.1, training=False
        )
        attn_weights_54 = None
        attn_output_72 = torch.matmul(attn_weights_55, value_states_67)
        attn_weights_55 = value_states_67 = None
        transpose_51 = attn_output_72.transpose(1, 2)
        attn_output_72 = None
        attn_output_73 = transpose_51.contiguous()
        transpose_51 = None
        attn_output_74 = attn_output_73.view(1, -1, 768)
        attn_output_73 = None
        attn_output_75 = torch._C._nn.linear(
            attn_output_74,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_74 = l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_78 = torch.nn.functional.dropout(attn_output_75, 0.1, False, False)
        attn_output_75 = None
        layer_output_4 = hidden_states_134 + dropout_78
        hidden_states_134 = dropout_78 = None
        to_109 = layer_output_4.to(torch.float32)
        pow_68 = to_109.pow(2)
        to_109 = None
        variance_51 = pow_68.mean(-1, keepdim=True)
        pow_68 = None
        add_163 = variance_51 + 1e-06
        variance_51 = None
        rsqrt_51 = torch.rsqrt(add_163)
        add_163 = None
        hidden_states_136 = layer_output_4 * rsqrt_51
        rsqrt_51 = None
        forwarded_states_16 = (
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_136
        )
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_136
        ) = None
        linear_160 = torch._C._nn.linear(
            forwarded_states_16,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_203 = 0.5 * linear_160
        pow_69 = torch.pow(linear_160, 3.0)
        mul_204 = 0.044715 * pow_69
        pow_69 = None
        add_164 = linear_160 + mul_204
        linear_160 = mul_204 = None
        mul_205 = 0.7978845608028654 * add_164
        add_164 = None
        tanh_16 = torch.tanh(mul_205)
        mul_205 = None
        add_165 = 1.0 + tanh_16
        tanh_16 = None
        hidden_gelu_16 = mul_203 * add_165
        mul_203 = add_165 = None
        hidden_linear_16 = torch._C._nn.linear(
            forwarded_states_16,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_16 = l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_137 = hidden_gelu_16 * hidden_linear_16
        hidden_gelu_16 = hidden_linear_16 = None
        hidden_states_138 = torch.nn.functional.dropout(
            hidden_states_137, 0.1, False, False
        )
        hidden_states_137 = None
        hidden_states_139 = torch._C._nn.linear(
            hidden_states_138,
            l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_138 = l_self_modules_decoder_modules_block_modules_4_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_80 = torch.nn.functional.dropout(hidden_states_139, 0.1, False, False)
        hidden_states_139 = None
        hidden_states_140 = layer_output_4 + dropout_80
        layer_output_4 = dropout_80 = None
        to_110 = hidden_states_140.to(torch.float32)
        pow_70 = to_110.pow(2)
        to_110 = None
        variance_52 = pow_70.mean(-1, keepdim=True)
        pow_70 = None
        add_167 = variance_52 + 1e-06
        variance_52 = None
        rsqrt_52 = torch.rsqrt(add_167)
        add_167 = None
        hidden_states_141 = hidden_states_140 * rsqrt_52
        rsqrt_52 = None
        normed_hidden_states_22 = (
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_141
        )
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_141
        ) = None
        query_states_44 = torch._C._nn.linear(
            normed_hidden_states_22,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_114 = query_states_44.view(1, -1, 12, 64)
        query_states_44 = None
        query_states_45 = view_114.transpose(1, 2)
        view_114 = None
        key_states_68 = torch._C._nn.linear(
            normed_hidden_states_22,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_68 = torch._C._nn.linear(
            normed_hidden_states_22,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_22 = l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_115 = key_states_68.view(1, -1, 12, 64)
        key_states_68 = None
        key_states_69 = view_115.transpose(1, 2)
        view_115 = None
        view_116 = value_states_68.view(1, -1, 12, 64)
        value_states_68 = None
        value_states_69 = view_116.transpose(1, 2)
        view_116 = None
        transpose_55 = key_states_69.transpose(3, 2)
        key_states_69 = None
        scores_44 = torch.matmul(query_states_45, transpose_55)
        query_states_45 = transpose_55 = None
        scores_44 += position_bias_4
        scores_45 = scores_44
        scores_44 = None
        float_26 = scores_45.float()
        softmax_22 = torch.nn.functional.softmax(float_26, dim=-1)
        float_26 = None
        attn_weights_56 = softmax_22.type_as(scores_45)
        softmax_22 = scores_45 = None
        attn_weights_57 = torch.nn.functional.dropout(
            attn_weights_56, p=0.1, training=False
        )
        attn_weights_56 = None
        attn_output_76 = torch.matmul(attn_weights_57, value_states_69)
        attn_weights_57 = value_states_69 = None
        transpose_56 = attn_output_76.transpose(1, 2)
        attn_output_76 = None
        attn_output_77 = transpose_56.contiguous()
        transpose_56 = None
        attn_output_78 = attn_output_77.view(1, -1, 768)
        attn_output_77 = None
        attn_output_79 = torch._C._nn.linear(
            attn_output_78,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_78 = l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_82 = torch.nn.functional.dropout(attn_output_79, 0.1, False, False)
        attn_output_79 = None
        hidden_states_142 = hidden_states_140 + dropout_82
        hidden_states_140 = dropout_82 = None
        getitem_176 = cache_position_1[-1]
        add_169 = getitem_176 + 1
        getitem_176 = add_169 = None
        to_111 = hidden_states_142.to(torch.float32)
        pow_71 = to_111.pow(2)
        to_111 = None
        variance_53 = pow_71.mean(-1, keepdim=True)
        pow_71 = None
        add_170 = variance_53 + 1e-06
        variance_53 = None
        rsqrt_53 = torch.rsqrt(add_170)
        add_170 = None
        hidden_states_143 = hidden_states_142 * rsqrt_53
        rsqrt_53 = None
        normed_hidden_states_23 = (
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_143
        )
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_143
        ) = None
        query_states_46 = torch._C._nn.linear(
            normed_hidden_states_23,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_23 = l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_118 = query_states_46.view(1, -1, 12, 64)
        query_states_46 = None
        query_states_47 = view_118.transpose(1, 2)
        view_118 = None
        key_states_70 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_70 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_119 = key_states_70.view(1, -1, 12, 64)
        key_states_70 = None
        key_states_71 = view_119.transpose(1, 2)
        view_119 = None
        view_120 = value_states_70.view(1, -1, 12, 64)
        value_states_70 = None
        value_states_71 = view_120.transpose(1, 2)
        view_120 = None
        transpose_60 = key_states_71.transpose(3, 2)
        key_states_71 = None
        scores_46 = torch.matmul(query_states_47, transpose_60)
        query_states_47 = transpose_60 = None
        scores_46 += position_bias_6
        scores_47 = scores_46
        scores_46 = None
        float_27 = scores_47.float()
        softmax_23 = torch.nn.functional.softmax(float_27, dim=-1)
        float_27 = None
        attn_weights_58 = softmax_23.type_as(scores_47)
        softmax_23 = scores_47 = None
        attn_weights_59 = torch.nn.functional.dropout(
            attn_weights_58, p=0.1, training=False
        )
        attn_weights_58 = None
        attn_output_80 = torch.matmul(attn_weights_59, value_states_71)
        attn_weights_59 = value_states_71 = None
        transpose_61 = attn_output_80.transpose(1, 2)
        attn_output_80 = None
        attn_output_81 = transpose_61.contiguous()
        transpose_61 = None
        attn_output_82 = attn_output_81.view(1, -1, 768)
        attn_output_81 = None
        attn_output_83 = torch._C._nn.linear(
            attn_output_82,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_82 = l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_84 = torch.nn.functional.dropout(attn_output_83, 0.1, False, False)
        attn_output_83 = None
        layer_output_5 = hidden_states_142 + dropout_84
        hidden_states_142 = dropout_84 = None
        to_112 = layer_output_5.to(torch.float32)
        pow_72 = to_112.pow(2)
        to_112 = None
        variance_54 = pow_72.mean(-1, keepdim=True)
        pow_72 = None
        add_172 = variance_54 + 1e-06
        variance_54 = None
        rsqrt_54 = torch.rsqrt(add_172)
        add_172 = None
        hidden_states_144 = layer_output_5 * rsqrt_54
        rsqrt_54 = None
        forwarded_states_17 = (
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_144
        )
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_144
        ) = None
        linear_171 = torch._C._nn.linear(
            forwarded_states_17,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_214 = 0.5 * linear_171
        pow_73 = torch.pow(linear_171, 3.0)
        mul_215 = 0.044715 * pow_73
        pow_73 = None
        add_173 = linear_171 + mul_215
        linear_171 = mul_215 = None
        mul_216 = 0.7978845608028654 * add_173
        add_173 = None
        tanh_17 = torch.tanh(mul_216)
        mul_216 = None
        add_174 = 1.0 + tanh_17
        tanh_17 = None
        hidden_gelu_17 = mul_214 * add_174
        mul_214 = add_174 = None
        hidden_linear_17 = torch._C._nn.linear(
            forwarded_states_17,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_17 = l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_145 = hidden_gelu_17 * hidden_linear_17
        hidden_gelu_17 = hidden_linear_17 = None
        hidden_states_146 = torch.nn.functional.dropout(
            hidden_states_145, 0.1, False, False
        )
        hidden_states_145 = None
        hidden_states_147 = torch._C._nn.linear(
            hidden_states_146,
            l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_146 = l_self_modules_decoder_modules_block_modules_5_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_86 = torch.nn.functional.dropout(hidden_states_147, 0.1, False, False)
        hidden_states_147 = None
        hidden_states_148 = layer_output_5 + dropout_86
        layer_output_5 = dropout_86 = None
        to_113 = hidden_states_148.to(torch.float32)
        pow_74 = to_113.pow(2)
        to_113 = None
        variance_55 = pow_74.mean(-1, keepdim=True)
        pow_74 = None
        add_176 = variance_55 + 1e-06
        variance_55 = None
        rsqrt_55 = torch.rsqrt(add_176)
        add_176 = None
        hidden_states_149 = hidden_states_148 * rsqrt_55
        rsqrt_55 = None
        normed_hidden_states_24 = (
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_149
        )
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_149
        ) = None
        query_states_48 = torch._C._nn.linear(
            normed_hidden_states_24,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_122 = query_states_48.view(1, -1, 12, 64)
        query_states_48 = None
        query_states_49 = view_122.transpose(1, 2)
        view_122 = None
        key_states_72 = torch._C._nn.linear(
            normed_hidden_states_24,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_72 = torch._C._nn.linear(
            normed_hidden_states_24,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_24 = l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_123 = key_states_72.view(1, -1, 12, 64)
        key_states_72 = None
        key_states_73 = view_123.transpose(1, 2)
        view_123 = None
        view_124 = value_states_72.view(1, -1, 12, 64)
        value_states_72 = None
        value_states_73 = view_124.transpose(1, 2)
        view_124 = None
        transpose_65 = key_states_73.transpose(3, 2)
        key_states_73 = None
        scores_48 = torch.matmul(query_states_49, transpose_65)
        query_states_49 = transpose_65 = None
        scores_48 += position_bias_4
        scores_49 = scores_48
        scores_48 = None
        float_28 = scores_49.float()
        softmax_24 = torch.nn.functional.softmax(float_28, dim=-1)
        float_28 = None
        attn_weights_60 = softmax_24.type_as(scores_49)
        softmax_24 = scores_49 = None
        attn_weights_61 = torch.nn.functional.dropout(
            attn_weights_60, p=0.1, training=False
        )
        attn_weights_60 = None
        attn_output_84 = torch.matmul(attn_weights_61, value_states_73)
        attn_weights_61 = value_states_73 = None
        transpose_66 = attn_output_84.transpose(1, 2)
        attn_output_84 = None
        attn_output_85 = transpose_66.contiguous()
        transpose_66 = None
        attn_output_86 = attn_output_85.view(1, -1, 768)
        attn_output_85 = None
        attn_output_87 = torch._C._nn.linear(
            attn_output_86,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_86 = l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_88 = torch.nn.functional.dropout(attn_output_87, 0.1, False, False)
        attn_output_87 = None
        hidden_states_150 = hidden_states_148 + dropout_88
        hidden_states_148 = dropout_88 = None
        getitem_177 = cache_position_1[-1]
        add_178 = getitem_177 + 1
        getitem_177 = add_178 = None
        to_114 = hidden_states_150.to(torch.float32)
        pow_75 = to_114.pow(2)
        to_114 = None
        variance_56 = pow_75.mean(-1, keepdim=True)
        pow_75 = None
        add_179 = variance_56 + 1e-06
        variance_56 = None
        rsqrt_56 = torch.rsqrt(add_179)
        add_179 = None
        hidden_states_151 = hidden_states_150 * rsqrt_56
        rsqrt_56 = None
        normed_hidden_states_25 = (
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_151
        )
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_151
        ) = None
        query_states_50 = torch._C._nn.linear(
            normed_hidden_states_25,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_25 = l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_126 = query_states_50.view(1, -1, 12, 64)
        query_states_50 = None
        query_states_51 = view_126.transpose(1, 2)
        view_126 = None
        key_states_74 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_74 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_127 = key_states_74.view(1, -1, 12, 64)
        key_states_74 = None
        key_states_75 = view_127.transpose(1, 2)
        view_127 = None
        view_128 = value_states_74.view(1, -1, 12, 64)
        value_states_74 = None
        value_states_75 = view_128.transpose(1, 2)
        view_128 = None
        transpose_70 = key_states_75.transpose(3, 2)
        key_states_75 = None
        scores_50 = torch.matmul(query_states_51, transpose_70)
        query_states_51 = transpose_70 = None
        scores_50 += position_bias_6
        scores_51 = scores_50
        scores_50 = None
        float_29 = scores_51.float()
        softmax_25 = torch.nn.functional.softmax(float_29, dim=-1)
        float_29 = None
        attn_weights_62 = softmax_25.type_as(scores_51)
        softmax_25 = scores_51 = None
        attn_weights_63 = torch.nn.functional.dropout(
            attn_weights_62, p=0.1, training=False
        )
        attn_weights_62 = None
        attn_output_88 = torch.matmul(attn_weights_63, value_states_75)
        attn_weights_63 = value_states_75 = None
        transpose_71 = attn_output_88.transpose(1, 2)
        attn_output_88 = None
        attn_output_89 = transpose_71.contiguous()
        transpose_71 = None
        attn_output_90 = attn_output_89.view(1, -1, 768)
        attn_output_89 = None
        attn_output_91 = torch._C._nn.linear(
            attn_output_90,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_90 = l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_90 = torch.nn.functional.dropout(attn_output_91, 0.1, False, False)
        attn_output_91 = None
        layer_output_6 = hidden_states_150 + dropout_90
        hidden_states_150 = dropout_90 = None
        to_115 = layer_output_6.to(torch.float32)
        pow_76 = to_115.pow(2)
        to_115 = None
        variance_57 = pow_76.mean(-1, keepdim=True)
        pow_76 = None
        add_181 = variance_57 + 1e-06
        variance_57 = None
        rsqrt_57 = torch.rsqrt(add_181)
        add_181 = None
        hidden_states_152 = layer_output_6 * rsqrt_57
        rsqrt_57 = None
        forwarded_states_18 = (
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_152
        )
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_152
        ) = None
        linear_182 = torch._C._nn.linear(
            forwarded_states_18,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_225 = 0.5 * linear_182
        pow_77 = torch.pow(linear_182, 3.0)
        mul_226 = 0.044715 * pow_77
        pow_77 = None
        add_182 = linear_182 + mul_226
        linear_182 = mul_226 = None
        mul_227 = 0.7978845608028654 * add_182
        add_182 = None
        tanh_18 = torch.tanh(mul_227)
        mul_227 = None
        add_183 = 1.0 + tanh_18
        tanh_18 = None
        hidden_gelu_18 = mul_225 * add_183
        mul_225 = add_183 = None
        hidden_linear_18 = torch._C._nn.linear(
            forwarded_states_18,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_18 = l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_153 = hidden_gelu_18 * hidden_linear_18
        hidden_gelu_18 = hidden_linear_18 = None
        hidden_states_154 = torch.nn.functional.dropout(
            hidden_states_153, 0.1, False, False
        )
        hidden_states_153 = None
        hidden_states_155 = torch._C._nn.linear(
            hidden_states_154,
            l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_154 = l_self_modules_decoder_modules_block_modules_6_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_92 = torch.nn.functional.dropout(hidden_states_155, 0.1, False, False)
        hidden_states_155 = None
        hidden_states_156 = layer_output_6 + dropout_92
        layer_output_6 = dropout_92 = None
        to_116 = hidden_states_156.to(torch.float32)
        pow_78 = to_116.pow(2)
        to_116 = None
        variance_58 = pow_78.mean(-1, keepdim=True)
        pow_78 = None
        add_185 = variance_58 + 1e-06
        variance_58 = None
        rsqrt_58 = torch.rsqrt(add_185)
        add_185 = None
        hidden_states_157 = hidden_states_156 * rsqrt_58
        rsqrt_58 = None
        normed_hidden_states_26 = (
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_157
        )
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_157
        ) = None
        query_states_52 = torch._C._nn.linear(
            normed_hidden_states_26,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_130 = query_states_52.view(1, -1, 12, 64)
        query_states_52 = None
        query_states_53 = view_130.transpose(1, 2)
        view_130 = None
        key_states_76 = torch._C._nn.linear(
            normed_hidden_states_26,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_76 = torch._C._nn.linear(
            normed_hidden_states_26,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_26 = l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_131 = key_states_76.view(1, -1, 12, 64)
        key_states_76 = None
        key_states_77 = view_131.transpose(1, 2)
        view_131 = None
        view_132 = value_states_76.view(1, -1, 12, 64)
        value_states_76 = None
        value_states_77 = view_132.transpose(1, 2)
        view_132 = None
        transpose_75 = key_states_77.transpose(3, 2)
        key_states_77 = None
        scores_52 = torch.matmul(query_states_53, transpose_75)
        query_states_53 = transpose_75 = None
        scores_52 += position_bias_4
        scores_53 = scores_52
        scores_52 = None
        float_30 = scores_53.float()
        softmax_26 = torch.nn.functional.softmax(float_30, dim=-1)
        float_30 = None
        attn_weights_64 = softmax_26.type_as(scores_53)
        softmax_26 = scores_53 = None
        attn_weights_65 = torch.nn.functional.dropout(
            attn_weights_64, p=0.1, training=False
        )
        attn_weights_64 = None
        attn_output_92 = torch.matmul(attn_weights_65, value_states_77)
        attn_weights_65 = value_states_77 = None
        transpose_76 = attn_output_92.transpose(1, 2)
        attn_output_92 = None
        attn_output_93 = transpose_76.contiguous()
        transpose_76 = None
        attn_output_94 = attn_output_93.view(1, -1, 768)
        attn_output_93 = None
        attn_output_95 = torch._C._nn.linear(
            attn_output_94,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_94 = l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_94 = torch.nn.functional.dropout(attn_output_95, 0.1, False, False)
        attn_output_95 = None
        hidden_states_158 = hidden_states_156 + dropout_94
        hidden_states_156 = dropout_94 = None
        getitem_178 = cache_position_1[-1]
        add_187 = getitem_178 + 1
        getitem_178 = add_187 = None
        to_117 = hidden_states_158.to(torch.float32)
        pow_79 = to_117.pow(2)
        to_117 = None
        variance_59 = pow_79.mean(-1, keepdim=True)
        pow_79 = None
        add_188 = variance_59 + 1e-06
        variance_59 = None
        rsqrt_59 = torch.rsqrt(add_188)
        add_188 = None
        hidden_states_159 = hidden_states_158 * rsqrt_59
        rsqrt_59 = None
        normed_hidden_states_27 = (
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_159
        )
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_159
        ) = None
        query_states_54 = torch._C._nn.linear(
            normed_hidden_states_27,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_27 = l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_134 = query_states_54.view(1, -1, 12, 64)
        query_states_54 = None
        query_states_55 = view_134.transpose(1, 2)
        view_134 = None
        key_states_78 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_78 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_135 = key_states_78.view(1, -1, 12, 64)
        key_states_78 = None
        key_states_79 = view_135.transpose(1, 2)
        view_135 = None
        view_136 = value_states_78.view(1, -1, 12, 64)
        value_states_78 = None
        value_states_79 = view_136.transpose(1, 2)
        view_136 = None
        transpose_80 = key_states_79.transpose(3, 2)
        key_states_79 = None
        scores_54 = torch.matmul(query_states_55, transpose_80)
        query_states_55 = transpose_80 = None
        scores_54 += position_bias_6
        scores_55 = scores_54
        scores_54 = None
        float_31 = scores_55.float()
        softmax_27 = torch.nn.functional.softmax(float_31, dim=-1)
        float_31 = None
        attn_weights_66 = softmax_27.type_as(scores_55)
        softmax_27 = scores_55 = None
        attn_weights_67 = torch.nn.functional.dropout(
            attn_weights_66, p=0.1, training=False
        )
        attn_weights_66 = None
        attn_output_96 = torch.matmul(attn_weights_67, value_states_79)
        attn_weights_67 = value_states_79 = None
        transpose_81 = attn_output_96.transpose(1, 2)
        attn_output_96 = None
        attn_output_97 = transpose_81.contiguous()
        transpose_81 = None
        attn_output_98 = attn_output_97.view(1, -1, 768)
        attn_output_97 = None
        attn_output_99 = torch._C._nn.linear(
            attn_output_98,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_98 = l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_96 = torch.nn.functional.dropout(attn_output_99, 0.1, False, False)
        attn_output_99 = None
        layer_output_7 = hidden_states_158 + dropout_96
        hidden_states_158 = dropout_96 = None
        to_118 = layer_output_7.to(torch.float32)
        pow_80 = to_118.pow(2)
        to_118 = None
        variance_60 = pow_80.mean(-1, keepdim=True)
        pow_80 = None
        add_190 = variance_60 + 1e-06
        variance_60 = None
        rsqrt_60 = torch.rsqrt(add_190)
        add_190 = None
        hidden_states_160 = layer_output_7 * rsqrt_60
        rsqrt_60 = None
        forwarded_states_19 = (
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_160
        )
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_160
        ) = None
        linear_193 = torch._C._nn.linear(
            forwarded_states_19,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_236 = 0.5 * linear_193
        pow_81 = torch.pow(linear_193, 3.0)
        mul_237 = 0.044715 * pow_81
        pow_81 = None
        add_191 = linear_193 + mul_237
        linear_193 = mul_237 = None
        mul_238 = 0.7978845608028654 * add_191
        add_191 = None
        tanh_19 = torch.tanh(mul_238)
        mul_238 = None
        add_192 = 1.0 + tanh_19
        tanh_19 = None
        hidden_gelu_19 = mul_236 * add_192
        mul_236 = add_192 = None
        hidden_linear_19 = torch._C._nn.linear(
            forwarded_states_19,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_19 = l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_161 = hidden_gelu_19 * hidden_linear_19
        hidden_gelu_19 = hidden_linear_19 = None
        hidden_states_162 = torch.nn.functional.dropout(
            hidden_states_161, 0.1, False, False
        )
        hidden_states_161 = None
        hidden_states_163 = torch._C._nn.linear(
            hidden_states_162,
            l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_162 = l_self_modules_decoder_modules_block_modules_7_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_98 = torch.nn.functional.dropout(hidden_states_163, 0.1, False, False)
        hidden_states_163 = None
        hidden_states_164 = layer_output_7 + dropout_98
        layer_output_7 = dropout_98 = None
        to_119 = hidden_states_164.to(torch.float32)
        pow_82 = to_119.pow(2)
        to_119 = None
        variance_61 = pow_82.mean(-1, keepdim=True)
        pow_82 = None
        add_194 = variance_61 + 1e-06
        variance_61 = None
        rsqrt_61 = torch.rsqrt(add_194)
        add_194 = None
        hidden_states_165 = hidden_states_164 * rsqrt_61
        rsqrt_61 = None
        normed_hidden_states_28 = (
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_165
        )
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_165
        ) = None
        query_states_56 = torch._C._nn.linear(
            normed_hidden_states_28,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_138 = query_states_56.view(1, -1, 12, 64)
        query_states_56 = None
        query_states_57 = view_138.transpose(1, 2)
        view_138 = None
        key_states_80 = torch._C._nn.linear(
            normed_hidden_states_28,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_80 = torch._C._nn.linear(
            normed_hidden_states_28,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_28 = l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_139 = key_states_80.view(1, -1, 12, 64)
        key_states_80 = None
        key_states_81 = view_139.transpose(1, 2)
        view_139 = None
        view_140 = value_states_80.view(1, -1, 12, 64)
        value_states_80 = None
        value_states_81 = view_140.transpose(1, 2)
        view_140 = None
        transpose_85 = key_states_81.transpose(3, 2)
        key_states_81 = None
        scores_56 = torch.matmul(query_states_57, transpose_85)
        query_states_57 = transpose_85 = None
        scores_56 += position_bias_4
        scores_57 = scores_56
        scores_56 = None
        float_32 = scores_57.float()
        softmax_28 = torch.nn.functional.softmax(float_32, dim=-1)
        float_32 = None
        attn_weights_68 = softmax_28.type_as(scores_57)
        softmax_28 = scores_57 = None
        attn_weights_69 = torch.nn.functional.dropout(
            attn_weights_68, p=0.1, training=False
        )
        attn_weights_68 = None
        attn_output_100 = torch.matmul(attn_weights_69, value_states_81)
        attn_weights_69 = value_states_81 = None
        transpose_86 = attn_output_100.transpose(1, 2)
        attn_output_100 = None
        attn_output_101 = transpose_86.contiguous()
        transpose_86 = None
        attn_output_102 = attn_output_101.view(1, -1, 768)
        attn_output_101 = None
        attn_output_103 = torch._C._nn.linear(
            attn_output_102,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_102 = l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_100 = torch.nn.functional.dropout(attn_output_103, 0.1, False, False)
        attn_output_103 = None
        hidden_states_166 = hidden_states_164 + dropout_100
        hidden_states_164 = dropout_100 = None
        getitem_179 = cache_position_1[-1]
        add_196 = getitem_179 + 1
        getitem_179 = add_196 = None
        to_120 = hidden_states_166.to(torch.float32)
        pow_83 = to_120.pow(2)
        to_120 = None
        variance_62 = pow_83.mean(-1, keepdim=True)
        pow_83 = None
        add_197 = variance_62 + 1e-06
        variance_62 = None
        rsqrt_62 = torch.rsqrt(add_197)
        add_197 = None
        hidden_states_167 = hidden_states_166 * rsqrt_62
        rsqrt_62 = None
        normed_hidden_states_29 = (
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_167
        )
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_167
        ) = None
        query_states_58 = torch._C._nn.linear(
            normed_hidden_states_29,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_29 = l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_142 = query_states_58.view(1, -1, 12, 64)
        query_states_58 = None
        query_states_59 = view_142.transpose(1, 2)
        view_142 = None
        key_states_82 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_82 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_143 = key_states_82.view(1, -1, 12, 64)
        key_states_82 = None
        key_states_83 = view_143.transpose(1, 2)
        view_143 = None
        view_144 = value_states_82.view(1, -1, 12, 64)
        value_states_82 = None
        value_states_83 = view_144.transpose(1, 2)
        view_144 = None
        transpose_90 = key_states_83.transpose(3, 2)
        key_states_83 = None
        scores_58 = torch.matmul(query_states_59, transpose_90)
        query_states_59 = transpose_90 = None
        scores_58 += position_bias_6
        scores_59 = scores_58
        scores_58 = None
        float_33 = scores_59.float()
        softmax_29 = torch.nn.functional.softmax(float_33, dim=-1)
        float_33 = None
        attn_weights_70 = softmax_29.type_as(scores_59)
        softmax_29 = scores_59 = None
        attn_weights_71 = torch.nn.functional.dropout(
            attn_weights_70, p=0.1, training=False
        )
        attn_weights_70 = None
        attn_output_104 = torch.matmul(attn_weights_71, value_states_83)
        attn_weights_71 = value_states_83 = None
        transpose_91 = attn_output_104.transpose(1, 2)
        attn_output_104 = None
        attn_output_105 = transpose_91.contiguous()
        transpose_91 = None
        attn_output_106 = attn_output_105.view(1, -1, 768)
        attn_output_105 = None
        attn_output_107 = torch._C._nn.linear(
            attn_output_106,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_106 = l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_102 = torch.nn.functional.dropout(attn_output_107, 0.1, False, False)
        attn_output_107 = None
        layer_output_8 = hidden_states_166 + dropout_102
        hidden_states_166 = dropout_102 = None
        to_121 = layer_output_8.to(torch.float32)
        pow_84 = to_121.pow(2)
        to_121 = None
        variance_63 = pow_84.mean(-1, keepdim=True)
        pow_84 = None
        add_199 = variance_63 + 1e-06
        variance_63 = None
        rsqrt_63 = torch.rsqrt(add_199)
        add_199 = None
        hidden_states_168 = layer_output_8 * rsqrt_63
        rsqrt_63 = None
        forwarded_states_20 = (
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_168
        )
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_168
        ) = None
        linear_204 = torch._C._nn.linear(
            forwarded_states_20,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_247 = 0.5 * linear_204
        pow_85 = torch.pow(linear_204, 3.0)
        mul_248 = 0.044715 * pow_85
        pow_85 = None
        add_200 = linear_204 + mul_248
        linear_204 = mul_248 = None
        mul_249 = 0.7978845608028654 * add_200
        add_200 = None
        tanh_20 = torch.tanh(mul_249)
        mul_249 = None
        add_201 = 1.0 + tanh_20
        tanh_20 = None
        hidden_gelu_20 = mul_247 * add_201
        mul_247 = add_201 = None
        hidden_linear_20 = torch._C._nn.linear(
            forwarded_states_20,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_20 = l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_169 = hidden_gelu_20 * hidden_linear_20
        hidden_gelu_20 = hidden_linear_20 = None
        hidden_states_170 = torch.nn.functional.dropout(
            hidden_states_169, 0.1, False, False
        )
        hidden_states_169 = None
        hidden_states_171 = torch._C._nn.linear(
            hidden_states_170,
            l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_170 = l_self_modules_decoder_modules_block_modules_8_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_104 = torch.nn.functional.dropout(hidden_states_171, 0.1, False, False)
        hidden_states_171 = None
        hidden_states_172 = layer_output_8 + dropout_104
        layer_output_8 = dropout_104 = None
        to_122 = hidden_states_172.to(torch.float32)
        pow_86 = to_122.pow(2)
        to_122 = None
        variance_64 = pow_86.mean(-1, keepdim=True)
        pow_86 = None
        add_203 = variance_64 + 1e-06
        variance_64 = None
        rsqrt_64 = torch.rsqrt(add_203)
        add_203 = None
        hidden_states_173 = hidden_states_172 * rsqrt_64
        rsqrt_64 = None
        normed_hidden_states_30 = (
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_173
        )
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_173
        ) = None
        query_states_60 = torch._C._nn.linear(
            normed_hidden_states_30,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_146 = query_states_60.view(1, -1, 12, 64)
        query_states_60 = None
        query_states_61 = view_146.transpose(1, 2)
        view_146 = None
        key_states_84 = torch._C._nn.linear(
            normed_hidden_states_30,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_84 = torch._C._nn.linear(
            normed_hidden_states_30,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_30 = l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_147 = key_states_84.view(1, -1, 12, 64)
        key_states_84 = None
        key_states_85 = view_147.transpose(1, 2)
        view_147 = None
        view_148 = value_states_84.view(1, -1, 12, 64)
        value_states_84 = None
        value_states_85 = view_148.transpose(1, 2)
        view_148 = None
        transpose_95 = key_states_85.transpose(3, 2)
        key_states_85 = None
        scores_60 = torch.matmul(query_states_61, transpose_95)
        query_states_61 = transpose_95 = None
        scores_60 += position_bias_4
        scores_61 = scores_60
        scores_60 = None
        float_34 = scores_61.float()
        softmax_30 = torch.nn.functional.softmax(float_34, dim=-1)
        float_34 = None
        attn_weights_72 = softmax_30.type_as(scores_61)
        softmax_30 = scores_61 = None
        attn_weights_73 = torch.nn.functional.dropout(
            attn_weights_72, p=0.1, training=False
        )
        attn_weights_72 = None
        attn_output_108 = torch.matmul(attn_weights_73, value_states_85)
        attn_weights_73 = value_states_85 = None
        transpose_96 = attn_output_108.transpose(1, 2)
        attn_output_108 = None
        attn_output_109 = transpose_96.contiguous()
        transpose_96 = None
        attn_output_110 = attn_output_109.view(1, -1, 768)
        attn_output_109 = None
        attn_output_111 = torch._C._nn.linear(
            attn_output_110,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_110 = l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_106 = torch.nn.functional.dropout(attn_output_111, 0.1, False, False)
        attn_output_111 = None
        hidden_states_174 = hidden_states_172 + dropout_106
        hidden_states_172 = dropout_106 = None
        getitem_180 = cache_position_1[-1]
        add_205 = getitem_180 + 1
        getitem_180 = add_205 = None
        to_123 = hidden_states_174.to(torch.float32)
        pow_87 = to_123.pow(2)
        to_123 = None
        variance_65 = pow_87.mean(-1, keepdim=True)
        pow_87 = None
        add_206 = variance_65 + 1e-06
        variance_65 = None
        rsqrt_65 = torch.rsqrt(add_206)
        add_206 = None
        hidden_states_175 = hidden_states_174 * rsqrt_65
        rsqrt_65 = None
        normed_hidden_states_31 = (
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_175
        )
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_175
        ) = None
        query_states_62 = torch._C._nn.linear(
            normed_hidden_states_31,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_31 = l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_150 = query_states_62.view(1, -1, 12, 64)
        query_states_62 = None
        query_states_63 = view_150.transpose(1, 2)
        view_150 = None
        key_states_86 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_86 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_151 = key_states_86.view(1, -1, 12, 64)
        key_states_86 = None
        key_states_87 = view_151.transpose(1, 2)
        view_151 = None
        view_152 = value_states_86.view(1, -1, 12, 64)
        value_states_86 = None
        value_states_87 = view_152.transpose(1, 2)
        view_152 = None
        transpose_100 = key_states_87.transpose(3, 2)
        key_states_87 = None
        scores_62 = torch.matmul(query_states_63, transpose_100)
        query_states_63 = transpose_100 = None
        scores_62 += position_bias_6
        scores_63 = scores_62
        scores_62 = None
        float_35 = scores_63.float()
        softmax_31 = torch.nn.functional.softmax(float_35, dim=-1)
        float_35 = None
        attn_weights_74 = softmax_31.type_as(scores_63)
        softmax_31 = scores_63 = None
        attn_weights_75 = torch.nn.functional.dropout(
            attn_weights_74, p=0.1, training=False
        )
        attn_weights_74 = None
        attn_output_112 = torch.matmul(attn_weights_75, value_states_87)
        attn_weights_75 = value_states_87 = None
        transpose_101 = attn_output_112.transpose(1, 2)
        attn_output_112 = None
        attn_output_113 = transpose_101.contiguous()
        transpose_101 = None
        attn_output_114 = attn_output_113.view(1, -1, 768)
        attn_output_113 = None
        attn_output_115 = torch._C._nn.linear(
            attn_output_114,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_114 = l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_108 = torch.nn.functional.dropout(attn_output_115, 0.1, False, False)
        attn_output_115 = None
        layer_output_9 = hidden_states_174 + dropout_108
        hidden_states_174 = dropout_108 = None
        to_124 = layer_output_9.to(torch.float32)
        pow_88 = to_124.pow(2)
        to_124 = None
        variance_66 = pow_88.mean(-1, keepdim=True)
        pow_88 = None
        add_208 = variance_66 + 1e-06
        variance_66 = None
        rsqrt_66 = torch.rsqrt(add_208)
        add_208 = None
        hidden_states_176 = layer_output_9 * rsqrt_66
        rsqrt_66 = None
        forwarded_states_21 = (
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_176
        )
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_176
        ) = None
        linear_215 = torch._C._nn.linear(
            forwarded_states_21,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_258 = 0.5 * linear_215
        pow_89 = torch.pow(linear_215, 3.0)
        mul_259 = 0.044715 * pow_89
        pow_89 = None
        add_209 = linear_215 + mul_259
        linear_215 = mul_259 = None
        mul_260 = 0.7978845608028654 * add_209
        add_209 = None
        tanh_21 = torch.tanh(mul_260)
        mul_260 = None
        add_210 = 1.0 + tanh_21
        tanh_21 = None
        hidden_gelu_21 = mul_258 * add_210
        mul_258 = add_210 = None
        hidden_linear_21 = torch._C._nn.linear(
            forwarded_states_21,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_21 = l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_177 = hidden_gelu_21 * hidden_linear_21
        hidden_gelu_21 = hidden_linear_21 = None
        hidden_states_178 = torch.nn.functional.dropout(
            hidden_states_177, 0.1, False, False
        )
        hidden_states_177 = None
        hidden_states_179 = torch._C._nn.linear(
            hidden_states_178,
            l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_178 = l_self_modules_decoder_modules_block_modules_9_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_110 = torch.nn.functional.dropout(hidden_states_179, 0.1, False, False)
        hidden_states_179 = None
        hidden_states_180 = layer_output_9 + dropout_110
        layer_output_9 = dropout_110 = None
        to_125 = hidden_states_180.to(torch.float32)
        pow_90 = to_125.pow(2)
        to_125 = None
        variance_67 = pow_90.mean(-1, keepdim=True)
        pow_90 = None
        add_212 = variance_67 + 1e-06
        variance_67 = None
        rsqrt_67 = torch.rsqrt(add_212)
        add_212 = None
        hidden_states_181 = hidden_states_180 * rsqrt_67
        rsqrt_67 = None
        normed_hidden_states_32 = (
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_181
        )
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_181
        ) = None
        query_states_64 = torch._C._nn.linear(
            normed_hidden_states_32,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_154 = query_states_64.view(1, -1, 12, 64)
        query_states_64 = None
        query_states_65 = view_154.transpose(1, 2)
        view_154 = None
        key_states_88 = torch._C._nn.linear(
            normed_hidden_states_32,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_88 = torch._C._nn.linear(
            normed_hidden_states_32,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_32 = l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_155 = key_states_88.view(1, -1, 12, 64)
        key_states_88 = None
        key_states_89 = view_155.transpose(1, 2)
        view_155 = None
        view_156 = value_states_88.view(1, -1, 12, 64)
        value_states_88 = None
        value_states_89 = view_156.transpose(1, 2)
        view_156 = None
        transpose_105 = key_states_89.transpose(3, 2)
        key_states_89 = None
        scores_64 = torch.matmul(query_states_65, transpose_105)
        query_states_65 = transpose_105 = None
        scores_64 += position_bias_4
        scores_65 = scores_64
        scores_64 = None
        float_36 = scores_65.float()
        softmax_32 = torch.nn.functional.softmax(float_36, dim=-1)
        float_36 = None
        attn_weights_76 = softmax_32.type_as(scores_65)
        softmax_32 = scores_65 = None
        attn_weights_77 = torch.nn.functional.dropout(
            attn_weights_76, p=0.1, training=False
        )
        attn_weights_76 = None
        attn_output_116 = torch.matmul(attn_weights_77, value_states_89)
        attn_weights_77 = value_states_89 = None
        transpose_106 = attn_output_116.transpose(1, 2)
        attn_output_116 = None
        attn_output_117 = transpose_106.contiguous()
        transpose_106 = None
        attn_output_118 = attn_output_117.view(1, -1, 768)
        attn_output_117 = None
        attn_output_119 = torch._C._nn.linear(
            attn_output_118,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_118 = l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_112 = torch.nn.functional.dropout(attn_output_119, 0.1, False, False)
        attn_output_119 = None
        hidden_states_182 = hidden_states_180 + dropout_112
        hidden_states_180 = dropout_112 = None
        getitem_181 = cache_position_1[-1]
        add_214 = getitem_181 + 1
        getitem_181 = add_214 = None
        to_126 = hidden_states_182.to(torch.float32)
        pow_91 = to_126.pow(2)
        to_126 = None
        variance_68 = pow_91.mean(-1, keepdim=True)
        pow_91 = None
        add_215 = variance_68 + 1e-06
        variance_68 = None
        rsqrt_68 = torch.rsqrt(add_215)
        add_215 = None
        hidden_states_183 = hidden_states_182 * rsqrt_68
        rsqrt_68 = None
        normed_hidden_states_33 = (
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_183
        )
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_183
        ) = None
        query_states_66 = torch._C._nn.linear(
            normed_hidden_states_33,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_33 = l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_158 = query_states_66.view(1, -1, 12, 64)
        query_states_66 = None
        query_states_67 = view_158.transpose(1, 2)
        view_158 = None
        key_states_90 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_90 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_159 = key_states_90.view(1, -1, 12, 64)
        key_states_90 = None
        key_states_91 = view_159.transpose(1, 2)
        view_159 = None
        view_160 = value_states_90.view(1, -1, 12, 64)
        value_states_90 = None
        value_states_91 = view_160.transpose(1, 2)
        view_160 = None
        transpose_110 = key_states_91.transpose(3, 2)
        key_states_91 = None
        scores_66 = torch.matmul(query_states_67, transpose_110)
        query_states_67 = transpose_110 = None
        scores_66 += position_bias_6
        scores_67 = scores_66
        scores_66 = None
        float_37 = scores_67.float()
        softmax_33 = torch.nn.functional.softmax(float_37, dim=-1)
        float_37 = None
        attn_weights_78 = softmax_33.type_as(scores_67)
        softmax_33 = scores_67 = None
        attn_weights_79 = torch.nn.functional.dropout(
            attn_weights_78, p=0.1, training=False
        )
        attn_weights_78 = None
        attn_output_120 = torch.matmul(attn_weights_79, value_states_91)
        attn_weights_79 = value_states_91 = None
        transpose_111 = attn_output_120.transpose(1, 2)
        attn_output_120 = None
        attn_output_121 = transpose_111.contiguous()
        transpose_111 = None
        attn_output_122 = attn_output_121.view(1, -1, 768)
        attn_output_121 = None
        attn_output_123 = torch._C._nn.linear(
            attn_output_122,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_122 = l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_114 = torch.nn.functional.dropout(attn_output_123, 0.1, False, False)
        attn_output_123 = None
        layer_output_10 = hidden_states_182 + dropout_114
        hidden_states_182 = dropout_114 = None
        to_127 = layer_output_10.to(torch.float32)
        pow_92 = to_127.pow(2)
        to_127 = None
        variance_69 = pow_92.mean(-1, keepdim=True)
        pow_92 = None
        add_217 = variance_69 + 1e-06
        variance_69 = None
        rsqrt_69 = torch.rsqrt(add_217)
        add_217 = None
        hidden_states_184 = layer_output_10 * rsqrt_69
        rsqrt_69 = None
        forwarded_states_22 = (
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_184
        )
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_184
        ) = None
        linear_226 = torch._C._nn.linear(
            forwarded_states_22,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_269 = 0.5 * linear_226
        pow_93 = torch.pow(linear_226, 3.0)
        mul_270 = 0.044715 * pow_93
        pow_93 = None
        add_218 = linear_226 + mul_270
        linear_226 = mul_270 = None
        mul_271 = 0.7978845608028654 * add_218
        add_218 = None
        tanh_22 = torch.tanh(mul_271)
        mul_271 = None
        add_219 = 1.0 + tanh_22
        tanh_22 = None
        hidden_gelu_22 = mul_269 * add_219
        mul_269 = add_219 = None
        hidden_linear_22 = torch._C._nn.linear(
            forwarded_states_22,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_22 = l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_185 = hidden_gelu_22 * hidden_linear_22
        hidden_gelu_22 = hidden_linear_22 = None
        hidden_states_186 = torch.nn.functional.dropout(
            hidden_states_185, 0.1, False, False
        )
        hidden_states_185 = None
        hidden_states_187 = torch._C._nn.linear(
            hidden_states_186,
            l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_186 = l_self_modules_decoder_modules_block_modules_10_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_116 = torch.nn.functional.dropout(hidden_states_187, 0.1, False, False)
        hidden_states_187 = None
        hidden_states_188 = layer_output_10 + dropout_116
        layer_output_10 = dropout_116 = None
        to_128 = hidden_states_188.to(torch.float32)
        pow_94 = to_128.pow(2)
        to_128 = None
        variance_70 = pow_94.mean(-1, keepdim=True)
        pow_94 = None
        add_221 = variance_70 + 1e-06
        variance_70 = None
        rsqrt_70 = torch.rsqrt(add_221)
        add_221 = None
        hidden_states_189 = hidden_states_188 * rsqrt_70
        rsqrt_70 = None
        normed_hidden_states_34 = (
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_
            * hidden_states_189
        )
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_layer_norm_parameters_weight_ = (
            hidden_states_189
        ) = None
        query_states_68 = torch._C._nn.linear(
            normed_hidden_states_34,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_q_parameters_weight_ = (
            None
        )
        view_162 = query_states_68.view(1, -1, 12, 64)
        query_states_68 = None
        query_states_69 = view_162.transpose(1, 2)
        view_162 = None
        key_states_92 = torch._C._nn.linear(
            normed_hidden_states_34,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_92 = torch._C._nn.linear(
            normed_hidden_states_34,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_,
            None,
        )
        normed_hidden_states_34 = l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_v_parameters_weight_ = (None)
        view_163 = key_states_92.view(1, -1, 12, 64)
        key_states_92 = None
        key_states_93 = view_163.transpose(1, 2)
        view_163 = None
        view_164 = value_states_92.view(1, -1, 12, 64)
        value_states_92 = None
        value_states_93 = view_164.transpose(1, 2)
        view_164 = None
        transpose_115 = key_states_93.transpose(3, 2)
        key_states_93 = None
        scores_68 = torch.matmul(query_states_69, transpose_115)
        query_states_69 = transpose_115 = None
        scores_68 += position_bias_4
        scores_69 = scores_68
        scores_68 = position_bias_4 = None
        float_38 = scores_69.float()
        softmax_34 = torch.nn.functional.softmax(float_38, dim=-1)
        float_38 = None
        attn_weights_80 = softmax_34.type_as(scores_69)
        softmax_34 = scores_69 = None
        attn_weights_81 = torch.nn.functional.dropout(
            attn_weights_80, p=0.1, training=False
        )
        attn_weights_80 = None
        attn_output_124 = torch.matmul(attn_weights_81, value_states_93)
        attn_weights_81 = value_states_93 = None
        transpose_116 = attn_output_124.transpose(1, 2)
        attn_output_124 = None
        attn_output_125 = transpose_116.contiguous()
        transpose_116 = None
        attn_output_126 = attn_output_125.view(1, -1, 768)
        attn_output_125 = None
        attn_output_127 = torch._C._nn.linear(
            attn_output_126,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_126 = l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_0_modules_self_attention_modules_o_parameters_weight_ = (None)
        dropout_118 = torch.nn.functional.dropout(attn_output_127, 0.1, False, False)
        attn_output_127 = None
        hidden_states_190 = hidden_states_188 + dropout_118
        hidden_states_188 = dropout_118 = None
        getitem_182 = cache_position_1[-1]
        cache_position_1 = None
        add_223 = getitem_182 + 1
        getitem_182 = add_223 = None
        to_129 = hidden_states_190.to(torch.float32)
        pow_95 = to_129.pow(2)
        to_129 = None
        variance_71 = pow_95.mean(-1, keepdim=True)
        pow_95 = None
        add_224 = variance_71 + 1e-06
        variance_71 = None
        rsqrt_71 = torch.rsqrt(add_224)
        add_224 = None
        hidden_states_191 = hidden_states_190 * rsqrt_71
        rsqrt_71 = None
        normed_hidden_states_35 = (
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_
            * hidden_states_191
        )
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_layer_norm_parameters_weight_ = (
            hidden_states_191
        ) = None
        query_states_70 = torch._C._nn.linear(
            normed_hidden_states_35,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_,
            None,
        )
        normed_hidden_states_35 = l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_q_parameters_weight_ = (None)
        view_166 = query_states_70.view(1, -1, 12, 64)
        query_states_70 = None
        query_states_71 = view_166.transpose(1, 2)
        view_166 = None
        key_states_94 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_k_parameters_weight_ = (
            None
        )
        value_states_94 = torch._C._nn.linear(
            hidden_states_99,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_v_parameters_weight_ = (
            None
        )
        view_167 = key_states_94.view(1, -1, 12, 64)
        key_states_94 = None
        key_states_95 = view_167.transpose(1, 2)
        view_167 = None
        view_168 = value_states_94.view(1, -1, 12, 64)
        value_states_94 = None
        value_states_95 = view_168.transpose(1, 2)
        view_168 = None
        transpose_120 = key_states_95.transpose(3, 2)
        key_states_95 = None
        scores_70 = torch.matmul(query_states_71, transpose_120)
        query_states_71 = transpose_120 = None
        scores_70 += position_bias_6
        scores_71 = scores_70
        scores_70 = position_bias_6 = None
        float_39 = scores_71.float()
        softmax_35 = torch.nn.functional.softmax(float_39, dim=-1)
        float_39 = None
        attn_weights_82 = softmax_35.type_as(scores_71)
        softmax_35 = scores_71 = None
        attn_weights_83 = torch.nn.functional.dropout(
            attn_weights_82, p=0.1, training=False
        )
        attn_weights_82 = None
        attn_output_128 = torch.matmul(attn_weights_83, value_states_95)
        attn_weights_83 = value_states_95 = None
        transpose_121 = attn_output_128.transpose(1, 2)
        attn_output_128 = None
        attn_output_129 = transpose_121.contiguous()
        transpose_121 = None
        attn_output_130 = attn_output_129.view(1, -1, 768)
        attn_output_129 = None
        attn_output_131 = torch._C._nn.linear(
            attn_output_130,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_,
            None,
        )
        attn_output_130 = l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_1_modules_enc_dec_attention_modules_o_parameters_weight_ = (None)
        dropout_120 = torch.nn.functional.dropout(attn_output_131, 0.1, False, False)
        attn_output_131 = None
        layer_output_11 = hidden_states_190 + dropout_120
        hidden_states_190 = dropout_120 = None
        to_130 = layer_output_11.to(torch.float32)
        pow_96 = to_130.pow(2)
        to_130 = None
        variance_72 = pow_96.mean(-1, keepdim=True)
        pow_96 = None
        add_226 = variance_72 + 1e-06
        variance_72 = None
        rsqrt_72 = torch.rsqrt(add_226)
        add_226 = None
        hidden_states_192 = layer_output_11 * rsqrt_72
        rsqrt_72 = None
        forwarded_states_23 = (
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_layer_norm_parameters_weight_
            * hidden_states_192
        )
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_layer_norm_parameters_weight_ = (
            hidden_states_192
        ) = None
        linear_237 = torch._C._nn.linear(
            forwarded_states_23,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_,
            None,
        )
        l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_0_parameters_weight_ = (
            None
        )
        mul_280 = 0.5 * linear_237
        pow_97 = torch.pow(linear_237, 3.0)
        mul_281 = 0.044715 * pow_97
        pow_97 = None
        add_227 = linear_237 + mul_281
        linear_237 = mul_281 = None
        mul_282 = 0.7978845608028654 * add_227
        add_227 = None
        tanh_23 = torch.tanh(mul_282)
        mul_282 = None
        add_228 = 1.0 + tanh_23
        tanh_23 = None
        hidden_gelu_23 = mul_280 * add_228
        mul_280 = add_228 = None
        hidden_linear_23 = torch._C._nn.linear(
            forwarded_states_23,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_,
            None,
        )
        forwarded_states_23 = l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wi_1_parameters_weight_ = (None)
        hidden_states_193 = hidden_gelu_23 * hidden_linear_23
        hidden_gelu_23 = hidden_linear_23 = None
        hidden_states_194 = torch.nn.functional.dropout(
            hidden_states_193, 0.1, False, False
        )
        hidden_states_193 = None
        hidden_states_195 = torch._C._nn.linear(
            hidden_states_194,
            l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_,
            None,
        )
        hidden_states_194 = l_self_modules_decoder_modules_block_modules_11_modules_layer_modules_2_modules_dense_relu_dense_modules_wo_parameters_weight_ = (None)
        dropout_122 = torch.nn.functional.dropout(hidden_states_195, 0.1, False, False)
        hidden_states_195 = None
        hidden_states_196 = layer_output_11 + dropout_122
        layer_output_11 = dropout_122 = None
        to_131 = hidden_states_196.to(torch.float32)
        pow_98 = to_131.pow(2)
        to_131 = None
        variance_73 = pow_98.mean(-1, keepdim=True)
        pow_98 = None
        add_230 = variance_73 + 1e-06
        variance_73 = None
        rsqrt_73 = torch.rsqrt(add_230)
        add_230 = None
        hidden_states_197 = hidden_states_196 * rsqrt_73
        hidden_states_196 = rsqrt_73 = None
        hidden_states_198 = (
            l_self_modules_decoder_modules_final_layer_norm_parameters_weight_
            * hidden_states_197
        )
        l_self_modules_decoder_modules_final_layer_norm_parameters_weight_ = (
            hidden_states_197
        ) = None
        hidden_states_199 = torch.nn.functional.dropout(
            hidden_states_198, 0.1, False, False
        )
        hidden_states_198 = None
        return (hidden_states_199, hidden_states_99)
