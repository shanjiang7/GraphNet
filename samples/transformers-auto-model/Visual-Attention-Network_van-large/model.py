import torch


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_pixel_values_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_
        l_pixel_values_ = L_pixel_values_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_
        hidden_state = torch.conv2d(
            l_pixel_values_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_,
            (4, 4),
            (3, 3),
            (1, 1),
            1,
        )
        l_pixel_values_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_1 = torch.nn.functional.batch_norm(
            hidden_state,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_2 = torch.nn.functional.batch_norm(
            hidden_state_1,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_1 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_2 = torch._C._nn.gelu(input_1)
        input_1 = None
        hidden_state_3 = torch.conv2d(
            input_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_4 = torch.conv2d(
            hidden_state_3,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            64,
        )
        hidden_state_3 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_5 = torch.conv2d(
            hidden_state_4,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_4 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended = input_2 * hidden_state_5
        input_2 = hidden_state_5 = None
        hidden_state_6 = torch.conv2d(
            attended,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_7 = hidden_state_6 + hidden_state_2
        hidden_state_6 = hidden_state_2 = None
        unsqueeze = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_1 = unsqueeze.unsqueeze(-1)
        unsqueeze = None
        hidden_state_8 = unsqueeze_1 * hidden_state_7
        unsqueeze_1 = hidden_state_7 = None
        hidden_state_9 = hidden_state_1 + hidden_state_8
        hidden_state_1 = hidden_state_8 = None
        hidden_state_10 = torch.nn.functional.batch_norm(
            hidden_state_9,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_11 = torch.conv2d(
            hidden_state_10,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_10 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_12 = torch.conv2d(
            hidden_state_11,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            512,
        )
        hidden_state_11 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_13 = torch._C._nn.gelu(hidden_state_12)
        hidden_state_12 = None
        hidden_state_14 = torch.nn.functional.dropout(
            hidden_state_13, 0.0, False, False
        )
        hidden_state_13 = None
        hidden_state_15 = torch.conv2d(
            hidden_state_14,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_14 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_16 = torch.nn.functional.dropout(
            hidden_state_15, 0.0, False, False
        )
        hidden_state_15 = None
        unsqueeze_2 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_3 = unsqueeze_2.unsqueeze(-1)
        unsqueeze_2 = None
        hidden_state_17 = unsqueeze_3 * hidden_state_16
        unsqueeze_3 = hidden_state_16 = None
        hidden_state_18 = hidden_state_9 + hidden_state_17
        hidden_state_9 = hidden_state_17 = None
        hidden_state_19 = torch.nn.functional.batch_norm(
            hidden_state_18,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = (None)
        input_3 = torch.conv2d(
            hidden_state_19,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_4 = torch._C._nn.gelu(input_3)
        input_3 = None
        hidden_state_20 = torch.conv2d(
            input_4,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_21 = torch.conv2d(
            hidden_state_20,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            64,
        )
        hidden_state_20 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_22 = torch.conv2d(
            hidden_state_21,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_21 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_1 = input_4 * hidden_state_22
        input_4 = hidden_state_22 = None
        hidden_state_23 = torch.conv2d(
            attended_1,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_1 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_24 = hidden_state_23 + hidden_state_19
        hidden_state_23 = hidden_state_19 = None
        unsqueeze_4 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_5 = unsqueeze_4.unsqueeze(-1)
        unsqueeze_4 = None
        hidden_state_25 = unsqueeze_5 * hidden_state_24
        unsqueeze_5 = hidden_state_24 = None
        hidden_state_26 = hidden_state_18 + hidden_state_25
        hidden_state_18 = hidden_state_25 = None
        hidden_state_27 = torch.nn.functional.batch_norm(
            hidden_state_26,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_28 = torch.conv2d(
            hidden_state_27,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_27 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_29 = torch.conv2d(
            hidden_state_28,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            512,
        )
        hidden_state_28 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_30 = torch._C._nn.gelu(hidden_state_29)
        hidden_state_29 = None
        hidden_state_31 = torch.nn.functional.dropout(
            hidden_state_30, 0.0, False, False
        )
        hidden_state_30 = None
        hidden_state_32 = torch.conv2d(
            hidden_state_31,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_31 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_33 = torch.nn.functional.dropout(
            hidden_state_32, 0.0, False, False
        )
        hidden_state_32 = None
        unsqueeze_6 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_7 = unsqueeze_6.unsqueeze(-1)
        unsqueeze_6 = None
        hidden_state_34 = unsqueeze_7 * hidden_state_33
        unsqueeze_7 = hidden_state_33 = None
        hidden_state_35 = hidden_state_26 + hidden_state_34
        hidden_state_26 = hidden_state_34 = None
        hidden_state_36 = torch.nn.functional.batch_norm(
            hidden_state_35,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = (None)
        input_5 = torch.conv2d(
            hidden_state_36,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_6 = torch._C._nn.gelu(input_5)
        input_5 = None
        hidden_state_37 = torch.conv2d(
            input_6,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_38 = torch.conv2d(
            hidden_state_37,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            64,
        )
        hidden_state_37 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_39 = torch.conv2d(
            hidden_state_38,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_38 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_2 = input_6 * hidden_state_39
        input_6 = hidden_state_39 = None
        hidden_state_40 = torch.conv2d(
            attended_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_2 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_41 = hidden_state_40 + hidden_state_36
        hidden_state_40 = hidden_state_36 = None
        unsqueeze_8 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_9 = unsqueeze_8.unsqueeze(-1)
        unsqueeze_8 = None
        hidden_state_42 = unsqueeze_9 * hidden_state_41
        unsqueeze_9 = hidden_state_41 = None
        hidden_state_43 = hidden_state_35 + hidden_state_42
        hidden_state_35 = hidden_state_42 = None
        hidden_state_44 = torch.nn.functional.batch_norm(
            hidden_state_43,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_45 = torch.conv2d(
            hidden_state_44,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_44 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_46 = torch.conv2d(
            hidden_state_45,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            512,
        )
        hidden_state_45 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_47 = torch._C._nn.gelu(hidden_state_46)
        hidden_state_46 = None
        hidden_state_48 = torch.nn.functional.dropout(
            hidden_state_47, 0.0, False, False
        )
        hidden_state_47 = None
        hidden_state_49 = torch.conv2d(
            hidden_state_48,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_48 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_50 = torch.nn.functional.dropout(
            hidden_state_49, 0.0, False, False
        )
        hidden_state_49 = None
        unsqueeze_10 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_11 = unsqueeze_10.unsqueeze(-1)
        unsqueeze_10 = None
        hidden_state_51 = unsqueeze_11 * hidden_state_50
        unsqueeze_11 = hidden_state_50 = None
        hidden_state_52 = hidden_state_43 + hidden_state_51
        hidden_state_43 = hidden_state_51 = None
        flatten = hidden_state_52.flatten(2)
        hidden_state_52 = None
        hidden_state_53 = flatten.transpose(1, 2)
        flatten = None
        hidden_state_54 = torch.nn.functional.layer_norm(
            hidden_state_53,
            (64,),
            l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_53 = l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_normalization_parameters_bias_ = (None)
        view = hidden_state_54.view(1, 56, 56, 64)
        hidden_state_54 = None
        hidden_state_55 = view.permute(0, 3, 1, 2)
        view = None
        hidden_state_56 = torch.conv2d(
            hidden_state_55,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_55 = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_57 = torch.nn.functional.batch_norm(
            hidden_state_56,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_56 = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_58 = torch.nn.functional.batch_norm(
            hidden_state_57,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_7 = torch.conv2d(
            hidden_state_58,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_8 = torch._C._nn.gelu(input_7)
        input_7 = None
        hidden_state_59 = torch.conv2d(
            input_8,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_60 = torch.conv2d(
            hidden_state_59,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_59 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_61 = torch.conv2d(
            hidden_state_60,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_60 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_3 = input_8 * hidden_state_61
        input_8 = hidden_state_61 = None
        hidden_state_62 = torch.conv2d(
            attended_3,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_3 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_63 = hidden_state_62 + hidden_state_58
        hidden_state_62 = hidden_state_58 = None
        unsqueeze_12 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_13 = unsqueeze_12.unsqueeze(-1)
        unsqueeze_12 = None
        hidden_state_64 = unsqueeze_13 * hidden_state_63
        unsqueeze_13 = hidden_state_63 = None
        hidden_state_65 = hidden_state_57 + hidden_state_64
        hidden_state_57 = hidden_state_64 = None
        hidden_state_66 = torch.nn.functional.batch_norm(
            hidden_state_65,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_67 = torch.conv2d(
            hidden_state_66,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_66 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_68 = torch.conv2d(
            hidden_state_67,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        hidden_state_67 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_69 = torch._C._nn.gelu(hidden_state_68)
        hidden_state_68 = None
        hidden_state_70 = torch.nn.functional.dropout(
            hidden_state_69, 0.0, False, False
        )
        hidden_state_69 = None
        hidden_state_71 = torch.conv2d(
            hidden_state_70,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_70 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_72 = torch.nn.functional.dropout(
            hidden_state_71, 0.0, False, False
        )
        hidden_state_71 = None
        unsqueeze_14 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_15 = unsqueeze_14.unsqueeze(-1)
        unsqueeze_14 = None
        hidden_state_73 = unsqueeze_15 * hidden_state_72
        unsqueeze_15 = hidden_state_72 = None
        hidden_state_74 = hidden_state_65 + hidden_state_73
        hidden_state_65 = hidden_state_73 = None
        hidden_state_75 = torch.nn.functional.batch_norm(
            hidden_state_74,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = (None)
        input_9 = torch.conv2d(
            hidden_state_75,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_10 = torch._C._nn.gelu(input_9)
        input_9 = None
        hidden_state_76 = torch.conv2d(
            input_10,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_77 = torch.conv2d(
            hidden_state_76,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_76 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_78 = torch.conv2d(
            hidden_state_77,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_77 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_4 = input_10 * hidden_state_78
        input_10 = hidden_state_78 = None
        hidden_state_79 = torch.conv2d(
            attended_4,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_4 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_80 = hidden_state_79 + hidden_state_75
        hidden_state_79 = hidden_state_75 = None
        unsqueeze_16 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_17 = unsqueeze_16.unsqueeze(-1)
        unsqueeze_16 = None
        hidden_state_81 = unsqueeze_17 * hidden_state_80
        unsqueeze_17 = hidden_state_80 = None
        hidden_state_82 = hidden_state_74 + hidden_state_81
        hidden_state_74 = hidden_state_81 = None
        hidden_state_83 = torch.nn.functional.batch_norm(
            hidden_state_82,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_84 = torch.conv2d(
            hidden_state_83,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_83 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_85 = torch.conv2d(
            hidden_state_84,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        hidden_state_84 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_86 = torch._C._nn.gelu(hidden_state_85)
        hidden_state_85 = None
        hidden_state_87 = torch.nn.functional.dropout(
            hidden_state_86, 0.0, False, False
        )
        hidden_state_86 = None
        hidden_state_88 = torch.conv2d(
            hidden_state_87,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_87 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_89 = torch.nn.functional.dropout(
            hidden_state_88, 0.0, False, False
        )
        hidden_state_88 = None
        unsqueeze_18 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_19 = unsqueeze_18.unsqueeze(-1)
        unsqueeze_18 = None
        hidden_state_90 = unsqueeze_19 * hidden_state_89
        unsqueeze_19 = hidden_state_89 = None
        hidden_state_91 = hidden_state_82 + hidden_state_90
        hidden_state_82 = hidden_state_90 = None
        hidden_state_92 = torch.nn.functional.batch_norm(
            hidden_state_91,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = (None)
        input_11 = torch.conv2d(
            hidden_state_92,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_12 = torch._C._nn.gelu(input_11)
        input_11 = None
        hidden_state_93 = torch.conv2d(
            input_12,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_94 = torch.conv2d(
            hidden_state_93,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_93 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_95 = torch.conv2d(
            hidden_state_94,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_94 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_5 = input_12 * hidden_state_95
        input_12 = hidden_state_95 = None
        hidden_state_96 = torch.conv2d(
            attended_5,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_5 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_97 = hidden_state_96 + hidden_state_92
        hidden_state_96 = hidden_state_92 = None
        unsqueeze_20 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_21 = unsqueeze_20.unsqueeze(-1)
        unsqueeze_20 = None
        hidden_state_98 = unsqueeze_21 * hidden_state_97
        unsqueeze_21 = hidden_state_97 = None
        hidden_state_99 = hidden_state_91 + hidden_state_98
        hidden_state_91 = hidden_state_98 = None
        hidden_state_100 = torch.nn.functional.batch_norm(
            hidden_state_99,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_101 = torch.conv2d(
            hidden_state_100,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_100 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_102 = torch.conv2d(
            hidden_state_101,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        hidden_state_101 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_103 = torch._C._nn.gelu(hidden_state_102)
        hidden_state_102 = None
        hidden_state_104 = torch.nn.functional.dropout(
            hidden_state_103, 0.0, False, False
        )
        hidden_state_103 = None
        hidden_state_105 = torch.conv2d(
            hidden_state_104,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_104 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_106 = torch.nn.functional.dropout(
            hidden_state_105, 0.0, False, False
        )
        hidden_state_105 = None
        unsqueeze_22 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_23 = unsqueeze_22.unsqueeze(-1)
        unsqueeze_22 = None
        hidden_state_107 = unsqueeze_23 * hidden_state_106
        unsqueeze_23 = hidden_state_106 = None
        hidden_state_108 = hidden_state_99 + hidden_state_107
        hidden_state_99 = hidden_state_107 = None
        hidden_state_109 = torch.nn.functional.batch_norm(
            hidden_state_108,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_ = (None)
        input_13 = torch.conv2d(
            hidden_state_109,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_14 = torch._C._nn.gelu(input_13)
        input_13 = None
        hidden_state_110 = torch.conv2d(
            input_14,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_111 = torch.conv2d(
            hidden_state_110,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_110 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_112 = torch.conv2d(
            hidden_state_111,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_111 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_6 = input_14 * hidden_state_112
        input_14 = hidden_state_112 = None
        hidden_state_113 = torch.conv2d(
            attended_6,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_6 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_114 = hidden_state_113 + hidden_state_109
        hidden_state_113 = hidden_state_109 = None
        unsqueeze_24 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_25 = unsqueeze_24.unsqueeze(-1)
        unsqueeze_24 = None
        hidden_state_115 = unsqueeze_25 * hidden_state_114
        unsqueeze_25 = hidden_state_114 = None
        hidden_state_116 = hidden_state_108 + hidden_state_115
        hidden_state_108 = hidden_state_115 = None
        hidden_state_117 = torch.nn.functional.batch_norm(
            hidden_state_116,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_118 = torch.conv2d(
            hidden_state_117,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_117 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_119 = torch.conv2d(
            hidden_state_118,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        hidden_state_118 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_120 = torch._C._nn.gelu(hidden_state_119)
        hidden_state_119 = None
        hidden_state_121 = torch.nn.functional.dropout(
            hidden_state_120, 0.0, False, False
        )
        hidden_state_120 = None
        hidden_state_122 = torch.conv2d(
            hidden_state_121,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_121 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_123 = torch.nn.functional.dropout(
            hidden_state_122, 0.0, False, False
        )
        hidden_state_122 = None
        unsqueeze_26 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_27 = unsqueeze_26.unsqueeze(-1)
        unsqueeze_26 = None
        hidden_state_124 = unsqueeze_27 * hidden_state_123
        unsqueeze_27 = hidden_state_123 = None
        hidden_state_125 = hidden_state_116 + hidden_state_124
        hidden_state_116 = hidden_state_124 = None
        hidden_state_126 = torch.nn.functional.batch_norm(
            hidden_state_125,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_ = (None)
        input_15 = torch.conv2d(
            hidden_state_126,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_16 = torch._C._nn.gelu(input_15)
        input_15 = None
        hidden_state_127 = torch.conv2d(
            input_16,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            128,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_128 = torch.conv2d(
            hidden_state_127,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            128,
        )
        hidden_state_127 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_129 = torch.conv2d(
            hidden_state_128,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_128 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_7 = input_16 * hidden_state_129
        input_16 = hidden_state_129 = None
        hidden_state_130 = torch.conv2d(
            attended_7,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_7 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_131 = hidden_state_130 + hidden_state_126
        hidden_state_130 = hidden_state_126 = None
        unsqueeze_28 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_29 = unsqueeze_28.unsqueeze(-1)
        unsqueeze_28 = None
        hidden_state_132 = unsqueeze_29 * hidden_state_131
        unsqueeze_29 = hidden_state_131 = None
        hidden_state_133 = hidden_state_125 + hidden_state_132
        hidden_state_125 = hidden_state_132 = None
        hidden_state_134 = torch.nn.functional.batch_norm(
            hidden_state_133,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_135 = torch.conv2d(
            hidden_state_134,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_134 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_136 = torch.conv2d(
            hidden_state_135,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1024,
        )
        hidden_state_135 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_137 = torch._C._nn.gelu(hidden_state_136)
        hidden_state_136 = None
        hidden_state_138 = torch.nn.functional.dropout(
            hidden_state_137, 0.0, False, False
        )
        hidden_state_137 = None
        hidden_state_139 = torch.conv2d(
            hidden_state_138,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_138 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_140 = torch.nn.functional.dropout(
            hidden_state_139, 0.0, False, False
        )
        hidden_state_139 = None
        unsqueeze_30 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_31 = unsqueeze_30.unsqueeze(-1)
        unsqueeze_30 = None
        hidden_state_141 = unsqueeze_31 * hidden_state_140
        unsqueeze_31 = hidden_state_140 = None
        hidden_state_142 = hidden_state_133 + hidden_state_141
        hidden_state_133 = hidden_state_141 = None
        flatten_1 = hidden_state_142.flatten(2)
        hidden_state_142 = None
        hidden_state_143 = flatten_1.transpose(1, 2)
        flatten_1 = None
        hidden_state_144 = torch.nn.functional.layer_norm(
            hidden_state_143,
            (128,),
            l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_143 = l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_normalization_parameters_bias_ = (None)
        view_1 = hidden_state_144.view(1, 28, 28, 128)
        hidden_state_144 = None
        hidden_state_145 = view_1.permute(0, 3, 1, 2)
        view_1 = None
        hidden_state_146 = torch.conv2d(
            hidden_state_145,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_145 = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_147 = torch.nn.functional.batch_norm(
            hidden_state_146,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_146 = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_148 = torch.nn.functional.batch_norm(
            hidden_state_147,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_17 = torch.conv2d(
            hidden_state_148,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_18 = torch._C._nn.gelu(input_17)
        input_17 = None
        hidden_state_149 = torch.conv2d(
            input_18,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_150 = torch.conv2d(
            hidden_state_149,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_149 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_151 = torch.conv2d(
            hidden_state_150,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_150 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_8 = input_18 * hidden_state_151
        input_18 = hidden_state_151 = None
        hidden_state_152 = torch.conv2d(
            attended_8,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_8 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_153 = hidden_state_152 + hidden_state_148
        hidden_state_152 = hidden_state_148 = None
        unsqueeze_32 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_33 = unsqueeze_32.unsqueeze(-1)
        unsqueeze_32 = None
        hidden_state_154 = unsqueeze_33 * hidden_state_153
        unsqueeze_33 = hidden_state_153 = None
        hidden_state_155 = hidden_state_147 + hidden_state_154
        hidden_state_147 = hidden_state_154 = None
        hidden_state_156 = torch.nn.functional.batch_norm(
            hidden_state_155,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_157 = torch.conv2d(
            hidden_state_156,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_156 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_158 = torch.conv2d(
            hidden_state_157,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_157 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_159 = torch._C._nn.gelu(hidden_state_158)
        hidden_state_158 = None
        hidden_state_160 = torch.nn.functional.dropout(
            hidden_state_159, 0.0, False, False
        )
        hidden_state_159 = None
        hidden_state_161 = torch.conv2d(
            hidden_state_160,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_160 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_162 = torch.nn.functional.dropout(
            hidden_state_161, 0.0, False, False
        )
        hidden_state_161 = None
        unsqueeze_34 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_35 = unsqueeze_34.unsqueeze(-1)
        unsqueeze_34 = None
        hidden_state_163 = unsqueeze_35 * hidden_state_162
        unsqueeze_35 = hidden_state_162 = None
        hidden_state_164 = hidden_state_155 + hidden_state_163
        hidden_state_155 = hidden_state_163 = None
        hidden_state_165 = torch.nn.functional.batch_norm(
            hidden_state_164,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = (None)
        input_19 = torch.conv2d(
            hidden_state_165,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_20 = torch._C._nn.gelu(input_19)
        input_19 = None
        hidden_state_166 = torch.conv2d(
            input_20,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_167 = torch.conv2d(
            hidden_state_166,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_166 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_168 = torch.conv2d(
            hidden_state_167,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_167 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_9 = input_20 * hidden_state_168
        input_20 = hidden_state_168 = None
        hidden_state_169 = torch.conv2d(
            attended_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_170 = hidden_state_169 + hidden_state_165
        hidden_state_169 = hidden_state_165 = None
        unsqueeze_36 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_37 = unsqueeze_36.unsqueeze(-1)
        unsqueeze_36 = None
        hidden_state_171 = unsqueeze_37 * hidden_state_170
        unsqueeze_37 = hidden_state_170 = None
        hidden_state_172 = hidden_state_164 + hidden_state_171
        hidden_state_164 = hidden_state_171 = None
        hidden_state_173 = torch.nn.functional.batch_norm(
            hidden_state_172,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_174 = torch.conv2d(
            hidden_state_173,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_173 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_175 = torch.conv2d(
            hidden_state_174,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_174 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_176 = torch._C._nn.gelu(hidden_state_175)
        hidden_state_175 = None
        hidden_state_177 = torch.nn.functional.dropout(
            hidden_state_176, 0.0, False, False
        )
        hidden_state_176 = None
        hidden_state_178 = torch.conv2d(
            hidden_state_177,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_177 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_179 = torch.nn.functional.dropout(
            hidden_state_178, 0.0, False, False
        )
        hidden_state_178 = None
        unsqueeze_38 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_39 = unsqueeze_38.unsqueeze(-1)
        unsqueeze_38 = None
        hidden_state_180 = unsqueeze_39 * hidden_state_179
        unsqueeze_39 = hidden_state_179 = None
        hidden_state_181 = hidden_state_172 + hidden_state_180
        hidden_state_172 = hidden_state_180 = None
        hidden_state_182 = torch.nn.functional.batch_norm(
            hidden_state_181,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = (None)
        input_21 = torch.conv2d(
            hidden_state_182,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_22 = torch._C._nn.gelu(input_21)
        input_21 = None
        hidden_state_183 = torch.conv2d(
            input_22,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_184 = torch.conv2d(
            hidden_state_183,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_183 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_185 = torch.conv2d(
            hidden_state_184,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_184 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_10 = input_22 * hidden_state_185
        input_22 = hidden_state_185 = None
        hidden_state_186 = torch.conv2d(
            attended_10,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_10 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_187 = hidden_state_186 + hidden_state_182
        hidden_state_186 = hidden_state_182 = None
        unsqueeze_40 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_41 = unsqueeze_40.unsqueeze(-1)
        unsqueeze_40 = None
        hidden_state_188 = unsqueeze_41 * hidden_state_187
        unsqueeze_41 = hidden_state_187 = None
        hidden_state_189 = hidden_state_181 + hidden_state_188
        hidden_state_181 = hidden_state_188 = None
        hidden_state_190 = torch.nn.functional.batch_norm(
            hidden_state_189,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_191 = torch.conv2d(
            hidden_state_190,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_190 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_192 = torch.conv2d(
            hidden_state_191,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_191 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_193 = torch._C._nn.gelu(hidden_state_192)
        hidden_state_192 = None
        hidden_state_194 = torch.nn.functional.dropout(
            hidden_state_193, 0.0, False, False
        )
        hidden_state_193 = None
        hidden_state_195 = torch.conv2d(
            hidden_state_194,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_194 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_196 = torch.nn.functional.dropout(
            hidden_state_195, 0.0, False, False
        )
        hidden_state_195 = None
        unsqueeze_42 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_43 = unsqueeze_42.unsqueeze(-1)
        unsqueeze_42 = None
        hidden_state_197 = unsqueeze_43 * hidden_state_196
        unsqueeze_43 = hidden_state_196 = None
        hidden_state_198 = hidden_state_189 + hidden_state_197
        hidden_state_189 = hidden_state_197 = None
        hidden_state_199 = torch.nn.functional.batch_norm(
            hidden_state_198,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_pre_normomalization_parameters_bias_ = (None)
        input_23 = torch.conv2d(
            hidden_state_199,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_24 = torch._C._nn.gelu(input_23)
        input_23 = None
        hidden_state_200 = torch.conv2d(
            input_24,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_201 = torch.conv2d(
            hidden_state_200,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_200 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_202 = torch.conv2d(
            hidden_state_201,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_201 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_11 = input_24 * hidden_state_202
        input_24 = hidden_state_202 = None
        hidden_state_203 = torch.conv2d(
            attended_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_204 = hidden_state_203 + hidden_state_199
        hidden_state_203 = hidden_state_199 = None
        unsqueeze_44 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_45 = unsqueeze_44.unsqueeze(-1)
        unsqueeze_44 = None
        hidden_state_205 = unsqueeze_45 * hidden_state_204
        unsqueeze_45 = hidden_state_204 = None
        hidden_state_206 = hidden_state_198 + hidden_state_205
        hidden_state_198 = hidden_state_205 = None
        hidden_state_207 = torch.nn.functional.batch_norm(
            hidden_state_206,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_208 = torch.conv2d(
            hidden_state_207,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_207 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_209 = torch.conv2d(
            hidden_state_208,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_208 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_210 = torch._C._nn.gelu(hidden_state_209)
        hidden_state_209 = None
        hidden_state_211 = torch.nn.functional.dropout(
            hidden_state_210, 0.0, False, False
        )
        hidden_state_210 = None
        hidden_state_212 = torch.conv2d(
            hidden_state_211,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_211 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_213 = torch.nn.functional.dropout(
            hidden_state_212, 0.0, False, False
        )
        hidden_state_212 = None
        unsqueeze_46 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_47 = unsqueeze_46.unsqueeze(-1)
        unsqueeze_46 = None
        hidden_state_214 = unsqueeze_47 * hidden_state_213
        unsqueeze_47 = hidden_state_213 = None
        hidden_state_215 = hidden_state_206 + hidden_state_214
        hidden_state_206 = hidden_state_214 = None
        hidden_state_216 = torch.nn.functional.batch_norm(
            hidden_state_215,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_pre_normomalization_parameters_bias_ = (None)
        input_25 = torch.conv2d(
            hidden_state_216,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_26 = torch._C._nn.gelu(input_25)
        input_25 = None
        hidden_state_217 = torch.conv2d(
            input_26,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_218 = torch.conv2d(
            hidden_state_217,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_217 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_219 = torch.conv2d(
            hidden_state_218,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_218 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_12 = input_26 * hidden_state_219
        input_26 = hidden_state_219 = None
        hidden_state_220 = torch.conv2d(
            attended_12,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_12 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_221 = hidden_state_220 + hidden_state_216
        hidden_state_220 = hidden_state_216 = None
        unsqueeze_48 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_49 = unsqueeze_48.unsqueeze(-1)
        unsqueeze_48 = None
        hidden_state_222 = unsqueeze_49 * hidden_state_221
        unsqueeze_49 = hidden_state_221 = None
        hidden_state_223 = hidden_state_215 + hidden_state_222
        hidden_state_215 = hidden_state_222 = None
        hidden_state_224 = torch.nn.functional.batch_norm(
            hidden_state_223,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_225 = torch.conv2d(
            hidden_state_224,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_224 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_226 = torch.conv2d(
            hidden_state_225,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_225 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_227 = torch._C._nn.gelu(hidden_state_226)
        hidden_state_226 = None
        hidden_state_228 = torch.nn.functional.dropout(
            hidden_state_227, 0.0, False, False
        )
        hidden_state_227 = None
        hidden_state_229 = torch.conv2d(
            hidden_state_228,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_228 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_230 = torch.nn.functional.dropout(
            hidden_state_229, 0.0, False, False
        )
        hidden_state_229 = None
        unsqueeze_50 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_51 = unsqueeze_50.unsqueeze(-1)
        unsqueeze_50 = None
        hidden_state_231 = unsqueeze_51 * hidden_state_230
        unsqueeze_51 = hidden_state_230 = None
        hidden_state_232 = hidden_state_223 + hidden_state_231
        hidden_state_223 = hidden_state_231 = None
        hidden_state_233 = torch.nn.functional.batch_norm(
            hidden_state_232,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_pre_normomalization_parameters_bias_ = (None)
        input_27 = torch.conv2d(
            hidden_state_233,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_28 = torch._C._nn.gelu(input_27)
        input_27 = None
        hidden_state_234 = torch.conv2d(
            input_28,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_235 = torch.conv2d(
            hidden_state_234,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_234 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_236 = torch.conv2d(
            hidden_state_235,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_235 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_13 = input_28 * hidden_state_236
        input_28 = hidden_state_236 = None
        hidden_state_237 = torch.conv2d(
            attended_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_238 = hidden_state_237 + hidden_state_233
        hidden_state_237 = hidden_state_233 = None
        unsqueeze_52 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_53 = unsqueeze_52.unsqueeze(-1)
        unsqueeze_52 = None
        hidden_state_239 = unsqueeze_53 * hidden_state_238
        unsqueeze_53 = hidden_state_238 = None
        hidden_state_240 = hidden_state_232 + hidden_state_239
        hidden_state_232 = hidden_state_239 = None
        hidden_state_241 = torch.nn.functional.batch_norm(
            hidden_state_240,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_242 = torch.conv2d(
            hidden_state_241,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_241 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_243 = torch.conv2d(
            hidden_state_242,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_242 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_244 = torch._C._nn.gelu(hidden_state_243)
        hidden_state_243 = None
        hidden_state_245 = torch.nn.functional.dropout(
            hidden_state_244, 0.0, False, False
        )
        hidden_state_244 = None
        hidden_state_246 = torch.conv2d(
            hidden_state_245,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_245 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_247 = torch.nn.functional.dropout(
            hidden_state_246, 0.0, False, False
        )
        hidden_state_246 = None
        unsqueeze_54 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_55 = unsqueeze_54.unsqueeze(-1)
        unsqueeze_54 = None
        hidden_state_248 = unsqueeze_55 * hidden_state_247
        unsqueeze_55 = hidden_state_247 = None
        hidden_state_249 = hidden_state_240 + hidden_state_248
        hidden_state_240 = hidden_state_248 = None
        hidden_state_250 = torch.nn.functional.batch_norm(
            hidden_state_249,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_pre_normomalization_parameters_bias_ = (None)
        input_29 = torch.conv2d(
            hidden_state_250,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_30 = torch._C._nn.gelu(input_29)
        input_29 = None
        hidden_state_251 = torch.conv2d(
            input_30,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_252 = torch.conv2d(
            hidden_state_251,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_251 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_253 = torch.conv2d(
            hidden_state_252,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_252 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_14 = input_30 * hidden_state_253
        input_30 = hidden_state_253 = None
        hidden_state_254 = torch.conv2d(
            attended_14,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_14 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_255 = hidden_state_254 + hidden_state_250
        hidden_state_254 = hidden_state_250 = None
        unsqueeze_56 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_57 = unsqueeze_56.unsqueeze(-1)
        unsqueeze_56 = None
        hidden_state_256 = unsqueeze_57 * hidden_state_255
        unsqueeze_57 = hidden_state_255 = None
        hidden_state_257 = hidden_state_249 + hidden_state_256
        hidden_state_249 = hidden_state_256 = None
        hidden_state_258 = torch.nn.functional.batch_norm(
            hidden_state_257,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_259 = torch.conv2d(
            hidden_state_258,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_258 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_260 = torch.conv2d(
            hidden_state_259,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_259 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_261 = torch._C._nn.gelu(hidden_state_260)
        hidden_state_260 = None
        hidden_state_262 = torch.nn.functional.dropout(
            hidden_state_261, 0.0, False, False
        )
        hidden_state_261 = None
        hidden_state_263 = torch.conv2d(
            hidden_state_262,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_262 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_264 = torch.nn.functional.dropout(
            hidden_state_263, 0.0, False, False
        )
        hidden_state_263 = None
        unsqueeze_58 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_59 = unsqueeze_58.unsqueeze(-1)
        unsqueeze_58 = None
        hidden_state_265 = unsqueeze_59 * hidden_state_264
        unsqueeze_59 = hidden_state_264 = None
        hidden_state_266 = hidden_state_257 + hidden_state_265
        hidden_state_257 = hidden_state_265 = None
        hidden_state_267 = torch.nn.functional.batch_norm(
            hidden_state_266,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_pre_normomalization_parameters_bias_ = (None)
        input_31 = torch.conv2d(
            hidden_state_267,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_32 = torch._C._nn.gelu(input_31)
        input_31 = None
        hidden_state_268 = torch.conv2d(
            input_32,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_269 = torch.conv2d(
            hidden_state_268,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_268 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_270 = torch.conv2d(
            hidden_state_269,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_269 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_15 = input_32 * hidden_state_270
        input_32 = hidden_state_270 = None
        hidden_state_271 = torch.conv2d(
            attended_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_272 = hidden_state_271 + hidden_state_267
        hidden_state_271 = hidden_state_267 = None
        unsqueeze_60 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_61 = unsqueeze_60.unsqueeze(-1)
        unsqueeze_60 = None
        hidden_state_273 = unsqueeze_61 * hidden_state_272
        unsqueeze_61 = hidden_state_272 = None
        hidden_state_274 = hidden_state_266 + hidden_state_273
        hidden_state_266 = hidden_state_273 = None
        hidden_state_275 = torch.nn.functional.batch_norm(
            hidden_state_274,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_276 = torch.conv2d(
            hidden_state_275,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_275 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_277 = torch.conv2d(
            hidden_state_276,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_276 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_278 = torch._C._nn.gelu(hidden_state_277)
        hidden_state_277 = None
        hidden_state_279 = torch.nn.functional.dropout(
            hidden_state_278, 0.0, False, False
        )
        hidden_state_278 = None
        hidden_state_280 = torch.conv2d(
            hidden_state_279,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_279 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_281 = torch.nn.functional.dropout(
            hidden_state_280, 0.0, False, False
        )
        hidden_state_280 = None
        unsqueeze_62 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_63 = unsqueeze_62.unsqueeze(-1)
        unsqueeze_62 = None
        hidden_state_282 = unsqueeze_63 * hidden_state_281
        unsqueeze_63 = hidden_state_281 = None
        hidden_state_283 = hidden_state_274 + hidden_state_282
        hidden_state_274 = hidden_state_282 = None
        hidden_state_284 = torch.nn.functional.batch_norm(
            hidden_state_283,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_pre_normomalization_parameters_bias_ = (None)
        input_33 = torch.conv2d(
            hidden_state_284,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_34 = torch._C._nn.gelu(input_33)
        input_33 = None
        hidden_state_285 = torch.conv2d(
            input_34,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_286 = torch.conv2d(
            hidden_state_285,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_285 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_287 = torch.conv2d(
            hidden_state_286,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_286 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_16 = input_34 * hidden_state_287
        input_34 = hidden_state_287 = None
        hidden_state_288 = torch.conv2d(
            attended_16,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_16 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_289 = hidden_state_288 + hidden_state_284
        hidden_state_288 = hidden_state_284 = None
        unsqueeze_64 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_65 = unsqueeze_64.unsqueeze(-1)
        unsqueeze_64 = None
        hidden_state_290 = unsqueeze_65 * hidden_state_289
        unsqueeze_65 = hidden_state_289 = None
        hidden_state_291 = hidden_state_283 + hidden_state_290
        hidden_state_283 = hidden_state_290 = None
        hidden_state_292 = torch.nn.functional.batch_norm(
            hidden_state_291,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_293 = torch.conv2d(
            hidden_state_292,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_292 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_294 = torch.conv2d(
            hidden_state_293,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_293 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_295 = torch._C._nn.gelu(hidden_state_294)
        hidden_state_294 = None
        hidden_state_296 = torch.nn.functional.dropout(
            hidden_state_295, 0.0, False, False
        )
        hidden_state_295 = None
        hidden_state_297 = torch.conv2d(
            hidden_state_296,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_296 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_298 = torch.nn.functional.dropout(
            hidden_state_297, 0.0, False, False
        )
        hidden_state_297 = None
        unsqueeze_66 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_67 = unsqueeze_66.unsqueeze(-1)
        unsqueeze_66 = None
        hidden_state_299 = unsqueeze_67 * hidden_state_298
        unsqueeze_67 = hidden_state_298 = None
        hidden_state_300 = hidden_state_291 + hidden_state_299
        hidden_state_291 = hidden_state_299 = None
        hidden_state_301 = torch.nn.functional.batch_norm(
            hidden_state_300,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_pre_normomalization_parameters_bias_ = (None)
        input_35 = torch.conv2d(
            hidden_state_301,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_36 = torch._C._nn.gelu(input_35)
        input_35 = None
        hidden_state_302 = torch.conv2d(
            input_36,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_303 = torch.conv2d(
            hidden_state_302,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_302 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_304 = torch.conv2d(
            hidden_state_303,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_303 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_17 = input_36 * hidden_state_304
        input_36 = hidden_state_304 = None
        hidden_state_305 = torch.conv2d(
            attended_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_306 = hidden_state_305 + hidden_state_301
        hidden_state_305 = hidden_state_301 = None
        unsqueeze_68 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_69 = unsqueeze_68.unsqueeze(-1)
        unsqueeze_68 = None
        hidden_state_307 = unsqueeze_69 * hidden_state_306
        unsqueeze_69 = hidden_state_306 = None
        hidden_state_308 = hidden_state_300 + hidden_state_307
        hidden_state_300 = hidden_state_307 = None
        hidden_state_309 = torch.nn.functional.batch_norm(
            hidden_state_308,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_310 = torch.conv2d(
            hidden_state_309,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_309 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_311 = torch.conv2d(
            hidden_state_310,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_310 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_312 = torch._C._nn.gelu(hidden_state_311)
        hidden_state_311 = None
        hidden_state_313 = torch.nn.functional.dropout(
            hidden_state_312, 0.0, False, False
        )
        hidden_state_312 = None
        hidden_state_314 = torch.conv2d(
            hidden_state_313,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_313 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_315 = torch.nn.functional.dropout(
            hidden_state_314, 0.0, False, False
        )
        hidden_state_314 = None
        unsqueeze_70 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_71 = unsqueeze_70.unsqueeze(-1)
        unsqueeze_70 = None
        hidden_state_316 = unsqueeze_71 * hidden_state_315
        unsqueeze_71 = hidden_state_315 = None
        hidden_state_317 = hidden_state_308 + hidden_state_316
        hidden_state_308 = hidden_state_316 = None
        hidden_state_318 = torch.nn.functional.batch_norm(
            hidden_state_317,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_pre_normomalization_parameters_bias_ = (None)
        input_37 = torch.conv2d(
            hidden_state_318,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_38 = torch._C._nn.gelu(input_37)
        input_37 = None
        hidden_state_319 = torch.conv2d(
            input_38,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_320 = torch.conv2d(
            hidden_state_319,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_319 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_321 = torch.conv2d(
            hidden_state_320,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_320 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_18 = input_38 * hidden_state_321
        input_38 = hidden_state_321 = None
        hidden_state_322 = torch.conv2d(
            attended_18,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_18 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_323 = hidden_state_322 + hidden_state_318
        hidden_state_322 = hidden_state_318 = None
        unsqueeze_72 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_73 = unsqueeze_72.unsqueeze(-1)
        unsqueeze_72 = None
        hidden_state_324 = unsqueeze_73 * hidden_state_323
        unsqueeze_73 = hidden_state_323 = None
        hidden_state_325 = hidden_state_317 + hidden_state_324
        hidden_state_317 = hidden_state_324 = None
        hidden_state_326 = torch.nn.functional.batch_norm(
            hidden_state_325,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_327 = torch.conv2d(
            hidden_state_326,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_326 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_328 = torch.conv2d(
            hidden_state_327,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_327 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_329 = torch._C._nn.gelu(hidden_state_328)
        hidden_state_328 = None
        hidden_state_330 = torch.nn.functional.dropout(
            hidden_state_329, 0.0, False, False
        )
        hidden_state_329 = None
        hidden_state_331 = torch.conv2d(
            hidden_state_330,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_330 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_332 = torch.nn.functional.dropout(
            hidden_state_331, 0.0, False, False
        )
        hidden_state_331 = None
        unsqueeze_74 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_75 = unsqueeze_74.unsqueeze(-1)
        unsqueeze_74 = None
        hidden_state_333 = unsqueeze_75 * hidden_state_332
        unsqueeze_75 = hidden_state_332 = None
        hidden_state_334 = hidden_state_325 + hidden_state_333
        hidden_state_325 = hidden_state_333 = None
        hidden_state_335 = torch.nn.functional.batch_norm(
            hidden_state_334,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_pre_normomalization_parameters_bias_ = (None)
        input_39 = torch.conv2d(
            hidden_state_335,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_40 = torch._C._nn.gelu(input_39)
        input_39 = None
        hidden_state_336 = torch.conv2d(
            input_40,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_337 = torch.conv2d(
            hidden_state_336,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_336 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_338 = torch.conv2d(
            hidden_state_337,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_337 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_19 = input_40 * hidden_state_338
        input_40 = hidden_state_338 = None
        hidden_state_339 = torch.conv2d(
            attended_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_340 = hidden_state_339 + hidden_state_335
        hidden_state_339 = hidden_state_335 = None
        unsqueeze_76 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_77 = unsqueeze_76.unsqueeze(-1)
        unsqueeze_76 = None
        hidden_state_341 = unsqueeze_77 * hidden_state_340
        unsqueeze_77 = hidden_state_340 = None
        hidden_state_342 = hidden_state_334 + hidden_state_341
        hidden_state_334 = hidden_state_341 = None
        hidden_state_343 = torch.nn.functional.batch_norm(
            hidden_state_342,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_344 = torch.conv2d(
            hidden_state_343,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_343 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_345 = torch.conv2d(
            hidden_state_344,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_344 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_346 = torch._C._nn.gelu(hidden_state_345)
        hidden_state_345 = None
        hidden_state_347 = torch.nn.functional.dropout(
            hidden_state_346, 0.0, False, False
        )
        hidden_state_346 = None
        hidden_state_348 = torch.conv2d(
            hidden_state_347,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_347 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_349 = torch.nn.functional.dropout(
            hidden_state_348, 0.0, False, False
        )
        hidden_state_348 = None
        unsqueeze_78 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_79 = unsqueeze_78.unsqueeze(-1)
        unsqueeze_78 = None
        hidden_state_350 = unsqueeze_79 * hidden_state_349
        unsqueeze_79 = hidden_state_349 = None
        hidden_state_351 = hidden_state_342 + hidden_state_350
        hidden_state_342 = hidden_state_350 = None
        hidden_state_352 = torch.nn.functional.batch_norm(
            hidden_state_351,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_pre_normomalization_parameters_bias_ = (None)
        input_41 = torch.conv2d(
            hidden_state_352,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_42 = torch._C._nn.gelu(input_41)
        input_41 = None
        hidden_state_353 = torch.conv2d(
            input_42,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_354 = torch.conv2d(
            hidden_state_353,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_353 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_355 = torch.conv2d(
            hidden_state_354,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_354 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_20 = input_42 * hidden_state_355
        input_42 = hidden_state_355 = None
        hidden_state_356 = torch.conv2d(
            attended_20,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_20 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_357 = hidden_state_356 + hidden_state_352
        hidden_state_356 = hidden_state_352 = None
        unsqueeze_80 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_81 = unsqueeze_80.unsqueeze(-1)
        unsqueeze_80 = None
        hidden_state_358 = unsqueeze_81 * hidden_state_357
        unsqueeze_81 = hidden_state_357 = None
        hidden_state_359 = hidden_state_351 + hidden_state_358
        hidden_state_351 = hidden_state_358 = None
        hidden_state_360 = torch.nn.functional.batch_norm(
            hidden_state_359,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_361 = torch.conv2d(
            hidden_state_360,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_360 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_362 = torch.conv2d(
            hidden_state_361,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_361 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_363 = torch._C._nn.gelu(hidden_state_362)
        hidden_state_362 = None
        hidden_state_364 = torch.nn.functional.dropout(
            hidden_state_363, 0.0, False, False
        )
        hidden_state_363 = None
        hidden_state_365 = torch.conv2d(
            hidden_state_364,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_364 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_366 = torch.nn.functional.dropout(
            hidden_state_365, 0.0, False, False
        )
        hidden_state_365 = None
        unsqueeze_82 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_83 = unsqueeze_82.unsqueeze(-1)
        unsqueeze_82 = None
        hidden_state_367 = unsqueeze_83 * hidden_state_366
        unsqueeze_83 = hidden_state_366 = None
        hidden_state_368 = hidden_state_359 + hidden_state_367
        hidden_state_359 = hidden_state_367 = None
        hidden_state_369 = torch.nn.functional.batch_norm(
            hidden_state_368,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_pre_normomalization_parameters_bias_ = (None)
        input_43 = torch.conv2d(
            hidden_state_369,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_44 = torch._C._nn.gelu(input_43)
        input_43 = None
        hidden_state_370 = torch.conv2d(
            input_44,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_371 = torch.conv2d(
            hidden_state_370,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_370 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_372 = torch.conv2d(
            hidden_state_371,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_371 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_21 = input_44 * hidden_state_372
        input_44 = hidden_state_372 = None
        hidden_state_373 = torch.conv2d(
            attended_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_374 = hidden_state_373 + hidden_state_369
        hidden_state_373 = hidden_state_369 = None
        unsqueeze_84 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_85 = unsqueeze_84.unsqueeze(-1)
        unsqueeze_84 = None
        hidden_state_375 = unsqueeze_85 * hidden_state_374
        unsqueeze_85 = hidden_state_374 = None
        hidden_state_376 = hidden_state_368 + hidden_state_375
        hidden_state_368 = hidden_state_375 = None
        hidden_state_377 = torch.nn.functional.batch_norm(
            hidden_state_376,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_378 = torch.conv2d(
            hidden_state_377,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_377 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_379 = torch.conv2d(
            hidden_state_378,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_378 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_380 = torch._C._nn.gelu(hidden_state_379)
        hidden_state_379 = None
        hidden_state_381 = torch.nn.functional.dropout(
            hidden_state_380, 0.0, False, False
        )
        hidden_state_380 = None
        hidden_state_382 = torch.conv2d(
            hidden_state_381,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_381 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_383 = torch.nn.functional.dropout(
            hidden_state_382, 0.0, False, False
        )
        hidden_state_382 = None
        unsqueeze_86 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_87 = unsqueeze_86.unsqueeze(-1)
        unsqueeze_86 = None
        hidden_state_384 = unsqueeze_87 * hidden_state_383
        unsqueeze_87 = hidden_state_383 = None
        hidden_state_385 = hidden_state_376 + hidden_state_384
        hidden_state_376 = hidden_state_384 = None
        hidden_state_386 = torch.nn.functional.batch_norm(
            hidden_state_385,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_pre_normomalization_parameters_bias_ = (None)
        input_45 = torch.conv2d(
            hidden_state_386,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_46 = torch._C._nn.gelu(input_45)
        input_45 = None
        hidden_state_387 = torch.conv2d(
            input_46,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_388 = torch.conv2d(
            hidden_state_387,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_387 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_389 = torch.conv2d(
            hidden_state_388,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_388 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_22 = input_46 * hidden_state_389
        input_46 = hidden_state_389 = None
        hidden_state_390 = torch.conv2d(
            attended_22,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_22 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_391 = hidden_state_390 + hidden_state_386
        hidden_state_390 = hidden_state_386 = None
        unsqueeze_88 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_89 = unsqueeze_88.unsqueeze(-1)
        unsqueeze_88 = None
        hidden_state_392 = unsqueeze_89 * hidden_state_391
        unsqueeze_89 = hidden_state_391 = None
        hidden_state_393 = hidden_state_385 + hidden_state_392
        hidden_state_385 = hidden_state_392 = None
        hidden_state_394 = torch.nn.functional.batch_norm(
            hidden_state_393,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_395 = torch.conv2d(
            hidden_state_394,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_394 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_396 = torch.conv2d(
            hidden_state_395,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_395 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_397 = torch._C._nn.gelu(hidden_state_396)
        hidden_state_396 = None
        hidden_state_398 = torch.nn.functional.dropout(
            hidden_state_397, 0.0, False, False
        )
        hidden_state_397 = None
        hidden_state_399 = torch.conv2d(
            hidden_state_398,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_398 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_400 = torch.nn.functional.dropout(
            hidden_state_399, 0.0, False, False
        )
        hidden_state_399 = None
        unsqueeze_90 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_91 = unsqueeze_90.unsqueeze(-1)
        unsqueeze_90 = None
        hidden_state_401 = unsqueeze_91 * hidden_state_400
        unsqueeze_91 = hidden_state_400 = None
        hidden_state_402 = hidden_state_393 + hidden_state_401
        hidden_state_393 = hidden_state_401 = None
        hidden_state_403 = torch.nn.functional.batch_norm(
            hidden_state_402,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_pre_normomalization_parameters_bias_ = (None)
        input_47 = torch.conv2d(
            hidden_state_403,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_48 = torch._C._nn.gelu(input_47)
        input_47 = None
        hidden_state_404 = torch.conv2d(
            input_48,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_405 = torch.conv2d(
            hidden_state_404,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_404 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_406 = torch.conv2d(
            hidden_state_405,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_405 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_23 = input_48 * hidden_state_406
        input_48 = hidden_state_406 = None
        hidden_state_407 = torch.conv2d(
            attended_23,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_23 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_408 = hidden_state_407 + hidden_state_403
        hidden_state_407 = hidden_state_403 = None
        unsqueeze_92 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_93 = unsqueeze_92.unsqueeze(-1)
        unsqueeze_92 = None
        hidden_state_409 = unsqueeze_93 * hidden_state_408
        unsqueeze_93 = hidden_state_408 = None
        hidden_state_410 = hidden_state_402 + hidden_state_409
        hidden_state_402 = hidden_state_409 = None
        hidden_state_411 = torch.nn.functional.batch_norm(
            hidden_state_410,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_412 = torch.conv2d(
            hidden_state_411,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_411 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_413 = torch.conv2d(
            hidden_state_412,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_412 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_414 = torch._C._nn.gelu(hidden_state_413)
        hidden_state_413 = None
        hidden_state_415 = torch.nn.functional.dropout(
            hidden_state_414, 0.0, False, False
        )
        hidden_state_414 = None
        hidden_state_416 = torch.conv2d(
            hidden_state_415,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_415 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_417 = torch.nn.functional.dropout(
            hidden_state_416, 0.0, False, False
        )
        hidden_state_416 = None
        unsqueeze_94 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_95 = unsqueeze_94.unsqueeze(-1)
        unsqueeze_94 = None
        hidden_state_418 = unsqueeze_95 * hidden_state_417
        unsqueeze_95 = hidden_state_417 = None
        hidden_state_419 = hidden_state_410 + hidden_state_418
        hidden_state_410 = hidden_state_418 = None
        hidden_state_420 = torch.nn.functional.batch_norm(
            hidden_state_419,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_pre_normomalization_parameters_bias_ = (None)
        input_49 = torch.conv2d(
            hidden_state_420,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_50 = torch._C._nn.gelu(input_49)
        input_49 = None
        hidden_state_421 = torch.conv2d(
            input_50,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_422 = torch.conv2d(
            hidden_state_421,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_421 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_423 = torch.conv2d(
            hidden_state_422,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_422 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_24 = input_50 * hidden_state_423
        input_50 = hidden_state_423 = None
        hidden_state_424 = torch.conv2d(
            attended_24,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_24 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_425 = hidden_state_424 + hidden_state_420
        hidden_state_424 = hidden_state_420 = None
        unsqueeze_96 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_97 = unsqueeze_96.unsqueeze(-1)
        unsqueeze_96 = None
        hidden_state_426 = unsqueeze_97 * hidden_state_425
        unsqueeze_97 = hidden_state_425 = None
        hidden_state_427 = hidden_state_419 + hidden_state_426
        hidden_state_419 = hidden_state_426 = None
        hidden_state_428 = torch.nn.functional.batch_norm(
            hidden_state_427,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_429 = torch.conv2d(
            hidden_state_428,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_428 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_430 = torch.conv2d(
            hidden_state_429,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_429 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_431 = torch._C._nn.gelu(hidden_state_430)
        hidden_state_430 = None
        hidden_state_432 = torch.nn.functional.dropout(
            hidden_state_431, 0.0, False, False
        )
        hidden_state_431 = None
        hidden_state_433 = torch.conv2d(
            hidden_state_432,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_432 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_434 = torch.nn.functional.dropout(
            hidden_state_433, 0.0, False, False
        )
        hidden_state_433 = None
        unsqueeze_98 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_16_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_99 = unsqueeze_98.unsqueeze(-1)
        unsqueeze_98 = None
        hidden_state_435 = unsqueeze_99 * hidden_state_434
        unsqueeze_99 = hidden_state_434 = None
        hidden_state_436 = hidden_state_427 + hidden_state_435
        hidden_state_427 = hidden_state_435 = None
        hidden_state_437 = torch.nn.functional.batch_norm(
            hidden_state_436,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_pre_normomalization_parameters_bias_ = (None)
        input_51 = torch.conv2d(
            hidden_state_437,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_52 = torch._C._nn.gelu(input_51)
        input_51 = None
        hidden_state_438 = torch.conv2d(
            input_52,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_439 = torch.conv2d(
            hidden_state_438,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_438 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_440 = torch.conv2d(
            hidden_state_439,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_439 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_25 = input_52 * hidden_state_440
        input_52 = hidden_state_440 = None
        hidden_state_441 = torch.conv2d(
            attended_25,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_25 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_442 = hidden_state_441 + hidden_state_437
        hidden_state_441 = hidden_state_437 = None
        unsqueeze_100 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_101 = unsqueeze_100.unsqueeze(-1)
        unsqueeze_100 = None
        hidden_state_443 = unsqueeze_101 * hidden_state_442
        unsqueeze_101 = hidden_state_442 = None
        hidden_state_444 = hidden_state_436 + hidden_state_443
        hidden_state_436 = hidden_state_443 = None
        hidden_state_445 = torch.nn.functional.batch_norm(
            hidden_state_444,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_446 = torch.conv2d(
            hidden_state_445,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_445 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_447 = torch.conv2d(
            hidden_state_446,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_446 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_448 = torch._C._nn.gelu(hidden_state_447)
        hidden_state_447 = None
        hidden_state_449 = torch.nn.functional.dropout(
            hidden_state_448, 0.0, False, False
        )
        hidden_state_448 = None
        hidden_state_450 = torch.conv2d(
            hidden_state_449,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_449 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_451 = torch.nn.functional.dropout(
            hidden_state_450, 0.0, False, False
        )
        hidden_state_450 = None
        unsqueeze_102 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_17_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_103 = unsqueeze_102.unsqueeze(-1)
        unsqueeze_102 = None
        hidden_state_452 = unsqueeze_103 * hidden_state_451
        unsqueeze_103 = hidden_state_451 = None
        hidden_state_453 = hidden_state_444 + hidden_state_452
        hidden_state_444 = hidden_state_452 = None
        hidden_state_454 = torch.nn.functional.batch_norm(
            hidden_state_453,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_pre_normomalization_parameters_bias_ = (None)
        input_53 = torch.conv2d(
            hidden_state_454,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_54 = torch._C._nn.gelu(input_53)
        input_53 = None
        hidden_state_455 = torch.conv2d(
            input_54,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_456 = torch.conv2d(
            hidden_state_455,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_455 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_457 = torch.conv2d(
            hidden_state_456,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_456 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_26 = input_54 * hidden_state_457
        input_54 = hidden_state_457 = None
        hidden_state_458 = torch.conv2d(
            attended_26,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_26 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_459 = hidden_state_458 + hidden_state_454
        hidden_state_458 = hidden_state_454 = None
        unsqueeze_104 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_105 = unsqueeze_104.unsqueeze(-1)
        unsqueeze_104 = None
        hidden_state_460 = unsqueeze_105 * hidden_state_459
        unsqueeze_105 = hidden_state_459 = None
        hidden_state_461 = hidden_state_453 + hidden_state_460
        hidden_state_453 = hidden_state_460 = None
        hidden_state_462 = torch.nn.functional.batch_norm(
            hidden_state_461,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_463 = torch.conv2d(
            hidden_state_462,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_462 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_464 = torch.conv2d(
            hidden_state_463,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_463 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_465 = torch._C._nn.gelu(hidden_state_464)
        hidden_state_464 = None
        hidden_state_466 = torch.nn.functional.dropout(
            hidden_state_465, 0.0, False, False
        )
        hidden_state_465 = None
        hidden_state_467 = torch.conv2d(
            hidden_state_466,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_466 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_468 = torch.nn.functional.dropout(
            hidden_state_467, 0.0, False, False
        )
        hidden_state_467 = None
        unsqueeze_106 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_18_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_107 = unsqueeze_106.unsqueeze(-1)
        unsqueeze_106 = None
        hidden_state_469 = unsqueeze_107 * hidden_state_468
        unsqueeze_107 = hidden_state_468 = None
        hidden_state_470 = hidden_state_461 + hidden_state_469
        hidden_state_461 = hidden_state_469 = None
        hidden_state_471 = torch.nn.functional.batch_norm(
            hidden_state_470,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_pre_normomalization_parameters_bias_ = (None)
        input_55 = torch.conv2d(
            hidden_state_471,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_56 = torch._C._nn.gelu(input_55)
        input_55 = None
        hidden_state_472 = torch.conv2d(
            input_56,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_473 = torch.conv2d(
            hidden_state_472,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_472 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_474 = torch.conv2d(
            hidden_state_473,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_473 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_27 = input_56 * hidden_state_474
        input_56 = hidden_state_474 = None
        hidden_state_475 = torch.conv2d(
            attended_27,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_27 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_476 = hidden_state_475 + hidden_state_471
        hidden_state_475 = hidden_state_471 = None
        unsqueeze_108 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_109 = unsqueeze_108.unsqueeze(-1)
        unsqueeze_108 = None
        hidden_state_477 = unsqueeze_109 * hidden_state_476
        unsqueeze_109 = hidden_state_476 = None
        hidden_state_478 = hidden_state_470 + hidden_state_477
        hidden_state_470 = hidden_state_477 = None
        hidden_state_479 = torch.nn.functional.batch_norm(
            hidden_state_478,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_480 = torch.conv2d(
            hidden_state_479,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_479 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_481 = torch.conv2d(
            hidden_state_480,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_480 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_482 = torch._C._nn.gelu(hidden_state_481)
        hidden_state_481 = None
        hidden_state_483 = torch.nn.functional.dropout(
            hidden_state_482, 0.0, False, False
        )
        hidden_state_482 = None
        hidden_state_484 = torch.conv2d(
            hidden_state_483,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_483 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_485 = torch.nn.functional.dropout(
            hidden_state_484, 0.0, False, False
        )
        hidden_state_484 = None
        unsqueeze_110 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_19_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_111 = unsqueeze_110.unsqueeze(-1)
        unsqueeze_110 = None
        hidden_state_486 = unsqueeze_111 * hidden_state_485
        unsqueeze_111 = hidden_state_485 = None
        hidden_state_487 = hidden_state_478 + hidden_state_486
        hidden_state_478 = hidden_state_486 = None
        hidden_state_488 = torch.nn.functional.batch_norm(
            hidden_state_487,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_pre_normomalization_parameters_bias_ = (None)
        input_57 = torch.conv2d(
            hidden_state_488,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_58 = torch._C._nn.gelu(input_57)
        input_57 = None
        hidden_state_489 = torch.conv2d(
            input_58,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_490 = torch.conv2d(
            hidden_state_489,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_489 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_491 = torch.conv2d(
            hidden_state_490,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_490 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_28 = input_58 * hidden_state_491
        input_58 = hidden_state_491 = None
        hidden_state_492 = torch.conv2d(
            attended_28,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_28 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_493 = hidden_state_492 + hidden_state_488
        hidden_state_492 = hidden_state_488 = None
        unsqueeze_112 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_113 = unsqueeze_112.unsqueeze(-1)
        unsqueeze_112 = None
        hidden_state_494 = unsqueeze_113 * hidden_state_493
        unsqueeze_113 = hidden_state_493 = None
        hidden_state_495 = hidden_state_487 + hidden_state_494
        hidden_state_487 = hidden_state_494 = None
        hidden_state_496 = torch.nn.functional.batch_norm(
            hidden_state_495,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_497 = torch.conv2d(
            hidden_state_496,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_496 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_498 = torch.conv2d(
            hidden_state_497,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_497 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_499 = torch._C._nn.gelu(hidden_state_498)
        hidden_state_498 = None
        hidden_state_500 = torch.nn.functional.dropout(
            hidden_state_499, 0.0, False, False
        )
        hidden_state_499 = None
        hidden_state_501 = torch.conv2d(
            hidden_state_500,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_500 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_502 = torch.nn.functional.dropout(
            hidden_state_501, 0.0, False, False
        )
        hidden_state_501 = None
        unsqueeze_114 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_20_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_115 = unsqueeze_114.unsqueeze(-1)
        unsqueeze_114 = None
        hidden_state_503 = unsqueeze_115 * hidden_state_502
        unsqueeze_115 = hidden_state_502 = None
        hidden_state_504 = hidden_state_495 + hidden_state_503
        hidden_state_495 = hidden_state_503 = None
        hidden_state_505 = torch.nn.functional.batch_norm(
            hidden_state_504,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_pre_normomalization_parameters_bias_ = (None)
        input_59 = torch.conv2d(
            hidden_state_505,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_60 = torch._C._nn.gelu(input_59)
        input_59 = None
        hidden_state_506 = torch.conv2d(
            input_60,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_507 = torch.conv2d(
            hidden_state_506,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_506 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_508 = torch.conv2d(
            hidden_state_507,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_507 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_29 = input_60 * hidden_state_508
        input_60 = hidden_state_508 = None
        hidden_state_509 = torch.conv2d(
            attended_29,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_29 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_510 = hidden_state_509 + hidden_state_505
        hidden_state_509 = hidden_state_505 = None
        unsqueeze_116 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_117 = unsqueeze_116.unsqueeze(-1)
        unsqueeze_116 = None
        hidden_state_511 = unsqueeze_117 * hidden_state_510
        unsqueeze_117 = hidden_state_510 = None
        hidden_state_512 = hidden_state_504 + hidden_state_511
        hidden_state_504 = hidden_state_511 = None
        hidden_state_513 = torch.nn.functional.batch_norm(
            hidden_state_512,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_514 = torch.conv2d(
            hidden_state_513,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_513 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_515 = torch.conv2d(
            hidden_state_514,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_514 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_516 = torch._C._nn.gelu(hidden_state_515)
        hidden_state_515 = None
        hidden_state_517 = torch.nn.functional.dropout(
            hidden_state_516, 0.0, False, False
        )
        hidden_state_516 = None
        hidden_state_518 = torch.conv2d(
            hidden_state_517,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_517 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_519 = torch.nn.functional.dropout(
            hidden_state_518, 0.0, False, False
        )
        hidden_state_518 = None
        unsqueeze_118 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_21_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_119 = unsqueeze_118.unsqueeze(-1)
        unsqueeze_118 = None
        hidden_state_520 = unsqueeze_119 * hidden_state_519
        unsqueeze_119 = hidden_state_519 = None
        hidden_state_521 = hidden_state_512 + hidden_state_520
        hidden_state_512 = hidden_state_520 = None
        hidden_state_522 = torch.nn.functional.batch_norm(
            hidden_state_521,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_pre_normomalization_parameters_bias_ = (None)
        input_61 = torch.conv2d(
            hidden_state_522,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_62 = torch._C._nn.gelu(input_61)
        input_61 = None
        hidden_state_523 = torch.conv2d(
            input_62,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_524 = torch.conv2d(
            hidden_state_523,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_523 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_525 = torch.conv2d(
            hidden_state_524,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_524 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_30 = input_62 * hidden_state_525
        input_62 = hidden_state_525 = None
        hidden_state_526 = torch.conv2d(
            attended_30,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_30 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_527 = hidden_state_526 + hidden_state_522
        hidden_state_526 = hidden_state_522 = None
        unsqueeze_120 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_121 = unsqueeze_120.unsqueeze(-1)
        unsqueeze_120 = None
        hidden_state_528 = unsqueeze_121 * hidden_state_527
        unsqueeze_121 = hidden_state_527 = None
        hidden_state_529 = hidden_state_521 + hidden_state_528
        hidden_state_521 = hidden_state_528 = None
        hidden_state_530 = torch.nn.functional.batch_norm(
            hidden_state_529,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_531 = torch.conv2d(
            hidden_state_530,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_530 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_532 = torch.conv2d(
            hidden_state_531,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_531 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_533 = torch._C._nn.gelu(hidden_state_532)
        hidden_state_532 = None
        hidden_state_534 = torch.nn.functional.dropout(
            hidden_state_533, 0.0, False, False
        )
        hidden_state_533 = None
        hidden_state_535 = torch.conv2d(
            hidden_state_534,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_534 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_536 = torch.nn.functional.dropout(
            hidden_state_535, 0.0, False, False
        )
        hidden_state_535 = None
        unsqueeze_122 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_22_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_123 = unsqueeze_122.unsqueeze(-1)
        unsqueeze_122 = None
        hidden_state_537 = unsqueeze_123 * hidden_state_536
        unsqueeze_123 = hidden_state_536 = None
        hidden_state_538 = hidden_state_529 + hidden_state_537
        hidden_state_529 = hidden_state_537 = None
        hidden_state_539 = torch.nn.functional.batch_norm(
            hidden_state_538,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_pre_normomalization_parameters_bias_ = (None)
        input_63 = torch.conv2d(
            hidden_state_539,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_64 = torch._C._nn.gelu(input_63)
        input_63 = None
        hidden_state_540 = torch.conv2d(
            input_64,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_541 = torch.conv2d(
            hidden_state_540,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_540 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_542 = torch.conv2d(
            hidden_state_541,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_541 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_31 = input_64 * hidden_state_542
        input_64 = hidden_state_542 = None
        hidden_state_543 = torch.conv2d(
            attended_31,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_31 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_544 = hidden_state_543 + hidden_state_539
        hidden_state_543 = hidden_state_539 = None
        unsqueeze_124 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_125 = unsqueeze_124.unsqueeze(-1)
        unsqueeze_124 = None
        hidden_state_545 = unsqueeze_125 * hidden_state_544
        unsqueeze_125 = hidden_state_544 = None
        hidden_state_546 = hidden_state_538 + hidden_state_545
        hidden_state_538 = hidden_state_545 = None
        hidden_state_547 = torch.nn.functional.batch_norm(
            hidden_state_546,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_548 = torch.conv2d(
            hidden_state_547,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_547 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_549 = torch.conv2d(
            hidden_state_548,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_548 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_550 = torch._C._nn.gelu(hidden_state_549)
        hidden_state_549 = None
        hidden_state_551 = torch.nn.functional.dropout(
            hidden_state_550, 0.0, False, False
        )
        hidden_state_550 = None
        hidden_state_552 = torch.conv2d(
            hidden_state_551,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_551 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_553 = torch.nn.functional.dropout(
            hidden_state_552, 0.0, False, False
        )
        hidden_state_552 = None
        unsqueeze_126 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_23_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_127 = unsqueeze_126.unsqueeze(-1)
        unsqueeze_126 = None
        hidden_state_554 = unsqueeze_127 * hidden_state_553
        unsqueeze_127 = hidden_state_553 = None
        hidden_state_555 = hidden_state_546 + hidden_state_554
        hidden_state_546 = hidden_state_554 = None
        hidden_state_556 = torch.nn.functional.batch_norm(
            hidden_state_555,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_pre_normomalization_parameters_bias_ = (None)
        input_65 = torch.conv2d(
            hidden_state_556,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_66 = torch._C._nn.gelu(input_65)
        input_65 = None
        hidden_state_557 = torch.conv2d(
            input_66,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_558 = torch.conv2d(
            hidden_state_557,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_557 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_559 = torch.conv2d(
            hidden_state_558,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_558 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_32 = input_66 * hidden_state_559
        input_66 = hidden_state_559 = None
        hidden_state_560 = torch.conv2d(
            attended_32,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_32 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_561 = hidden_state_560 + hidden_state_556
        hidden_state_560 = hidden_state_556 = None
        unsqueeze_128 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_129 = unsqueeze_128.unsqueeze(-1)
        unsqueeze_128 = None
        hidden_state_562 = unsqueeze_129 * hidden_state_561
        unsqueeze_129 = hidden_state_561 = None
        hidden_state_563 = hidden_state_555 + hidden_state_562
        hidden_state_555 = hidden_state_562 = None
        hidden_state_564 = torch.nn.functional.batch_norm(
            hidden_state_563,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_565 = torch.conv2d(
            hidden_state_564,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_564 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_566 = torch.conv2d(
            hidden_state_565,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_565 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_567 = torch._C._nn.gelu(hidden_state_566)
        hidden_state_566 = None
        hidden_state_568 = torch.nn.functional.dropout(
            hidden_state_567, 0.0, False, False
        )
        hidden_state_567 = None
        hidden_state_569 = torch.conv2d(
            hidden_state_568,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_568 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_570 = torch.nn.functional.dropout(
            hidden_state_569, 0.0, False, False
        )
        hidden_state_569 = None
        unsqueeze_130 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_24_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_131 = unsqueeze_130.unsqueeze(-1)
        unsqueeze_130 = None
        hidden_state_571 = unsqueeze_131 * hidden_state_570
        unsqueeze_131 = hidden_state_570 = None
        hidden_state_572 = hidden_state_563 + hidden_state_571
        hidden_state_563 = hidden_state_571 = None
        hidden_state_573 = torch.nn.functional.batch_norm(
            hidden_state_572,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_pre_normomalization_parameters_bias_ = (None)
        input_67 = torch.conv2d(
            hidden_state_573,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_68 = torch._C._nn.gelu(input_67)
        input_67 = None
        hidden_state_574 = torch.conv2d(
            input_68,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_575 = torch.conv2d(
            hidden_state_574,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_574 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_576 = torch.conv2d(
            hidden_state_575,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_575 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_33 = input_68 * hidden_state_576
        input_68 = hidden_state_576 = None
        hidden_state_577 = torch.conv2d(
            attended_33,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_33 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_578 = hidden_state_577 + hidden_state_573
        hidden_state_577 = hidden_state_573 = None
        unsqueeze_132 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_133 = unsqueeze_132.unsqueeze(-1)
        unsqueeze_132 = None
        hidden_state_579 = unsqueeze_133 * hidden_state_578
        unsqueeze_133 = hidden_state_578 = None
        hidden_state_580 = hidden_state_572 + hidden_state_579
        hidden_state_572 = hidden_state_579 = None
        hidden_state_581 = torch.nn.functional.batch_norm(
            hidden_state_580,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_582 = torch.conv2d(
            hidden_state_581,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_581 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_583 = torch.conv2d(
            hidden_state_582,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_582 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_584 = torch._C._nn.gelu(hidden_state_583)
        hidden_state_583 = None
        hidden_state_585 = torch.nn.functional.dropout(
            hidden_state_584, 0.0, False, False
        )
        hidden_state_584 = None
        hidden_state_586 = torch.conv2d(
            hidden_state_585,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_585 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_587 = torch.nn.functional.dropout(
            hidden_state_586, 0.0, False, False
        )
        hidden_state_586 = None
        unsqueeze_134 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_25_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_135 = unsqueeze_134.unsqueeze(-1)
        unsqueeze_134 = None
        hidden_state_588 = unsqueeze_135 * hidden_state_587
        unsqueeze_135 = hidden_state_587 = None
        hidden_state_589 = hidden_state_580 + hidden_state_588
        hidden_state_580 = hidden_state_588 = None
        hidden_state_590 = torch.nn.functional.batch_norm(
            hidden_state_589,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_pre_normomalization_parameters_bias_ = (None)
        input_69 = torch.conv2d(
            hidden_state_590,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_70 = torch._C._nn.gelu(input_69)
        input_69 = None
        hidden_state_591 = torch.conv2d(
            input_70,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            320,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_592 = torch.conv2d(
            hidden_state_591,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            320,
        )
        hidden_state_591 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_593 = torch.conv2d(
            hidden_state_592,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_592 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_34 = input_70 * hidden_state_593
        input_70 = hidden_state_593 = None
        hidden_state_594 = torch.conv2d(
            attended_34,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_34 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_595 = hidden_state_594 + hidden_state_590
        hidden_state_594 = hidden_state_590 = None
        unsqueeze_136 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_137 = unsqueeze_136.unsqueeze(-1)
        unsqueeze_136 = None
        hidden_state_596 = unsqueeze_137 * hidden_state_595
        unsqueeze_137 = hidden_state_595 = None
        hidden_state_597 = hidden_state_589 + hidden_state_596
        hidden_state_589 = hidden_state_596 = None
        hidden_state_598 = torch.nn.functional.batch_norm(
            hidden_state_597,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_599 = torch.conv2d(
            hidden_state_598,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_598 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_600 = torch.conv2d(
            hidden_state_599,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            1280,
        )
        hidden_state_599 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_601 = torch._C._nn.gelu(hidden_state_600)
        hidden_state_600 = None
        hidden_state_602 = torch.nn.functional.dropout(
            hidden_state_601, 0.0, False, False
        )
        hidden_state_601 = None
        hidden_state_603 = torch.conv2d(
            hidden_state_602,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_602 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_604 = torch.nn.functional.dropout(
            hidden_state_603, 0.0, False, False
        )
        hidden_state_603 = None
        unsqueeze_138 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_26_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_139 = unsqueeze_138.unsqueeze(-1)
        unsqueeze_138 = None
        hidden_state_605 = unsqueeze_139 * hidden_state_604
        unsqueeze_139 = hidden_state_604 = None
        hidden_state_606 = hidden_state_597 + hidden_state_605
        hidden_state_597 = hidden_state_605 = None
        flatten_2 = hidden_state_606.flatten(2)
        hidden_state_606 = None
        hidden_state_607 = flatten_2.transpose(1, 2)
        flatten_2 = None
        hidden_state_608 = torch.nn.functional.layer_norm(
            hidden_state_607,
            (320,),
            l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_607 = l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_normalization_parameters_bias_ = (None)
        view_2 = hidden_state_608.view(1, 14, 14, 320)
        hidden_state_608 = None
        hidden_state_609 = view_2.permute(0, 3, 1, 2)
        view_2 = None
        hidden_state_610 = torch.conv2d(
            hidden_state_609,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_609 = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_convolution_parameters_bias_ = (None)
        hidden_state_611 = torch.nn.functional.batch_norm(
            hidden_state_610,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_610 = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_embeddings_modules_normalization_parameters_bias_ = (None)
        hidden_state_612 = torch.nn.functional.batch_norm(
            hidden_state_611,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_pre_normomalization_parameters_bias_ = (None)
        input_71 = torch.conv2d(
            hidden_state_612,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_72 = torch._C._nn.gelu(input_71)
        input_71 = None
        hidden_state_613 = torch.conv2d(
            input_72,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            512,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_614 = torch.conv2d(
            hidden_state_613,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            512,
        )
        hidden_state_613 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_615 = torch.conv2d(
            hidden_state_614,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_614 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_35 = input_72 * hidden_state_615
        input_72 = hidden_state_615 = None
        hidden_state_616 = torch.conv2d(
            attended_35,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_35 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_617 = hidden_state_616 + hidden_state_612
        hidden_state_616 = hidden_state_612 = None
        unsqueeze_140 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_141 = unsqueeze_140.unsqueeze(-1)
        unsqueeze_140 = None
        hidden_state_618 = unsqueeze_141 * hidden_state_617
        unsqueeze_141 = hidden_state_617 = None
        hidden_state_619 = hidden_state_611 + hidden_state_618
        hidden_state_611 = hidden_state_618 = None
        hidden_state_620 = torch.nn.functional.batch_norm(
            hidden_state_619,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_621 = torch.conv2d(
            hidden_state_620,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_620 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_622 = torch.conv2d(
            hidden_state_621,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            2048,
        )
        hidden_state_621 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_623 = torch._C._nn.gelu(hidden_state_622)
        hidden_state_622 = None
        hidden_state_624 = torch.nn.functional.dropout(
            hidden_state_623, 0.0, False, False
        )
        hidden_state_623 = None
        hidden_state_625 = torch.conv2d(
            hidden_state_624,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_624 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_626 = torch.nn.functional.dropout(
            hidden_state_625, 0.0, False, False
        )
        hidden_state_625 = None
        unsqueeze_142 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_0_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_143 = unsqueeze_142.unsqueeze(-1)
        unsqueeze_142 = None
        hidden_state_627 = unsqueeze_143 * hidden_state_626
        unsqueeze_143 = hidden_state_626 = None
        hidden_state_628 = hidden_state_619 + hidden_state_627
        hidden_state_619 = hidden_state_627 = None
        hidden_state_629 = torch.nn.functional.batch_norm(
            hidden_state_628,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_pre_normomalization_parameters_bias_ = (None)
        input_73 = torch.conv2d(
            hidden_state_629,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_74 = torch._C._nn.gelu(input_73)
        input_73 = None
        hidden_state_630 = torch.conv2d(
            input_74,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            512,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_631 = torch.conv2d(
            hidden_state_630,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            512,
        )
        hidden_state_630 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_632 = torch.conv2d(
            hidden_state_631,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_631 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_36 = input_74 * hidden_state_632
        input_74 = hidden_state_632 = None
        hidden_state_633 = torch.conv2d(
            attended_36,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_36 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_634 = hidden_state_633 + hidden_state_629
        hidden_state_633 = hidden_state_629 = None
        unsqueeze_144 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_145 = unsqueeze_144.unsqueeze(-1)
        unsqueeze_144 = None
        hidden_state_635 = unsqueeze_145 * hidden_state_634
        unsqueeze_145 = hidden_state_634 = None
        hidden_state_636 = hidden_state_628 + hidden_state_635
        hidden_state_628 = hidden_state_635 = None
        hidden_state_637 = torch.nn.functional.batch_norm(
            hidden_state_636,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_638 = torch.conv2d(
            hidden_state_637,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_637 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_639 = torch.conv2d(
            hidden_state_638,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            2048,
        )
        hidden_state_638 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_640 = torch._C._nn.gelu(hidden_state_639)
        hidden_state_639 = None
        hidden_state_641 = torch.nn.functional.dropout(
            hidden_state_640, 0.0, False, False
        )
        hidden_state_640 = None
        hidden_state_642 = torch.conv2d(
            hidden_state_641,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_641 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_643 = torch.nn.functional.dropout(
            hidden_state_642, 0.0, False, False
        )
        hidden_state_642 = None
        unsqueeze_146 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_1_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_147 = unsqueeze_146.unsqueeze(-1)
        unsqueeze_146 = None
        hidden_state_644 = unsqueeze_147 * hidden_state_643
        unsqueeze_147 = hidden_state_643 = None
        hidden_state_645 = hidden_state_636 + hidden_state_644
        hidden_state_636 = hidden_state_644 = None
        hidden_state_646 = torch.nn.functional.batch_norm(
            hidden_state_645,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_pre_normomalization_parameters_bias_ = (None)
        input_75 = torch.conv2d(
            hidden_state_646,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_pre_projection_modules_conv_parameters_bias_ = (None)
        input_76 = torch._C._nn.gelu(input_75)
        input_75 = None
        hidden_state_647 = torch.conv2d(
            input_76,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_,
            (1, 1),
            (2, 2),
            (1, 1),
            512,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_648 = torch.conv2d(
            hidden_state_647,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_,
            (1, 1),
            (9, 9),
            (3, 3),
            512,
        )
        hidden_state_647 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_depth_wise_dilated_parameters_bias_ = (None)
        hidden_state_649 = torch.conv2d(
            hidden_state_648,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_648 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_attention_layer_modules_attention_modules_point_wise_parameters_bias_ = (None)
        attended_37 = input_76 * hidden_state_649
        input_76 = hidden_state_649 = None
        hidden_state_650 = torch.conv2d(
            attended_37,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        attended_37 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_modules_post_projection_parameters_bias_ = (None)
        hidden_state_651 = hidden_state_650 + hidden_state_646
        hidden_state_650 = hidden_state_646 = None
        unsqueeze_148 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_attention_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_149 = unsqueeze_148.unsqueeze(-1)
        unsqueeze_148 = None
        hidden_state_652 = unsqueeze_149 * hidden_state_651
        unsqueeze_149 = hidden_state_651 = None
        hidden_state_653 = hidden_state_645 + hidden_state_652
        hidden_state_645 = hidden_state_652 = None
        hidden_state_654 = torch.nn.functional.batch_norm(
            hidden_state_653,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_post_normalization_parameters_bias_ = (None)
        hidden_state_655 = torch.conv2d(
            hidden_state_654,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_654 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_in_dense_parameters_bias_ = (None)
        hidden_state_656 = torch.conv2d(
            hidden_state_655,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_,
            (1, 1),
            (1, 1),
            (1, 1),
            2048,
        )
        hidden_state_655 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_depth_wise_parameters_bias_ = (None)
        hidden_state_657 = torch._C._nn.gelu(hidden_state_656)
        hidden_state_656 = None
        hidden_state_658 = torch.nn.functional.dropout(
            hidden_state_657, 0.0, False, False
        )
        hidden_state_657 = None
        hidden_state_659 = torch.conv2d(
            hidden_state_658,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_,
            (1, 1),
            (0, 0),
            (1, 1),
            1,
        )
        hidden_state_658 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_modules_out_dense_parameters_bias_ = (None)
        hidden_state_660 = torch.nn.functional.dropout(
            hidden_state_659, 0.0, False, False
        )
        hidden_state_659 = None
        unsqueeze_150 = l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_.unsqueeze(
            -1
        )
        l_self_modules_encoder_modules_stages_modules_3_modules_layers_modules_2_modules_mlp_scaling_parameters_weight_ = (
            None
        )
        unsqueeze_151 = unsqueeze_150.unsqueeze(-1)
        unsqueeze_150 = None
        hidden_state_661 = unsqueeze_151 * hidden_state_660
        unsqueeze_151 = hidden_state_660 = None
        hidden_state_662 = hidden_state_653 + hidden_state_661
        hidden_state_653 = hidden_state_661 = None
        flatten_3 = hidden_state_662.flatten(2)
        hidden_state_662 = None
        hidden_state_663 = flatten_3.transpose(1, 2)
        flatten_3 = None
        hidden_state_664 = torch.nn.functional.layer_norm(
            hidden_state_663,
            (512,),
            l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_,
            1e-06,
        )
        hidden_state_663 = l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_3_modules_normalization_parameters_bias_ = (None)
        view_3 = hidden_state_664.view(1, 7, 7, 512)
        hidden_state_664 = None
        hidden_state_665 = view_3.permute(0, 3, 1, 2)
        view_3 = None
        pooled_output = hidden_state_665.mean(dim=[-2, -1])
        return (hidden_state_665, pooled_output)
