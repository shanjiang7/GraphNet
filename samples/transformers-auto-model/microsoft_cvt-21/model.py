import torch


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_pixel_values_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_pixel_values_ = L_pixel_values_
        l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_ = (
            L_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_bias_
        pixel_values = torch.conv2d(
            l_pixel_values_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_,
            (4, 4),
            (2, 2),
            (1, 1),
            1,
        )
        l_pixel_values_ = l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = (None)
        view = pixel_values.view(1, 64, 3136)
        pixel_values = None
        pixel_values_1 = view.permute(0, 2, 1)
        view = None
        pixel_values_2 = torch.nn.functional.layer_norm(
            pixel_values_1,
            (64,),
            l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_,
            1e-05,
        )
        pixel_values_1 = l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = (None)
        permute_1 = pixel_values_2.permute(0, 2, 1)
        pixel_values_2 = None
        pixel_values_3 = permute_1.view(1, 64, 56, 56)
        permute_1 = None
        hidden_state = torch.nn.functional.dropout(pixel_values_3, 0.0, False, False)
        pixel_values_3 = None
        view_2 = hidden_state.view(1, 64, 3136)
        hidden_state = None
        hidden_state_1 = view_2.permute(0, 2, 1)
        view_2 = None
        layer_norm_1 = torch.nn.functional.layer_norm(
            hidden_state_1,
            (64,),
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = (None)
        permute_3 = layer_norm_1.permute(0, 2, 1)
        layer_norm_1 = None
        hidden_state_2 = permute_3.view(1, 64, 56, 56)
        permute_3 = None
        hidden_state_3 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_4 = torch.nn.functional.batch_norm(
            hidden_state_3,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_3 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_4 = hidden_state_4.view(1, 64, 784)
        hidden_state_4 = None
        hidden_state_5 = view_4.permute(0, 2, 1)
        view_4 = None
        hidden_state_6 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_7 = torch.nn.functional.batch_norm(
            hidden_state_6,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_6 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_5 = hidden_state_7.view(1, 64, 3136)
        hidden_state_7 = None
        hidden_state_8 = view_5.permute(0, 2, 1)
        view_5 = None
        hidden_state_9 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            64,
        )
        hidden_state_2 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_10 = torch.nn.functional.batch_norm(
            hidden_state_9,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_9 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_6 = hidden_state_10.view(1, 64, 784)
        hidden_state_10 = None
        hidden_state_11 = view_6.permute(0, 2, 1)
        view_6 = None
        linear = torch._C._nn.linear(
            hidden_state_8,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        hidden_state_8 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_7 = linear.view(1, 3136, 1, 64)
        linear = None
        query = view_7.permute(0, 2, 1, 3)
        view_7 = None
        linear_1 = torch._C._nn.linear(
            hidden_state_5,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        hidden_state_5 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_8 = linear_1.view(1, 784, 1, 64)
        linear_1 = None
        key = view_8.permute(0, 2, 1, 3)
        view_8 = None
        linear_2 = torch._C._nn.linear(
            hidden_state_11,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        hidden_state_11 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_9 = linear_2.view(1, 784, 1, 64)
        linear_2 = None
        value = view_9.permute(0, 2, 1, 3)
        view_9 = None
        einsum = torch.functional.einsum("bhlk,bhtk->bhlt", [query, key])
        query = key = None
        attention_score = einsum * 0.125
        einsum = None
        attention_probs = torch.nn.functional.softmax(attention_score, dim=-1)
        attention_score = None
        attention_probs_1 = torch.nn.functional.dropout(
            attention_probs, 0.0, False, False
        )
        attention_probs = None
        context = torch.functional.einsum("bhlt,bhtv->bhlv", [attention_probs_1, value])
        attention_probs_1 = value = None
        permute_10 = context.permute(0, 2, 1, 3)
        context = None
        contiguous = permute_10.contiguous()
        permute_10 = None
        context_1 = contiguous.view(1, 3136, 64)
        contiguous = None
        hidden_state_12 = torch._C._nn.linear(
            context_1,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_1 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_13 = torch.nn.functional.dropout(
            hidden_state_12, 0.0, False, False
        )
        hidden_state_12 = None
        hidden_state_14 = hidden_state_13 + hidden_state_1
        hidden_state_13 = hidden_state_1 = None
        layer_output = torch.nn.functional.layer_norm(
            hidden_state_14,
            (64,),
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_15 = torch._C._nn.linear(
            layer_output,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_16 = torch._C._nn.gelu(hidden_state_15, approximate="none")
        hidden_state_15 = None
        hidden_state_17 = torch._C._nn.linear(
            hidden_state_16,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_16 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_18 = torch.nn.functional.dropout(
            hidden_state_17, 0.0, False, False
        )
        hidden_state_17 = None
        hidden_state_19 = hidden_state_18 + hidden_state_14
        hidden_state_18 = hidden_state_14 = None
        permute_11 = hidden_state_19.permute(0, 2, 1)
        hidden_state_19 = None
        hidden_state_20 = permute_11.view(1, 64, 56, 56)
        permute_11 = None
        pixel_values_4 = torch.conv2d(
            hidden_state_20,
            l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_20 = l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = (None)
        view_12 = pixel_values_4.view(1, 192, 784)
        pixel_values_4 = None
        pixel_values_5 = view_12.permute(0, 2, 1)
        view_12 = None
        pixel_values_6 = torch.nn.functional.layer_norm(
            pixel_values_5,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_,
            1e-05,
        )
        pixel_values_5 = l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = (None)
        permute_13 = pixel_values_6.permute(0, 2, 1)
        pixel_values_6 = None
        pixel_values_7 = permute_13.view(1, 192, 28, 28)
        permute_13 = None
        hidden_state_21 = torch.nn.functional.dropout(pixel_values_7, 0.0, False, False)
        pixel_values_7 = None
        view_14 = hidden_state_21.view(1, 192, 784)
        hidden_state_21 = None
        hidden_state_22 = view_14.permute(0, 2, 1)
        view_14 = None
        layer_norm_4 = torch.nn.functional.layer_norm(
            hidden_state_22,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = (None)
        permute_15 = layer_norm_4.permute(0, 2, 1)
        layer_norm_4 = None
        hidden_state_23 = permute_15.view(1, 192, 28, 28)
        permute_15 = None
        hidden_state_24 = torch.conv2d(
            hidden_state_23,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_25 = torch.nn.functional.batch_norm(
            hidden_state_24,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_24 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_16 = hidden_state_25.view(1, 192, 196)
        hidden_state_25 = None
        hidden_state_26 = view_16.permute(0, 2, 1)
        view_16 = None
        hidden_state_27 = torch.conv2d(
            hidden_state_23,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_28 = torch.nn.functional.batch_norm(
            hidden_state_27,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_27 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_17 = hidden_state_28.view(1, 192, 784)
        hidden_state_28 = None
        hidden_state_29 = view_17.permute(0, 2, 1)
        view_17 = None
        hidden_state_30 = torch.conv2d(
            hidden_state_23,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        hidden_state_23 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_31 = torch.nn.functional.batch_norm(
            hidden_state_30,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_30 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_18 = hidden_state_31.view(1, 192, 196)
        hidden_state_31 = None
        hidden_state_32 = view_18.permute(0, 2, 1)
        view_18 = None
        linear_6 = torch._C._nn.linear(
            hidden_state_29,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        hidden_state_29 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_19 = linear_6.view(1, 784, 3, 64)
        linear_6 = None
        query_1 = view_19.permute(0, 2, 1, 3)
        view_19 = None
        linear_7 = torch._C._nn.linear(
            hidden_state_26,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        hidden_state_26 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_20 = linear_7.view(1, 196, 3, 64)
        linear_7 = None
        key_1 = view_20.permute(0, 2, 1, 3)
        view_20 = None
        linear_8 = torch._C._nn.linear(
            hidden_state_32,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        hidden_state_32 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_21 = linear_8.view(1, 196, 3, 64)
        linear_8 = None
        value_1 = view_21.permute(0, 2, 1, 3)
        view_21 = None
        einsum_2 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_1, key_1])
        query_1 = key_1 = None
        attention_score_1 = einsum_2 * 0.07216878364870322
        einsum_2 = None
        attention_probs_2 = torch.nn.functional.softmax(attention_score_1, dim=-1)
        attention_score_1 = None
        attention_probs_3 = torch.nn.functional.dropout(
            attention_probs_2, 0.0, False, False
        )
        attention_probs_2 = None
        context_2 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_3, value_1]
        )
        attention_probs_3 = value_1 = None
        permute_22 = context_2.permute(0, 2, 1, 3)
        context_2 = None
        contiguous_1 = permute_22.contiguous()
        permute_22 = None
        context_3 = contiguous_1.view(1, 784, 192)
        contiguous_1 = None
        hidden_state_33 = torch._C._nn.linear(
            context_3,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_3 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_34 = torch.nn.functional.dropout(
            hidden_state_33, 0.0, False, False
        )
        hidden_state_33 = None
        hidden_state_35 = hidden_state_34 + hidden_state_22
        hidden_state_34 = hidden_state_22 = None
        layer_output_1 = torch.nn.functional.layer_norm(
            hidden_state_35,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_36 = torch._C._nn.linear(
            layer_output_1,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_1 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_37 = torch._C._nn.gelu(hidden_state_36, approximate="none")
        hidden_state_36 = None
        hidden_state_38 = torch._C._nn.linear(
            hidden_state_37,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_37 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_39 = torch.nn.functional.dropout(
            hidden_state_38, 0.0, False, False
        )
        hidden_state_38 = None
        hidden_state_40 = hidden_state_39 + hidden_state_35
        hidden_state_39 = hidden_state_35 = None
        layer_norm_6 = torch.nn.functional.layer_norm(
            hidden_state_40,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_ = (None)
        permute_23 = layer_norm_6.permute(0, 2, 1)
        layer_norm_6 = None
        hidden_state_41 = permute_23.view(1, 192, 28, 28)
        permute_23 = None
        hidden_state_42 = torch.conv2d(
            hidden_state_41,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_43 = torch.nn.functional.batch_norm(
            hidden_state_42,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_42 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_24 = hidden_state_43.view(1, 192, 196)
        hidden_state_43 = None
        hidden_state_44 = view_24.permute(0, 2, 1)
        view_24 = None
        hidden_state_45 = torch.conv2d(
            hidden_state_41,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_46 = torch.nn.functional.batch_norm(
            hidden_state_45,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_45 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_25 = hidden_state_46.view(1, 192, 784)
        hidden_state_46 = None
        hidden_state_47 = view_25.permute(0, 2, 1)
        view_25 = None
        hidden_state_48 = torch.conv2d(
            hidden_state_41,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        hidden_state_41 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_49 = torch.nn.functional.batch_norm(
            hidden_state_48,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_48 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_26 = hidden_state_49.view(1, 192, 196)
        hidden_state_49 = None
        hidden_state_50 = view_26.permute(0, 2, 1)
        view_26 = None
        linear_12 = torch._C._nn.linear(
            hidden_state_47,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        hidden_state_47 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_27 = linear_12.view(1, 784, 3, 64)
        linear_12 = None
        query_2 = view_27.permute(0, 2, 1, 3)
        view_27 = None
        linear_13 = torch._C._nn.linear(
            hidden_state_44,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        hidden_state_44 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_28 = linear_13.view(1, 196, 3, 64)
        linear_13 = None
        key_2 = view_28.permute(0, 2, 1, 3)
        view_28 = None
        linear_14 = torch._C._nn.linear(
            hidden_state_50,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        hidden_state_50 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_29 = linear_14.view(1, 196, 3, 64)
        linear_14 = None
        value_2 = view_29.permute(0, 2, 1, 3)
        view_29 = None
        einsum_4 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_2, key_2])
        query_2 = key_2 = None
        attention_score_2 = einsum_4 * 0.07216878364870322
        einsum_4 = None
        attention_probs_4 = torch.nn.functional.softmax(attention_score_2, dim=-1)
        attention_score_2 = None
        attention_probs_5 = torch.nn.functional.dropout(
            attention_probs_4, 0.0, False, False
        )
        attention_probs_4 = None
        context_4 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_5, value_2]
        )
        attention_probs_5 = value_2 = None
        permute_30 = context_4.permute(0, 2, 1, 3)
        context_4 = None
        contiguous_2 = permute_30.contiguous()
        permute_30 = None
        context_5 = contiguous_2.view(1, 784, 192)
        contiguous_2 = None
        hidden_state_51 = torch._C._nn.linear(
            context_5,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_5 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_52 = torch.nn.functional.dropout(
            hidden_state_51, 0.0, False, False
        )
        hidden_state_51 = None
        hidden_state_53 = hidden_state_52 + hidden_state_40
        hidden_state_52 = hidden_state_40 = None
        layer_output_2 = torch.nn.functional.layer_norm(
            hidden_state_53,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_54 = torch._C._nn.linear(
            layer_output_2,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_2 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_55 = torch._C._nn.gelu(hidden_state_54, approximate="none")
        hidden_state_54 = None
        hidden_state_56 = torch._C._nn.linear(
            hidden_state_55,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_55 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_57 = torch.nn.functional.dropout(
            hidden_state_56, 0.0, False, False
        )
        hidden_state_56 = None
        hidden_state_58 = hidden_state_57 + hidden_state_53
        hidden_state_57 = hidden_state_53 = None
        layer_norm_8 = torch.nn.functional.layer_norm(
            hidden_state_58,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_before_parameters_bias_ = (None)
        permute_31 = layer_norm_8.permute(0, 2, 1)
        layer_norm_8 = None
        hidden_state_59 = permute_31.view(1, 192, 28, 28)
        permute_31 = None
        hidden_state_60 = torch.conv2d(
            hidden_state_59,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_61 = torch.nn.functional.batch_norm(
            hidden_state_60,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_60 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_32 = hidden_state_61.view(1, 192, 196)
        hidden_state_61 = None
        hidden_state_62 = view_32.permute(0, 2, 1)
        view_32 = None
        hidden_state_63 = torch.conv2d(
            hidden_state_59,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_64 = torch.nn.functional.batch_norm(
            hidden_state_63,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_63 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_33 = hidden_state_64.view(1, 192, 784)
        hidden_state_64 = None
        hidden_state_65 = view_33.permute(0, 2, 1)
        view_33 = None
        hidden_state_66 = torch.conv2d(
            hidden_state_59,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        hidden_state_59 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_67 = torch.nn.functional.batch_norm(
            hidden_state_66,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_66 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_34 = hidden_state_67.view(1, 192, 196)
        hidden_state_67 = None
        hidden_state_68 = view_34.permute(0, 2, 1)
        view_34 = None
        linear_18 = torch._C._nn.linear(
            hidden_state_65,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        hidden_state_65 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_35 = linear_18.view(1, 784, 3, 64)
        linear_18 = None
        query_3 = view_35.permute(0, 2, 1, 3)
        view_35 = None
        linear_19 = torch._C._nn.linear(
            hidden_state_62,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        hidden_state_62 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_36 = linear_19.view(1, 196, 3, 64)
        linear_19 = None
        key_3 = view_36.permute(0, 2, 1, 3)
        view_36 = None
        linear_20 = torch._C._nn.linear(
            hidden_state_68,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        hidden_state_68 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_37 = linear_20.view(1, 196, 3, 64)
        linear_20 = None
        value_3 = view_37.permute(0, 2, 1, 3)
        view_37 = None
        einsum_6 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_3, key_3])
        query_3 = key_3 = None
        attention_score_3 = einsum_6 * 0.07216878364870322
        einsum_6 = None
        attention_probs_6 = torch.nn.functional.softmax(attention_score_3, dim=-1)
        attention_score_3 = None
        attention_probs_7 = torch.nn.functional.dropout(
            attention_probs_6, 0.0, False, False
        )
        attention_probs_6 = None
        context_6 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_7, value_3]
        )
        attention_probs_7 = value_3 = None
        permute_38 = context_6.permute(0, 2, 1, 3)
        context_6 = None
        contiguous_3 = permute_38.contiguous()
        permute_38 = None
        context_7 = contiguous_3.view(1, 784, 192)
        contiguous_3 = None
        hidden_state_69 = torch._C._nn.linear(
            context_7,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_7 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_70 = torch.nn.functional.dropout(
            hidden_state_69, 0.0, False, False
        )
        hidden_state_69 = None
        hidden_state_71 = hidden_state_70 + hidden_state_58
        hidden_state_70 = hidden_state_58 = None
        layer_output_3 = torch.nn.functional.layer_norm(
            hidden_state_71,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_72 = torch._C._nn.linear(
            layer_output_3,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_3 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_73 = torch._C._nn.gelu(hidden_state_72, approximate="none")
        hidden_state_72 = None
        hidden_state_74 = torch._C._nn.linear(
            hidden_state_73,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_73 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_75 = torch.nn.functional.dropout(
            hidden_state_74, 0.0, False, False
        )
        hidden_state_74 = None
        hidden_state_76 = hidden_state_75 + hidden_state_71
        hidden_state_75 = hidden_state_71 = None
        layer_norm_10 = torch.nn.functional.layer_norm(
            hidden_state_76,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_before_parameters_bias_ = (None)
        permute_39 = layer_norm_10.permute(0, 2, 1)
        layer_norm_10 = None
        hidden_state_77 = permute_39.view(1, 192, 28, 28)
        permute_39 = None
        hidden_state_78 = torch.conv2d(
            hidden_state_77,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_79 = torch.nn.functional.batch_norm(
            hidden_state_78,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_78 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_40 = hidden_state_79.view(1, 192, 196)
        hidden_state_79 = None
        hidden_state_80 = view_40.permute(0, 2, 1)
        view_40 = None
        hidden_state_81 = torch.conv2d(
            hidden_state_77,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_82 = torch.nn.functional.batch_norm(
            hidden_state_81,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_81 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_41 = hidden_state_82.view(1, 192, 784)
        hidden_state_82 = None
        hidden_state_83 = view_41.permute(0, 2, 1)
        view_41 = None
        hidden_state_84 = torch.conv2d(
            hidden_state_77,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        hidden_state_77 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_85 = torch.nn.functional.batch_norm(
            hidden_state_84,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_84 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_42 = hidden_state_85.view(1, 192, 196)
        hidden_state_85 = None
        hidden_state_86 = view_42.permute(0, 2, 1)
        view_42 = None
        linear_24 = torch._C._nn.linear(
            hidden_state_83,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        hidden_state_83 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_43 = linear_24.view(1, 784, 3, 64)
        linear_24 = None
        query_4 = view_43.permute(0, 2, 1, 3)
        view_43 = None
        linear_25 = torch._C._nn.linear(
            hidden_state_80,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        hidden_state_80 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_44 = linear_25.view(1, 196, 3, 64)
        linear_25 = None
        key_4 = view_44.permute(0, 2, 1, 3)
        view_44 = None
        linear_26 = torch._C._nn.linear(
            hidden_state_86,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        hidden_state_86 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_45 = linear_26.view(1, 196, 3, 64)
        linear_26 = None
        value_4 = view_45.permute(0, 2, 1, 3)
        view_45 = None
        einsum_8 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_4, key_4])
        query_4 = key_4 = None
        attention_score_4 = einsum_8 * 0.07216878364870322
        einsum_8 = None
        attention_probs_8 = torch.nn.functional.softmax(attention_score_4, dim=-1)
        attention_score_4 = None
        attention_probs_9 = torch.nn.functional.dropout(
            attention_probs_8, 0.0, False, False
        )
        attention_probs_8 = None
        context_8 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_9, value_4]
        )
        attention_probs_9 = value_4 = None
        permute_46 = context_8.permute(0, 2, 1, 3)
        context_8 = None
        contiguous_4 = permute_46.contiguous()
        permute_46 = None
        context_9 = contiguous_4.view(1, 784, 192)
        contiguous_4 = None
        hidden_state_87 = torch._C._nn.linear(
            context_9,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_9 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_88 = torch.nn.functional.dropout(
            hidden_state_87, 0.0, False, False
        )
        hidden_state_87 = None
        hidden_state_89 = hidden_state_88 + hidden_state_76
        hidden_state_88 = hidden_state_76 = None
        layer_output_4 = torch.nn.functional.layer_norm(
            hidden_state_89,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_90 = torch._C._nn.linear(
            layer_output_4,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_4 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_91 = torch._C._nn.gelu(hidden_state_90, approximate="none")
        hidden_state_90 = None
        hidden_state_92 = torch._C._nn.linear(
            hidden_state_91,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_91 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_93 = torch.nn.functional.dropout(
            hidden_state_92, 0.0, False, False
        )
        hidden_state_92 = None
        hidden_state_94 = hidden_state_93 + hidden_state_89
        hidden_state_93 = hidden_state_89 = None
        permute_47 = hidden_state_94.permute(0, 2, 1)
        hidden_state_94 = None
        hidden_state_95 = permute_47.view(1, 192, 28, 28)
        permute_47 = None
        pixel_values_8 = torch.conv2d(
            hidden_state_95,
            l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_95 = l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = (None)
        view_48 = pixel_values_8.view(1, 384, 196)
        pixel_values_8 = None
        pixel_values_9 = view_48.permute(0, 2, 1)
        view_48 = None
        pixel_values_10 = torch.nn.functional.layer_norm(
            pixel_values_9,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_,
            1e-05,
        )
        pixel_values_9 = l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = (None)
        permute_49 = pixel_values_10.permute(0, 2, 1)
        pixel_values_10 = None
        pixel_values_11 = permute_49.view(1, 384, 14, 14)
        permute_49 = None
        hidden_state_96 = torch.nn.functional.dropout(
            pixel_values_11, 0.0, False, False
        )
        pixel_values_11 = None
        view_50 = hidden_state_96.view(1, 384, 196)
        hidden_state_96 = None
        hidden_state_97 = view_50.permute(0, 2, 1)
        view_50 = None
        cls_token = l_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_.expand(
            1, -1, -1
        )
        l_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_ = None
        hidden_state_98 = torch.cat((cls_token, hidden_state_97), dim=1)
        cls_token = hidden_state_97 = None
        layer_norm_13 = torch.nn.functional.layer_norm(
            hidden_state_98,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = (None)
        split = torch.functional.split(layer_norm_13, [1, 196], 1)
        layer_norm_13 = None
        cls_token_1 = split[0]
        hidden_state_99 = split[1]
        split = None
        permute_51 = hidden_state_99.permute(0, 2, 1)
        hidden_state_99 = None
        hidden_state_100 = permute_51.view(1, 384, 14, 14)
        permute_51 = None
        hidden_state_101 = torch.conv2d(
            hidden_state_100,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_102 = torch.nn.functional.batch_norm(
            hidden_state_101,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_101 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_52 = hidden_state_102.view(1, 384, 49)
        hidden_state_102 = None
        hidden_state_103 = view_52.permute(0, 2, 1)
        view_52 = None
        hidden_state_104 = torch.conv2d(
            hidden_state_100,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_105 = torch.nn.functional.batch_norm(
            hidden_state_104,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_104 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_53 = hidden_state_105.view(1, 384, 196)
        hidden_state_105 = None
        hidden_state_106 = view_53.permute(0, 2, 1)
        view_53 = None
        hidden_state_107 = torch.conv2d(
            hidden_state_100,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_100 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_108 = torch.nn.functional.batch_norm(
            hidden_state_107,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_107 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_54 = hidden_state_108.view(1, 384, 49)
        hidden_state_108 = None
        hidden_state_109 = view_54.permute(0, 2, 1)
        view_54 = None
        query_5 = torch.cat((cls_token_1, hidden_state_106), dim=1)
        hidden_state_106 = None
        key_5 = torch.cat((cls_token_1, hidden_state_103), dim=1)
        hidden_state_103 = None
        value_5 = torch.cat((cls_token_1, hidden_state_109), dim=1)
        cls_token_1 = hidden_state_109 = None
        linear_30 = torch._C._nn.linear(
            query_5,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_5 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_55 = linear_30.view(1, 197, 6, 64)
        linear_30 = None
        query_6 = view_55.permute(0, 2, 1, 3)
        view_55 = None
        linear_31 = torch._C._nn.linear(
            key_5,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_5 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_56 = linear_31.view(1, 50, 6, 64)
        linear_31 = None
        key_6 = view_56.permute(0, 2, 1, 3)
        view_56 = None
        linear_32 = torch._C._nn.linear(
            value_5,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_5 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_57 = linear_32.view(1, 50, 6, 64)
        linear_32 = None
        value_6 = view_57.permute(0, 2, 1, 3)
        view_57 = None
        einsum_10 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_6, key_6])
        query_6 = key_6 = None
        attention_score_5 = einsum_10 * 0.05103103630798288
        einsum_10 = None
        attention_probs_10 = torch.nn.functional.softmax(attention_score_5, dim=-1)
        attention_score_5 = None
        attention_probs_11 = torch.nn.functional.dropout(
            attention_probs_10, 0.0, False, False
        )
        attention_probs_10 = None
        context_10 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_11, value_6]
        )
        attention_probs_11 = value_6 = None
        permute_58 = context_10.permute(0, 2, 1, 3)
        context_10 = None
        contiguous_5 = permute_58.contiguous()
        permute_58 = None
        context_11 = contiguous_5.view(1, 197, 384)
        contiguous_5 = None
        hidden_state_110 = torch._C._nn.linear(
            context_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_111 = torch.nn.functional.dropout(
            hidden_state_110, 0.0, False, False
        )
        hidden_state_110 = None
        hidden_state_112 = hidden_state_111 + hidden_state_98
        hidden_state_111 = hidden_state_98 = None
        layer_output_5 = torch.nn.functional.layer_norm(
            hidden_state_112,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_113 = torch._C._nn.linear(
            layer_output_5,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_5 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_114 = torch._C._nn.gelu(hidden_state_113, approximate="none")
        hidden_state_113 = None
        hidden_state_115 = torch._C._nn.linear(
            hidden_state_114,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_114 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_116 = torch.nn.functional.dropout(
            hidden_state_115, 0.0, False, False
        )
        hidden_state_115 = None
        hidden_state_117 = hidden_state_116 + hidden_state_112
        hidden_state_116 = hidden_state_112 = None
        layer_norm_15 = torch.nn.functional.layer_norm(
            hidden_state_117,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_ = (None)
        split_1 = torch.functional.split(layer_norm_15, [1, 196], 1)
        layer_norm_15 = None
        cls_token_2 = split_1[0]
        hidden_state_118 = split_1[1]
        split_1 = None
        permute_59 = hidden_state_118.permute(0, 2, 1)
        hidden_state_118 = None
        hidden_state_119 = permute_59.view(1, 384, 14, 14)
        permute_59 = None
        hidden_state_120 = torch.conv2d(
            hidden_state_119,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_121 = torch.nn.functional.batch_norm(
            hidden_state_120,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_120 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_60 = hidden_state_121.view(1, 384, 49)
        hidden_state_121 = None
        hidden_state_122 = view_60.permute(0, 2, 1)
        view_60 = None
        hidden_state_123 = torch.conv2d(
            hidden_state_119,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_124 = torch.nn.functional.batch_norm(
            hidden_state_123,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_123 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_61 = hidden_state_124.view(1, 384, 196)
        hidden_state_124 = None
        hidden_state_125 = view_61.permute(0, 2, 1)
        view_61 = None
        hidden_state_126 = torch.conv2d(
            hidden_state_119,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_119 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_127 = torch.nn.functional.batch_norm(
            hidden_state_126,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_126 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_62 = hidden_state_127.view(1, 384, 49)
        hidden_state_127 = None
        hidden_state_128 = view_62.permute(0, 2, 1)
        view_62 = None
        query_7 = torch.cat((cls_token_2, hidden_state_125), dim=1)
        hidden_state_125 = None
        key_7 = torch.cat((cls_token_2, hidden_state_122), dim=1)
        hidden_state_122 = None
        value_7 = torch.cat((cls_token_2, hidden_state_128), dim=1)
        cls_token_2 = hidden_state_128 = None
        linear_36 = torch._C._nn.linear(
            query_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_63 = linear_36.view(1, 197, 6, 64)
        linear_36 = None
        query_8 = view_63.permute(0, 2, 1, 3)
        view_63 = None
        linear_37 = torch._C._nn.linear(
            key_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_64 = linear_37.view(1, 50, 6, 64)
        linear_37 = None
        key_8 = view_64.permute(0, 2, 1, 3)
        view_64 = None
        linear_38 = torch._C._nn.linear(
            value_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_65 = linear_38.view(1, 50, 6, 64)
        linear_38 = None
        value_8 = view_65.permute(0, 2, 1, 3)
        view_65 = None
        einsum_12 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_8, key_8])
        query_8 = key_8 = None
        attention_score_6 = einsum_12 * 0.05103103630798288
        einsum_12 = None
        attention_probs_12 = torch.nn.functional.softmax(attention_score_6, dim=-1)
        attention_score_6 = None
        attention_probs_13 = torch.nn.functional.dropout(
            attention_probs_12, 0.0, False, False
        )
        attention_probs_12 = None
        context_12 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_13, value_8]
        )
        attention_probs_13 = value_8 = None
        permute_66 = context_12.permute(0, 2, 1, 3)
        context_12 = None
        contiguous_6 = permute_66.contiguous()
        permute_66 = None
        context_13 = contiguous_6.view(1, 197, 384)
        contiguous_6 = None
        hidden_state_129 = torch._C._nn.linear(
            context_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_130 = torch.nn.functional.dropout(
            hidden_state_129, 0.0, False, False
        )
        hidden_state_129 = None
        hidden_state_131 = hidden_state_130 + hidden_state_117
        hidden_state_130 = hidden_state_117 = None
        layer_output_6 = torch.nn.functional.layer_norm(
            hidden_state_131,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_132 = torch._C._nn.linear(
            layer_output_6,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_6 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_133 = torch._C._nn.gelu(hidden_state_132, approximate="none")
        hidden_state_132 = None
        hidden_state_134 = torch._C._nn.linear(
            hidden_state_133,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_133 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_135 = torch.nn.functional.dropout(
            hidden_state_134, 0.0, False, False
        )
        hidden_state_134 = None
        hidden_state_136 = hidden_state_135 + hidden_state_131
        hidden_state_135 = hidden_state_131 = None
        layer_norm_17 = torch.nn.functional.layer_norm(
            hidden_state_136,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_ = (None)
        split_2 = torch.functional.split(layer_norm_17, [1, 196], 1)
        layer_norm_17 = None
        cls_token_3 = split_2[0]
        hidden_state_137 = split_2[1]
        split_2 = None
        permute_67 = hidden_state_137.permute(0, 2, 1)
        hidden_state_137 = None
        hidden_state_138 = permute_67.view(1, 384, 14, 14)
        permute_67 = None
        hidden_state_139 = torch.conv2d(
            hidden_state_138,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_140 = torch.nn.functional.batch_norm(
            hidden_state_139,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_139 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_68 = hidden_state_140.view(1, 384, 49)
        hidden_state_140 = None
        hidden_state_141 = view_68.permute(0, 2, 1)
        view_68 = None
        hidden_state_142 = torch.conv2d(
            hidden_state_138,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_143 = torch.nn.functional.batch_norm(
            hidden_state_142,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_142 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_69 = hidden_state_143.view(1, 384, 196)
        hidden_state_143 = None
        hidden_state_144 = view_69.permute(0, 2, 1)
        view_69 = None
        hidden_state_145 = torch.conv2d(
            hidden_state_138,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_138 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_146 = torch.nn.functional.batch_norm(
            hidden_state_145,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_145 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_70 = hidden_state_146.view(1, 384, 49)
        hidden_state_146 = None
        hidden_state_147 = view_70.permute(0, 2, 1)
        view_70 = None
        query_9 = torch.cat((cls_token_3, hidden_state_144), dim=1)
        hidden_state_144 = None
        key_9 = torch.cat((cls_token_3, hidden_state_141), dim=1)
        hidden_state_141 = None
        value_9 = torch.cat((cls_token_3, hidden_state_147), dim=1)
        cls_token_3 = hidden_state_147 = None
        linear_42 = torch._C._nn.linear(
            query_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_71 = linear_42.view(1, 197, 6, 64)
        linear_42 = None
        query_10 = view_71.permute(0, 2, 1, 3)
        view_71 = None
        linear_43 = torch._C._nn.linear(
            key_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_72 = linear_43.view(1, 50, 6, 64)
        linear_43 = None
        key_10 = view_72.permute(0, 2, 1, 3)
        view_72 = None
        linear_44 = torch._C._nn.linear(
            value_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_73 = linear_44.view(1, 50, 6, 64)
        linear_44 = None
        value_10 = view_73.permute(0, 2, 1, 3)
        view_73 = None
        einsum_14 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_10, key_10])
        query_10 = key_10 = None
        attention_score_7 = einsum_14 * 0.05103103630798288
        einsum_14 = None
        attention_probs_14 = torch.nn.functional.softmax(attention_score_7, dim=-1)
        attention_score_7 = None
        attention_probs_15 = torch.nn.functional.dropout(
            attention_probs_14, 0.0, False, False
        )
        attention_probs_14 = None
        context_14 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_15, value_10]
        )
        attention_probs_15 = value_10 = None
        permute_74 = context_14.permute(0, 2, 1, 3)
        context_14 = None
        contiguous_7 = permute_74.contiguous()
        permute_74 = None
        context_15 = contiguous_7.view(1, 197, 384)
        contiguous_7 = None
        hidden_state_148 = torch._C._nn.linear(
            context_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_149 = torch.nn.functional.dropout(
            hidden_state_148, 0.0, False, False
        )
        hidden_state_148 = None
        hidden_state_150 = hidden_state_149 + hidden_state_136
        hidden_state_149 = hidden_state_136 = None
        layer_output_7 = torch.nn.functional.layer_norm(
            hidden_state_150,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_151 = torch._C._nn.linear(
            layer_output_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_152 = torch._C._nn.gelu(hidden_state_151, approximate="none")
        hidden_state_151 = None
        hidden_state_153 = torch._C._nn.linear(
            hidden_state_152,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_152 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_154 = torch.nn.functional.dropout(
            hidden_state_153, 0.0, False, False
        )
        hidden_state_153 = None
        hidden_state_155 = hidden_state_154 + hidden_state_150
        hidden_state_154 = hidden_state_150 = None
        layer_norm_19 = torch.nn.functional.layer_norm(
            hidden_state_155,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_ = (None)
        split_3 = torch.functional.split(layer_norm_19, [1, 196], 1)
        layer_norm_19 = None
        cls_token_4 = split_3[0]
        hidden_state_156 = split_3[1]
        split_3 = None
        permute_75 = hidden_state_156.permute(0, 2, 1)
        hidden_state_156 = None
        hidden_state_157 = permute_75.view(1, 384, 14, 14)
        permute_75 = None
        hidden_state_158 = torch.conv2d(
            hidden_state_157,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_159 = torch.nn.functional.batch_norm(
            hidden_state_158,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_158 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_76 = hidden_state_159.view(1, 384, 49)
        hidden_state_159 = None
        hidden_state_160 = view_76.permute(0, 2, 1)
        view_76 = None
        hidden_state_161 = torch.conv2d(
            hidden_state_157,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_162 = torch.nn.functional.batch_norm(
            hidden_state_161,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_161 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_77 = hidden_state_162.view(1, 384, 196)
        hidden_state_162 = None
        hidden_state_163 = view_77.permute(0, 2, 1)
        view_77 = None
        hidden_state_164 = torch.conv2d(
            hidden_state_157,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_157 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_165 = torch.nn.functional.batch_norm(
            hidden_state_164,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_164 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_78 = hidden_state_165.view(1, 384, 49)
        hidden_state_165 = None
        hidden_state_166 = view_78.permute(0, 2, 1)
        view_78 = None
        query_11 = torch.cat((cls_token_4, hidden_state_163), dim=1)
        hidden_state_163 = None
        key_11 = torch.cat((cls_token_4, hidden_state_160), dim=1)
        hidden_state_160 = None
        value_11 = torch.cat((cls_token_4, hidden_state_166), dim=1)
        cls_token_4 = hidden_state_166 = None
        linear_48 = torch._C._nn.linear(
            query_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_79 = linear_48.view(1, 197, 6, 64)
        linear_48 = None
        query_12 = view_79.permute(0, 2, 1, 3)
        view_79 = None
        linear_49 = torch._C._nn.linear(
            key_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_80 = linear_49.view(1, 50, 6, 64)
        linear_49 = None
        key_12 = view_80.permute(0, 2, 1, 3)
        view_80 = None
        linear_50 = torch._C._nn.linear(
            value_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_81 = linear_50.view(1, 50, 6, 64)
        linear_50 = None
        value_12 = view_81.permute(0, 2, 1, 3)
        view_81 = None
        einsum_16 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_12, key_12])
        query_12 = key_12 = None
        attention_score_8 = einsum_16 * 0.05103103630798288
        einsum_16 = None
        attention_probs_16 = torch.nn.functional.softmax(attention_score_8, dim=-1)
        attention_score_8 = None
        attention_probs_17 = torch.nn.functional.dropout(
            attention_probs_16, 0.0, False, False
        )
        attention_probs_16 = None
        context_16 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_17, value_12]
        )
        attention_probs_17 = value_12 = None
        permute_82 = context_16.permute(0, 2, 1, 3)
        context_16 = None
        contiguous_8 = permute_82.contiguous()
        permute_82 = None
        context_17 = contiguous_8.view(1, 197, 384)
        contiguous_8 = None
        hidden_state_167 = torch._C._nn.linear(
            context_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_168 = torch.nn.functional.dropout(
            hidden_state_167, 0.0, False, False
        )
        hidden_state_167 = None
        hidden_state_169 = hidden_state_168 + hidden_state_155
        hidden_state_168 = hidden_state_155 = None
        layer_output_8 = torch.nn.functional.layer_norm(
            hidden_state_169,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_170 = torch._C._nn.linear(
            layer_output_8,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_8 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_171 = torch._C._nn.gelu(hidden_state_170, approximate="none")
        hidden_state_170 = None
        hidden_state_172 = torch._C._nn.linear(
            hidden_state_171,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_171 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_173 = torch.nn.functional.dropout(
            hidden_state_172, 0.0, False, False
        )
        hidden_state_172 = None
        hidden_state_174 = hidden_state_173 + hidden_state_169
        hidden_state_173 = hidden_state_169 = None
        layer_norm_21 = torch.nn.functional.layer_norm(
            hidden_state_174,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_ = (None)
        split_4 = torch.functional.split(layer_norm_21, [1, 196], 1)
        layer_norm_21 = None
        cls_token_5 = split_4[0]
        hidden_state_175 = split_4[1]
        split_4 = None
        permute_83 = hidden_state_175.permute(0, 2, 1)
        hidden_state_175 = None
        hidden_state_176 = permute_83.view(1, 384, 14, 14)
        permute_83 = None
        hidden_state_177 = torch.conv2d(
            hidden_state_176,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_178 = torch.nn.functional.batch_norm(
            hidden_state_177,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_177 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_84 = hidden_state_178.view(1, 384, 49)
        hidden_state_178 = None
        hidden_state_179 = view_84.permute(0, 2, 1)
        view_84 = None
        hidden_state_180 = torch.conv2d(
            hidden_state_176,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_181 = torch.nn.functional.batch_norm(
            hidden_state_180,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_180 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_85 = hidden_state_181.view(1, 384, 196)
        hidden_state_181 = None
        hidden_state_182 = view_85.permute(0, 2, 1)
        view_85 = None
        hidden_state_183 = torch.conv2d(
            hidden_state_176,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_176 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_184 = torch.nn.functional.batch_norm(
            hidden_state_183,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_183 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_86 = hidden_state_184.view(1, 384, 49)
        hidden_state_184 = None
        hidden_state_185 = view_86.permute(0, 2, 1)
        view_86 = None
        query_13 = torch.cat((cls_token_5, hidden_state_182), dim=1)
        hidden_state_182 = None
        key_13 = torch.cat((cls_token_5, hidden_state_179), dim=1)
        hidden_state_179 = None
        value_13 = torch.cat((cls_token_5, hidden_state_185), dim=1)
        cls_token_5 = hidden_state_185 = None
        linear_54 = torch._C._nn.linear(
            query_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_87 = linear_54.view(1, 197, 6, 64)
        linear_54 = None
        query_14 = view_87.permute(0, 2, 1, 3)
        view_87 = None
        linear_55 = torch._C._nn.linear(
            key_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_88 = linear_55.view(1, 50, 6, 64)
        linear_55 = None
        key_14 = view_88.permute(0, 2, 1, 3)
        view_88 = None
        linear_56 = torch._C._nn.linear(
            value_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_89 = linear_56.view(1, 50, 6, 64)
        linear_56 = None
        value_14 = view_89.permute(0, 2, 1, 3)
        view_89 = None
        einsum_18 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_14, key_14])
        query_14 = key_14 = None
        attention_score_9 = einsum_18 * 0.05103103630798288
        einsum_18 = None
        attention_probs_18 = torch.nn.functional.softmax(attention_score_9, dim=-1)
        attention_score_9 = None
        attention_probs_19 = torch.nn.functional.dropout(
            attention_probs_18, 0.0, False, False
        )
        attention_probs_18 = None
        context_18 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_19, value_14]
        )
        attention_probs_19 = value_14 = None
        permute_90 = context_18.permute(0, 2, 1, 3)
        context_18 = None
        contiguous_9 = permute_90.contiguous()
        permute_90 = None
        context_19 = contiguous_9.view(1, 197, 384)
        contiguous_9 = None
        hidden_state_186 = torch._C._nn.linear(
            context_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_187 = torch.nn.functional.dropout(
            hidden_state_186, 0.0, False, False
        )
        hidden_state_186 = None
        hidden_state_188 = hidden_state_187 + hidden_state_174
        hidden_state_187 = hidden_state_174 = None
        layer_output_9 = torch.nn.functional.layer_norm(
            hidden_state_188,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_189 = torch._C._nn.linear(
            layer_output_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_190 = torch._C._nn.gelu(hidden_state_189, approximate="none")
        hidden_state_189 = None
        hidden_state_191 = torch._C._nn.linear(
            hidden_state_190,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_190 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_192 = torch.nn.functional.dropout(
            hidden_state_191, 0.0, False, False
        )
        hidden_state_191 = None
        hidden_state_193 = hidden_state_192 + hidden_state_188
        hidden_state_192 = hidden_state_188 = None
        layer_norm_23 = torch.nn.functional.layer_norm(
            hidden_state_193,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_ = (None)
        split_5 = torch.functional.split(layer_norm_23, [1, 196], 1)
        layer_norm_23 = None
        cls_token_6 = split_5[0]
        hidden_state_194 = split_5[1]
        split_5 = None
        permute_91 = hidden_state_194.permute(0, 2, 1)
        hidden_state_194 = None
        hidden_state_195 = permute_91.view(1, 384, 14, 14)
        permute_91 = None
        hidden_state_196 = torch.conv2d(
            hidden_state_195,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_197 = torch.nn.functional.batch_norm(
            hidden_state_196,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_196 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_92 = hidden_state_197.view(1, 384, 49)
        hidden_state_197 = None
        hidden_state_198 = view_92.permute(0, 2, 1)
        view_92 = None
        hidden_state_199 = torch.conv2d(
            hidden_state_195,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_200 = torch.nn.functional.batch_norm(
            hidden_state_199,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_199 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_93 = hidden_state_200.view(1, 384, 196)
        hidden_state_200 = None
        hidden_state_201 = view_93.permute(0, 2, 1)
        view_93 = None
        hidden_state_202 = torch.conv2d(
            hidden_state_195,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_195 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_203 = torch.nn.functional.batch_norm(
            hidden_state_202,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_202 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_94 = hidden_state_203.view(1, 384, 49)
        hidden_state_203 = None
        hidden_state_204 = view_94.permute(0, 2, 1)
        view_94 = None
        query_15 = torch.cat((cls_token_6, hidden_state_201), dim=1)
        hidden_state_201 = None
        key_15 = torch.cat((cls_token_6, hidden_state_198), dim=1)
        hidden_state_198 = None
        value_15 = torch.cat((cls_token_6, hidden_state_204), dim=1)
        cls_token_6 = hidden_state_204 = None
        linear_60 = torch._C._nn.linear(
            query_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_95 = linear_60.view(1, 197, 6, 64)
        linear_60 = None
        query_16 = view_95.permute(0, 2, 1, 3)
        view_95 = None
        linear_61 = torch._C._nn.linear(
            key_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_96 = linear_61.view(1, 50, 6, 64)
        linear_61 = None
        key_16 = view_96.permute(0, 2, 1, 3)
        view_96 = None
        linear_62 = torch._C._nn.linear(
            value_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_97 = linear_62.view(1, 50, 6, 64)
        linear_62 = None
        value_16 = view_97.permute(0, 2, 1, 3)
        view_97 = None
        einsum_20 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_16, key_16])
        query_16 = key_16 = None
        attention_score_10 = einsum_20 * 0.05103103630798288
        einsum_20 = None
        attention_probs_20 = torch.nn.functional.softmax(attention_score_10, dim=-1)
        attention_score_10 = None
        attention_probs_21 = torch.nn.functional.dropout(
            attention_probs_20, 0.0, False, False
        )
        attention_probs_20 = None
        context_20 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_21, value_16]
        )
        attention_probs_21 = value_16 = None
        permute_98 = context_20.permute(0, 2, 1, 3)
        context_20 = None
        contiguous_10 = permute_98.contiguous()
        permute_98 = None
        context_21 = contiguous_10.view(1, 197, 384)
        contiguous_10 = None
        hidden_state_205 = torch._C._nn.linear(
            context_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_206 = torch.nn.functional.dropout(
            hidden_state_205, 0.0, False, False
        )
        hidden_state_205 = None
        hidden_state_207 = hidden_state_206 + hidden_state_193
        hidden_state_206 = hidden_state_193 = None
        layer_output_10 = torch.nn.functional.layer_norm(
            hidden_state_207,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_208 = torch._C._nn.linear(
            layer_output_10,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_10 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_209 = torch._C._nn.gelu(hidden_state_208, approximate="none")
        hidden_state_208 = None
        hidden_state_210 = torch._C._nn.linear(
            hidden_state_209,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_209 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_211 = torch.nn.functional.dropout(
            hidden_state_210, 0.0, False, False
        )
        hidden_state_210 = None
        hidden_state_212 = hidden_state_211 + hidden_state_207
        hidden_state_211 = hidden_state_207 = None
        layer_norm_25 = torch.nn.functional.layer_norm(
            hidden_state_212,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_ = (None)
        split_6 = torch.functional.split(layer_norm_25, [1, 196], 1)
        layer_norm_25 = None
        cls_token_7 = split_6[0]
        hidden_state_213 = split_6[1]
        split_6 = None
        permute_99 = hidden_state_213.permute(0, 2, 1)
        hidden_state_213 = None
        hidden_state_214 = permute_99.view(1, 384, 14, 14)
        permute_99 = None
        hidden_state_215 = torch.conv2d(
            hidden_state_214,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_216 = torch.nn.functional.batch_norm(
            hidden_state_215,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_215 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_100 = hidden_state_216.view(1, 384, 49)
        hidden_state_216 = None
        hidden_state_217 = view_100.permute(0, 2, 1)
        view_100 = None
        hidden_state_218 = torch.conv2d(
            hidden_state_214,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_219 = torch.nn.functional.batch_norm(
            hidden_state_218,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_218 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_101 = hidden_state_219.view(1, 384, 196)
        hidden_state_219 = None
        hidden_state_220 = view_101.permute(0, 2, 1)
        view_101 = None
        hidden_state_221 = torch.conv2d(
            hidden_state_214,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_214 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_222 = torch.nn.functional.batch_norm(
            hidden_state_221,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_221 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_102 = hidden_state_222.view(1, 384, 49)
        hidden_state_222 = None
        hidden_state_223 = view_102.permute(0, 2, 1)
        view_102 = None
        query_17 = torch.cat((cls_token_7, hidden_state_220), dim=1)
        hidden_state_220 = None
        key_17 = torch.cat((cls_token_7, hidden_state_217), dim=1)
        hidden_state_217 = None
        value_17 = torch.cat((cls_token_7, hidden_state_223), dim=1)
        cls_token_7 = hidden_state_223 = None
        linear_66 = torch._C._nn.linear(
            query_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_103 = linear_66.view(1, 197, 6, 64)
        linear_66 = None
        query_18 = view_103.permute(0, 2, 1, 3)
        view_103 = None
        linear_67 = torch._C._nn.linear(
            key_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_104 = linear_67.view(1, 50, 6, 64)
        linear_67 = None
        key_18 = view_104.permute(0, 2, 1, 3)
        view_104 = None
        linear_68 = torch._C._nn.linear(
            value_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_105 = linear_68.view(1, 50, 6, 64)
        linear_68 = None
        value_18 = view_105.permute(0, 2, 1, 3)
        view_105 = None
        einsum_22 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_18, key_18])
        query_18 = key_18 = None
        attention_score_11 = einsum_22 * 0.05103103630798288
        einsum_22 = None
        attention_probs_22 = torch.nn.functional.softmax(attention_score_11, dim=-1)
        attention_score_11 = None
        attention_probs_23 = torch.nn.functional.dropout(
            attention_probs_22, 0.0, False, False
        )
        attention_probs_22 = None
        context_22 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_23, value_18]
        )
        attention_probs_23 = value_18 = None
        permute_106 = context_22.permute(0, 2, 1, 3)
        context_22 = None
        contiguous_11 = permute_106.contiguous()
        permute_106 = None
        context_23 = contiguous_11.view(1, 197, 384)
        contiguous_11 = None
        hidden_state_224 = torch._C._nn.linear(
            context_23,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_23 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_225 = torch.nn.functional.dropout(
            hidden_state_224, 0.0, False, False
        )
        hidden_state_224 = None
        hidden_state_226 = hidden_state_225 + hidden_state_212
        hidden_state_225 = hidden_state_212 = None
        layer_output_11 = torch.nn.functional.layer_norm(
            hidden_state_226,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_227 = torch._C._nn.linear(
            layer_output_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_228 = torch._C._nn.gelu(hidden_state_227, approximate="none")
        hidden_state_227 = None
        hidden_state_229 = torch._C._nn.linear(
            hidden_state_228,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_228 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_230 = torch.nn.functional.dropout(
            hidden_state_229, 0.0, False, False
        )
        hidden_state_229 = None
        hidden_state_231 = hidden_state_230 + hidden_state_226
        hidden_state_230 = hidden_state_226 = None
        layer_norm_27 = torch.nn.functional.layer_norm(
            hidden_state_231,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_ = (None)
        split_7 = torch.functional.split(layer_norm_27, [1, 196], 1)
        layer_norm_27 = None
        cls_token_8 = split_7[0]
        hidden_state_232 = split_7[1]
        split_7 = None
        permute_107 = hidden_state_232.permute(0, 2, 1)
        hidden_state_232 = None
        hidden_state_233 = permute_107.view(1, 384, 14, 14)
        permute_107 = None
        hidden_state_234 = torch.conv2d(
            hidden_state_233,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_235 = torch.nn.functional.batch_norm(
            hidden_state_234,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_234 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_108 = hidden_state_235.view(1, 384, 49)
        hidden_state_235 = None
        hidden_state_236 = view_108.permute(0, 2, 1)
        view_108 = None
        hidden_state_237 = torch.conv2d(
            hidden_state_233,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_238 = torch.nn.functional.batch_norm(
            hidden_state_237,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_237 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_109 = hidden_state_238.view(1, 384, 196)
        hidden_state_238 = None
        hidden_state_239 = view_109.permute(0, 2, 1)
        view_109 = None
        hidden_state_240 = torch.conv2d(
            hidden_state_233,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_233 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_241 = torch.nn.functional.batch_norm(
            hidden_state_240,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_240 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_110 = hidden_state_241.view(1, 384, 49)
        hidden_state_241 = None
        hidden_state_242 = view_110.permute(0, 2, 1)
        view_110 = None
        query_19 = torch.cat((cls_token_8, hidden_state_239), dim=1)
        hidden_state_239 = None
        key_19 = torch.cat((cls_token_8, hidden_state_236), dim=1)
        hidden_state_236 = None
        value_19 = torch.cat((cls_token_8, hidden_state_242), dim=1)
        cls_token_8 = hidden_state_242 = None
        linear_72 = torch._C._nn.linear(
            query_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_111 = linear_72.view(1, 197, 6, 64)
        linear_72 = None
        query_20 = view_111.permute(0, 2, 1, 3)
        view_111 = None
        linear_73 = torch._C._nn.linear(
            key_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_112 = linear_73.view(1, 50, 6, 64)
        linear_73 = None
        key_20 = view_112.permute(0, 2, 1, 3)
        view_112 = None
        linear_74 = torch._C._nn.linear(
            value_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_113 = linear_74.view(1, 50, 6, 64)
        linear_74 = None
        value_20 = view_113.permute(0, 2, 1, 3)
        view_113 = None
        einsum_24 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_20, key_20])
        query_20 = key_20 = None
        attention_score_12 = einsum_24 * 0.05103103630798288
        einsum_24 = None
        attention_probs_24 = torch.nn.functional.softmax(attention_score_12, dim=-1)
        attention_score_12 = None
        attention_probs_25 = torch.nn.functional.dropout(
            attention_probs_24, 0.0, False, False
        )
        attention_probs_24 = None
        context_24 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_25, value_20]
        )
        attention_probs_25 = value_20 = None
        permute_114 = context_24.permute(0, 2, 1, 3)
        context_24 = None
        contiguous_12 = permute_114.contiguous()
        permute_114 = None
        context_25 = contiguous_12.view(1, 197, 384)
        contiguous_12 = None
        hidden_state_243 = torch._C._nn.linear(
            context_25,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_25 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_244 = torch.nn.functional.dropout(
            hidden_state_243, 0.0, False, False
        )
        hidden_state_243 = None
        hidden_state_245 = hidden_state_244 + hidden_state_231
        hidden_state_244 = hidden_state_231 = None
        layer_output_12 = torch.nn.functional.layer_norm(
            hidden_state_245,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_246 = torch._C._nn.linear(
            layer_output_12,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_12 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_247 = torch._C._nn.gelu(hidden_state_246, approximate="none")
        hidden_state_246 = None
        hidden_state_248 = torch._C._nn.linear(
            hidden_state_247,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_247 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_249 = torch.nn.functional.dropout(
            hidden_state_248, 0.0, False, False
        )
        hidden_state_248 = None
        hidden_state_250 = hidden_state_249 + hidden_state_245
        hidden_state_249 = hidden_state_245 = None
        layer_norm_29 = torch.nn.functional.layer_norm(
            hidden_state_250,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_ = (None)
        split_8 = torch.functional.split(layer_norm_29, [1, 196], 1)
        layer_norm_29 = None
        cls_token_9 = split_8[0]
        hidden_state_251 = split_8[1]
        split_8 = None
        permute_115 = hidden_state_251.permute(0, 2, 1)
        hidden_state_251 = None
        hidden_state_252 = permute_115.view(1, 384, 14, 14)
        permute_115 = None
        hidden_state_253 = torch.conv2d(
            hidden_state_252,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_254 = torch.nn.functional.batch_norm(
            hidden_state_253,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_253 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_116 = hidden_state_254.view(1, 384, 49)
        hidden_state_254 = None
        hidden_state_255 = view_116.permute(0, 2, 1)
        view_116 = None
        hidden_state_256 = torch.conv2d(
            hidden_state_252,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_257 = torch.nn.functional.batch_norm(
            hidden_state_256,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_256 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_117 = hidden_state_257.view(1, 384, 196)
        hidden_state_257 = None
        hidden_state_258 = view_117.permute(0, 2, 1)
        view_117 = None
        hidden_state_259 = torch.conv2d(
            hidden_state_252,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_252 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_260 = torch.nn.functional.batch_norm(
            hidden_state_259,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_259 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_118 = hidden_state_260.view(1, 384, 49)
        hidden_state_260 = None
        hidden_state_261 = view_118.permute(0, 2, 1)
        view_118 = None
        query_21 = torch.cat((cls_token_9, hidden_state_258), dim=1)
        hidden_state_258 = None
        key_21 = torch.cat((cls_token_9, hidden_state_255), dim=1)
        hidden_state_255 = None
        value_21 = torch.cat((cls_token_9, hidden_state_261), dim=1)
        cls_token_9 = hidden_state_261 = None
        linear_78 = torch._C._nn.linear(
            query_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_119 = linear_78.view(1, 197, 6, 64)
        linear_78 = None
        query_22 = view_119.permute(0, 2, 1, 3)
        view_119 = None
        linear_79 = torch._C._nn.linear(
            key_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_120 = linear_79.view(1, 50, 6, 64)
        linear_79 = None
        key_22 = view_120.permute(0, 2, 1, 3)
        view_120 = None
        linear_80 = torch._C._nn.linear(
            value_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_121 = linear_80.view(1, 50, 6, 64)
        linear_80 = None
        value_22 = view_121.permute(0, 2, 1, 3)
        view_121 = None
        einsum_26 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_22, key_22])
        query_22 = key_22 = None
        attention_score_13 = einsum_26 * 0.05103103630798288
        einsum_26 = None
        attention_probs_26 = torch.nn.functional.softmax(attention_score_13, dim=-1)
        attention_score_13 = None
        attention_probs_27 = torch.nn.functional.dropout(
            attention_probs_26, 0.0, False, False
        )
        attention_probs_26 = None
        context_26 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_27, value_22]
        )
        attention_probs_27 = value_22 = None
        permute_122 = context_26.permute(0, 2, 1, 3)
        context_26 = None
        contiguous_13 = permute_122.contiguous()
        permute_122 = None
        context_27 = contiguous_13.view(1, 197, 384)
        contiguous_13 = None
        hidden_state_262 = torch._C._nn.linear(
            context_27,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_27 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_263 = torch.nn.functional.dropout(
            hidden_state_262, 0.0, False, False
        )
        hidden_state_262 = None
        hidden_state_264 = hidden_state_263 + hidden_state_250
        hidden_state_263 = hidden_state_250 = None
        layer_output_13 = torch.nn.functional.layer_norm(
            hidden_state_264,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_265 = torch._C._nn.linear(
            layer_output_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_266 = torch._C._nn.gelu(hidden_state_265, approximate="none")
        hidden_state_265 = None
        hidden_state_267 = torch._C._nn.linear(
            hidden_state_266,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_266 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_268 = torch.nn.functional.dropout(
            hidden_state_267, 0.0, False, False
        )
        hidden_state_267 = None
        hidden_state_269 = hidden_state_268 + hidden_state_264
        hidden_state_268 = hidden_state_264 = None
        layer_norm_31 = torch.nn.functional.layer_norm(
            hidden_state_269,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_ = (None)
        split_9 = torch.functional.split(layer_norm_31, [1, 196], 1)
        layer_norm_31 = None
        cls_token_10 = split_9[0]
        hidden_state_270 = split_9[1]
        split_9 = None
        permute_123 = hidden_state_270.permute(0, 2, 1)
        hidden_state_270 = None
        hidden_state_271 = permute_123.view(1, 384, 14, 14)
        permute_123 = None
        hidden_state_272 = torch.conv2d(
            hidden_state_271,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_273 = torch.nn.functional.batch_norm(
            hidden_state_272,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_272 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_124 = hidden_state_273.view(1, 384, 49)
        hidden_state_273 = None
        hidden_state_274 = view_124.permute(0, 2, 1)
        view_124 = None
        hidden_state_275 = torch.conv2d(
            hidden_state_271,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_276 = torch.nn.functional.batch_norm(
            hidden_state_275,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_275 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_125 = hidden_state_276.view(1, 384, 196)
        hidden_state_276 = None
        hidden_state_277 = view_125.permute(0, 2, 1)
        view_125 = None
        hidden_state_278 = torch.conv2d(
            hidden_state_271,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_271 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_279 = torch.nn.functional.batch_norm(
            hidden_state_278,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_278 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_126 = hidden_state_279.view(1, 384, 49)
        hidden_state_279 = None
        hidden_state_280 = view_126.permute(0, 2, 1)
        view_126 = None
        query_23 = torch.cat((cls_token_10, hidden_state_277), dim=1)
        hidden_state_277 = None
        key_23 = torch.cat((cls_token_10, hidden_state_274), dim=1)
        hidden_state_274 = None
        value_23 = torch.cat((cls_token_10, hidden_state_280), dim=1)
        cls_token_10 = hidden_state_280 = None
        linear_84 = torch._C._nn.linear(
            query_23,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_23 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_127 = linear_84.view(1, 197, 6, 64)
        linear_84 = None
        query_24 = view_127.permute(0, 2, 1, 3)
        view_127 = None
        linear_85 = torch._C._nn.linear(
            key_23,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_23 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_128 = linear_85.view(1, 50, 6, 64)
        linear_85 = None
        key_24 = view_128.permute(0, 2, 1, 3)
        view_128 = None
        linear_86 = torch._C._nn.linear(
            value_23,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_23 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_129 = linear_86.view(1, 50, 6, 64)
        linear_86 = None
        value_24 = view_129.permute(0, 2, 1, 3)
        view_129 = None
        einsum_28 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_24, key_24])
        query_24 = key_24 = None
        attention_score_14 = einsum_28 * 0.05103103630798288
        einsum_28 = None
        attention_probs_28 = torch.nn.functional.softmax(attention_score_14, dim=-1)
        attention_score_14 = None
        attention_probs_29 = torch.nn.functional.dropout(
            attention_probs_28, 0.0, False, False
        )
        attention_probs_28 = None
        context_28 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_29, value_24]
        )
        attention_probs_29 = value_24 = None
        permute_130 = context_28.permute(0, 2, 1, 3)
        context_28 = None
        contiguous_14 = permute_130.contiguous()
        permute_130 = None
        context_29 = contiguous_14.view(1, 197, 384)
        contiguous_14 = None
        hidden_state_281 = torch._C._nn.linear(
            context_29,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_29 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_282 = torch.nn.functional.dropout(
            hidden_state_281, 0.0, False, False
        )
        hidden_state_281 = None
        hidden_state_283 = hidden_state_282 + hidden_state_269
        hidden_state_282 = hidden_state_269 = None
        layer_output_14 = torch.nn.functional.layer_norm(
            hidden_state_283,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_284 = torch._C._nn.linear(
            layer_output_14,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_14 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_285 = torch._C._nn.gelu(hidden_state_284, approximate="none")
        hidden_state_284 = None
        hidden_state_286 = torch._C._nn.linear(
            hidden_state_285,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_285 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_287 = torch.nn.functional.dropout(
            hidden_state_286, 0.0, False, False
        )
        hidden_state_286 = None
        hidden_state_288 = hidden_state_287 + hidden_state_283
        hidden_state_287 = hidden_state_283 = None
        layer_norm_33 = torch.nn.functional.layer_norm(
            hidden_state_288,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_before_parameters_bias_ = (None)
        split_10 = torch.functional.split(layer_norm_33, [1, 196], 1)
        layer_norm_33 = None
        cls_token_11 = split_10[0]
        hidden_state_289 = split_10[1]
        split_10 = None
        permute_131 = hidden_state_289.permute(0, 2, 1)
        hidden_state_289 = None
        hidden_state_290 = permute_131.view(1, 384, 14, 14)
        permute_131 = None
        hidden_state_291 = torch.conv2d(
            hidden_state_290,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_292 = torch.nn.functional.batch_norm(
            hidden_state_291,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_291 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_132 = hidden_state_292.view(1, 384, 49)
        hidden_state_292 = None
        hidden_state_293 = view_132.permute(0, 2, 1)
        view_132 = None
        hidden_state_294 = torch.conv2d(
            hidden_state_290,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_295 = torch.nn.functional.batch_norm(
            hidden_state_294,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_294 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_133 = hidden_state_295.view(1, 384, 196)
        hidden_state_295 = None
        hidden_state_296 = view_133.permute(0, 2, 1)
        view_133 = None
        hidden_state_297 = torch.conv2d(
            hidden_state_290,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_290 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_298 = torch.nn.functional.batch_norm(
            hidden_state_297,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_297 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_134 = hidden_state_298.view(1, 384, 49)
        hidden_state_298 = None
        hidden_state_299 = view_134.permute(0, 2, 1)
        view_134 = None
        query_25 = torch.cat((cls_token_11, hidden_state_296), dim=1)
        hidden_state_296 = None
        key_25 = torch.cat((cls_token_11, hidden_state_293), dim=1)
        hidden_state_293 = None
        value_25 = torch.cat((cls_token_11, hidden_state_299), dim=1)
        cls_token_11 = hidden_state_299 = None
        linear_90 = torch._C._nn.linear(
            query_25,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_25 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_135 = linear_90.view(1, 197, 6, 64)
        linear_90 = None
        query_26 = view_135.permute(0, 2, 1, 3)
        view_135 = None
        linear_91 = torch._C._nn.linear(
            key_25,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_25 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_136 = linear_91.view(1, 50, 6, 64)
        linear_91 = None
        key_26 = view_136.permute(0, 2, 1, 3)
        view_136 = None
        linear_92 = torch._C._nn.linear(
            value_25,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_25 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_137 = linear_92.view(1, 50, 6, 64)
        linear_92 = None
        value_26 = view_137.permute(0, 2, 1, 3)
        view_137 = None
        einsum_30 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_26, key_26])
        query_26 = key_26 = None
        attention_score_15 = einsum_30 * 0.05103103630798288
        einsum_30 = None
        attention_probs_30 = torch.nn.functional.softmax(attention_score_15, dim=-1)
        attention_score_15 = None
        attention_probs_31 = torch.nn.functional.dropout(
            attention_probs_30, 0.0, False, False
        )
        attention_probs_30 = None
        context_30 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_31, value_26]
        )
        attention_probs_31 = value_26 = None
        permute_138 = context_30.permute(0, 2, 1, 3)
        context_30 = None
        contiguous_15 = permute_138.contiguous()
        permute_138 = None
        context_31 = contiguous_15.view(1, 197, 384)
        contiguous_15 = None
        hidden_state_300 = torch._C._nn.linear(
            context_31,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_31 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_301 = torch.nn.functional.dropout(
            hidden_state_300, 0.0, False, False
        )
        hidden_state_300 = None
        hidden_state_302 = hidden_state_301 + hidden_state_288
        hidden_state_301 = hidden_state_288 = None
        layer_output_15 = torch.nn.functional.layer_norm(
            hidden_state_302,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_303 = torch._C._nn.linear(
            layer_output_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_304 = torch._C._nn.gelu(hidden_state_303, approximate="none")
        hidden_state_303 = None
        hidden_state_305 = torch._C._nn.linear(
            hidden_state_304,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_304 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_10_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_306 = torch.nn.functional.dropout(
            hidden_state_305, 0.0, False, False
        )
        hidden_state_305 = None
        hidden_state_307 = hidden_state_306 + hidden_state_302
        hidden_state_306 = hidden_state_302 = None
        layer_norm_35 = torch.nn.functional.layer_norm(
            hidden_state_307,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_before_parameters_bias_ = (None)
        split_11 = torch.functional.split(layer_norm_35, [1, 196], 1)
        layer_norm_35 = None
        cls_token_12 = split_11[0]
        hidden_state_308 = split_11[1]
        split_11 = None
        permute_139 = hidden_state_308.permute(0, 2, 1)
        hidden_state_308 = None
        hidden_state_309 = permute_139.view(1, 384, 14, 14)
        permute_139 = None
        hidden_state_310 = torch.conv2d(
            hidden_state_309,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_311 = torch.nn.functional.batch_norm(
            hidden_state_310,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_310 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_140 = hidden_state_311.view(1, 384, 49)
        hidden_state_311 = None
        hidden_state_312 = view_140.permute(0, 2, 1)
        view_140 = None
        hidden_state_313 = torch.conv2d(
            hidden_state_309,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_314 = torch.nn.functional.batch_norm(
            hidden_state_313,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_313 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_141 = hidden_state_314.view(1, 384, 196)
        hidden_state_314 = None
        hidden_state_315 = view_141.permute(0, 2, 1)
        view_141 = None
        hidden_state_316 = torch.conv2d(
            hidden_state_309,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_309 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_317 = torch.nn.functional.batch_norm(
            hidden_state_316,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_316 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_142 = hidden_state_317.view(1, 384, 49)
        hidden_state_317 = None
        hidden_state_318 = view_142.permute(0, 2, 1)
        view_142 = None
        query_27 = torch.cat((cls_token_12, hidden_state_315), dim=1)
        hidden_state_315 = None
        key_27 = torch.cat((cls_token_12, hidden_state_312), dim=1)
        hidden_state_312 = None
        value_27 = torch.cat((cls_token_12, hidden_state_318), dim=1)
        cls_token_12 = hidden_state_318 = None
        linear_96 = torch._C._nn.linear(
            query_27,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_27 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_143 = linear_96.view(1, 197, 6, 64)
        linear_96 = None
        query_28 = view_143.permute(0, 2, 1, 3)
        view_143 = None
        linear_97 = torch._C._nn.linear(
            key_27,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_27 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_144 = linear_97.view(1, 50, 6, 64)
        linear_97 = None
        key_28 = view_144.permute(0, 2, 1, 3)
        view_144 = None
        linear_98 = torch._C._nn.linear(
            value_27,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_27 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_145 = linear_98.view(1, 50, 6, 64)
        linear_98 = None
        value_28 = view_145.permute(0, 2, 1, 3)
        view_145 = None
        einsum_32 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_28, key_28])
        query_28 = key_28 = None
        attention_score_16 = einsum_32 * 0.05103103630798288
        einsum_32 = None
        attention_probs_32 = torch.nn.functional.softmax(attention_score_16, dim=-1)
        attention_score_16 = None
        attention_probs_33 = torch.nn.functional.dropout(
            attention_probs_32, 0.0, False, False
        )
        attention_probs_32 = None
        context_32 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_33, value_28]
        )
        attention_probs_33 = value_28 = None
        permute_146 = context_32.permute(0, 2, 1, 3)
        context_32 = None
        contiguous_16 = permute_146.contiguous()
        permute_146 = None
        context_33 = contiguous_16.view(1, 197, 384)
        contiguous_16 = None
        hidden_state_319 = torch._C._nn.linear(
            context_33,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_33 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_320 = torch.nn.functional.dropout(
            hidden_state_319, 0.0, False, False
        )
        hidden_state_319 = None
        hidden_state_321 = hidden_state_320 + hidden_state_307
        hidden_state_320 = hidden_state_307 = None
        layer_output_16 = torch.nn.functional.layer_norm(
            hidden_state_321,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_322 = torch._C._nn.linear(
            layer_output_16,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_16 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_323 = torch._C._nn.gelu(hidden_state_322, approximate="none")
        hidden_state_322 = None
        hidden_state_324 = torch._C._nn.linear(
            hidden_state_323,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_323 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_11_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_325 = torch.nn.functional.dropout(
            hidden_state_324, 0.0, False, False
        )
        hidden_state_324 = None
        hidden_state_326 = hidden_state_325 + hidden_state_321
        hidden_state_325 = hidden_state_321 = None
        layer_norm_37 = torch.nn.functional.layer_norm(
            hidden_state_326,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_before_parameters_bias_ = (None)
        split_12 = torch.functional.split(layer_norm_37, [1, 196], 1)
        layer_norm_37 = None
        cls_token_13 = split_12[0]
        hidden_state_327 = split_12[1]
        split_12 = None
        permute_147 = hidden_state_327.permute(0, 2, 1)
        hidden_state_327 = None
        hidden_state_328 = permute_147.view(1, 384, 14, 14)
        permute_147 = None
        hidden_state_329 = torch.conv2d(
            hidden_state_328,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_330 = torch.nn.functional.batch_norm(
            hidden_state_329,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_329 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_148 = hidden_state_330.view(1, 384, 49)
        hidden_state_330 = None
        hidden_state_331 = view_148.permute(0, 2, 1)
        view_148 = None
        hidden_state_332 = torch.conv2d(
            hidden_state_328,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_333 = torch.nn.functional.batch_norm(
            hidden_state_332,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_332 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_149 = hidden_state_333.view(1, 384, 196)
        hidden_state_333 = None
        hidden_state_334 = view_149.permute(0, 2, 1)
        view_149 = None
        hidden_state_335 = torch.conv2d(
            hidden_state_328,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_328 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_336 = torch.nn.functional.batch_norm(
            hidden_state_335,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_335 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_150 = hidden_state_336.view(1, 384, 49)
        hidden_state_336 = None
        hidden_state_337 = view_150.permute(0, 2, 1)
        view_150 = None
        query_29 = torch.cat((cls_token_13, hidden_state_334), dim=1)
        hidden_state_334 = None
        key_29 = torch.cat((cls_token_13, hidden_state_331), dim=1)
        hidden_state_331 = None
        value_29 = torch.cat((cls_token_13, hidden_state_337), dim=1)
        cls_token_13 = hidden_state_337 = None
        linear_102 = torch._C._nn.linear(
            query_29,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_29 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_151 = linear_102.view(1, 197, 6, 64)
        linear_102 = None
        query_30 = view_151.permute(0, 2, 1, 3)
        view_151 = None
        linear_103 = torch._C._nn.linear(
            key_29,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_29 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_152 = linear_103.view(1, 50, 6, 64)
        linear_103 = None
        key_30 = view_152.permute(0, 2, 1, 3)
        view_152 = None
        linear_104 = torch._C._nn.linear(
            value_29,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_29 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_153 = linear_104.view(1, 50, 6, 64)
        linear_104 = None
        value_30 = view_153.permute(0, 2, 1, 3)
        view_153 = None
        einsum_34 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_30, key_30])
        query_30 = key_30 = None
        attention_score_17 = einsum_34 * 0.05103103630798288
        einsum_34 = None
        attention_probs_34 = torch.nn.functional.softmax(attention_score_17, dim=-1)
        attention_score_17 = None
        attention_probs_35 = torch.nn.functional.dropout(
            attention_probs_34, 0.0, False, False
        )
        attention_probs_34 = None
        context_34 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_35, value_30]
        )
        attention_probs_35 = value_30 = None
        permute_154 = context_34.permute(0, 2, 1, 3)
        context_34 = None
        contiguous_17 = permute_154.contiguous()
        permute_154 = None
        context_35 = contiguous_17.view(1, 197, 384)
        contiguous_17 = None
        hidden_state_338 = torch._C._nn.linear(
            context_35,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_35 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_339 = torch.nn.functional.dropout(
            hidden_state_338, 0.0, False, False
        )
        hidden_state_338 = None
        hidden_state_340 = hidden_state_339 + hidden_state_326
        hidden_state_339 = hidden_state_326 = None
        layer_output_17 = torch.nn.functional.layer_norm(
            hidden_state_340,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_341 = torch._C._nn.linear(
            layer_output_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_342 = torch._C._nn.gelu(hidden_state_341, approximate="none")
        hidden_state_341 = None
        hidden_state_343 = torch._C._nn.linear(
            hidden_state_342,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_342 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_12_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_344 = torch.nn.functional.dropout(
            hidden_state_343, 0.0, False, False
        )
        hidden_state_343 = None
        hidden_state_345 = hidden_state_344 + hidden_state_340
        hidden_state_344 = hidden_state_340 = None
        layer_norm_39 = torch.nn.functional.layer_norm(
            hidden_state_345,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_before_parameters_bias_ = (None)
        split_13 = torch.functional.split(layer_norm_39, [1, 196], 1)
        layer_norm_39 = None
        cls_token_14 = split_13[0]
        hidden_state_346 = split_13[1]
        split_13 = None
        permute_155 = hidden_state_346.permute(0, 2, 1)
        hidden_state_346 = None
        hidden_state_347 = permute_155.view(1, 384, 14, 14)
        permute_155 = None
        hidden_state_348 = torch.conv2d(
            hidden_state_347,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_349 = torch.nn.functional.batch_norm(
            hidden_state_348,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_348 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_156 = hidden_state_349.view(1, 384, 49)
        hidden_state_349 = None
        hidden_state_350 = view_156.permute(0, 2, 1)
        view_156 = None
        hidden_state_351 = torch.conv2d(
            hidden_state_347,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_352 = torch.nn.functional.batch_norm(
            hidden_state_351,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_351 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_157 = hidden_state_352.view(1, 384, 196)
        hidden_state_352 = None
        hidden_state_353 = view_157.permute(0, 2, 1)
        view_157 = None
        hidden_state_354 = torch.conv2d(
            hidden_state_347,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_347 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_355 = torch.nn.functional.batch_norm(
            hidden_state_354,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_354 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_158 = hidden_state_355.view(1, 384, 49)
        hidden_state_355 = None
        hidden_state_356 = view_158.permute(0, 2, 1)
        view_158 = None
        query_31 = torch.cat((cls_token_14, hidden_state_353), dim=1)
        hidden_state_353 = None
        key_31 = torch.cat((cls_token_14, hidden_state_350), dim=1)
        hidden_state_350 = None
        value_31 = torch.cat((cls_token_14, hidden_state_356), dim=1)
        cls_token_14 = hidden_state_356 = None
        linear_108 = torch._C._nn.linear(
            query_31,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_31 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_159 = linear_108.view(1, 197, 6, 64)
        linear_108 = None
        query_32 = view_159.permute(0, 2, 1, 3)
        view_159 = None
        linear_109 = torch._C._nn.linear(
            key_31,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_31 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_160 = linear_109.view(1, 50, 6, 64)
        linear_109 = None
        key_32 = view_160.permute(0, 2, 1, 3)
        view_160 = None
        linear_110 = torch._C._nn.linear(
            value_31,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_31 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_161 = linear_110.view(1, 50, 6, 64)
        linear_110 = None
        value_32 = view_161.permute(0, 2, 1, 3)
        view_161 = None
        einsum_36 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_32, key_32])
        query_32 = key_32 = None
        attention_score_18 = einsum_36 * 0.05103103630798288
        einsum_36 = None
        attention_probs_36 = torch.nn.functional.softmax(attention_score_18, dim=-1)
        attention_score_18 = None
        attention_probs_37 = torch.nn.functional.dropout(
            attention_probs_36, 0.0, False, False
        )
        attention_probs_36 = None
        context_36 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_37, value_32]
        )
        attention_probs_37 = value_32 = None
        permute_162 = context_36.permute(0, 2, 1, 3)
        context_36 = None
        contiguous_18 = permute_162.contiguous()
        permute_162 = None
        context_37 = contiguous_18.view(1, 197, 384)
        contiguous_18 = None
        hidden_state_357 = torch._C._nn.linear(
            context_37,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_37 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_358 = torch.nn.functional.dropout(
            hidden_state_357, 0.0, False, False
        )
        hidden_state_357 = None
        hidden_state_359 = hidden_state_358 + hidden_state_345
        hidden_state_358 = hidden_state_345 = None
        layer_output_18 = torch.nn.functional.layer_norm(
            hidden_state_359,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_360 = torch._C._nn.linear(
            layer_output_18,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_18 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_361 = torch._C._nn.gelu(hidden_state_360, approximate="none")
        hidden_state_360 = None
        hidden_state_362 = torch._C._nn.linear(
            hidden_state_361,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_361 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_13_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_363 = torch.nn.functional.dropout(
            hidden_state_362, 0.0, False, False
        )
        hidden_state_362 = None
        hidden_state_364 = hidden_state_363 + hidden_state_359
        hidden_state_363 = hidden_state_359 = None
        layer_norm_41 = torch.nn.functional.layer_norm(
            hidden_state_364,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_before_parameters_bias_ = (None)
        split_14 = torch.functional.split(layer_norm_41, [1, 196], 1)
        layer_norm_41 = None
        cls_token_15 = split_14[0]
        hidden_state_365 = split_14[1]
        split_14 = None
        permute_163 = hidden_state_365.permute(0, 2, 1)
        hidden_state_365 = None
        hidden_state_366 = permute_163.view(1, 384, 14, 14)
        permute_163 = None
        hidden_state_367 = torch.conv2d(
            hidden_state_366,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_368 = torch.nn.functional.batch_norm(
            hidden_state_367,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_367 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_164 = hidden_state_368.view(1, 384, 49)
        hidden_state_368 = None
        hidden_state_369 = view_164.permute(0, 2, 1)
        view_164 = None
        hidden_state_370 = torch.conv2d(
            hidden_state_366,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_371 = torch.nn.functional.batch_norm(
            hidden_state_370,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_370 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_165 = hidden_state_371.view(1, 384, 196)
        hidden_state_371 = None
        hidden_state_372 = view_165.permute(0, 2, 1)
        view_165 = None
        hidden_state_373 = torch.conv2d(
            hidden_state_366,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_366 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_374 = torch.nn.functional.batch_norm(
            hidden_state_373,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_373 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_166 = hidden_state_374.view(1, 384, 49)
        hidden_state_374 = None
        hidden_state_375 = view_166.permute(0, 2, 1)
        view_166 = None
        query_33 = torch.cat((cls_token_15, hidden_state_372), dim=1)
        hidden_state_372 = None
        key_33 = torch.cat((cls_token_15, hidden_state_369), dim=1)
        hidden_state_369 = None
        value_33 = torch.cat((cls_token_15, hidden_state_375), dim=1)
        cls_token_15 = hidden_state_375 = None
        linear_114 = torch._C._nn.linear(
            query_33,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_33 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_167 = linear_114.view(1, 197, 6, 64)
        linear_114 = None
        query_34 = view_167.permute(0, 2, 1, 3)
        view_167 = None
        linear_115 = torch._C._nn.linear(
            key_33,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_33 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_168 = linear_115.view(1, 50, 6, 64)
        linear_115 = None
        key_34 = view_168.permute(0, 2, 1, 3)
        view_168 = None
        linear_116 = torch._C._nn.linear(
            value_33,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_33 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_169 = linear_116.view(1, 50, 6, 64)
        linear_116 = None
        value_34 = view_169.permute(0, 2, 1, 3)
        view_169 = None
        einsum_38 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_34, key_34])
        query_34 = key_34 = None
        attention_score_19 = einsum_38 * 0.05103103630798288
        einsum_38 = None
        attention_probs_38 = torch.nn.functional.softmax(attention_score_19, dim=-1)
        attention_score_19 = None
        attention_probs_39 = torch.nn.functional.dropout(
            attention_probs_38, 0.0, False, False
        )
        attention_probs_38 = None
        context_38 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_39, value_34]
        )
        attention_probs_39 = value_34 = None
        permute_170 = context_38.permute(0, 2, 1, 3)
        context_38 = None
        contiguous_19 = permute_170.contiguous()
        permute_170 = None
        context_39 = contiguous_19.view(1, 197, 384)
        contiguous_19 = None
        hidden_state_376 = torch._C._nn.linear(
            context_39,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_39 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_377 = torch.nn.functional.dropout(
            hidden_state_376, 0.0, False, False
        )
        hidden_state_376 = None
        hidden_state_378 = hidden_state_377 + hidden_state_364
        hidden_state_377 = hidden_state_364 = None
        layer_output_19 = torch.nn.functional.layer_norm(
            hidden_state_378,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_379 = torch._C._nn.linear(
            layer_output_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_380 = torch._C._nn.gelu(hidden_state_379, approximate="none")
        hidden_state_379 = None
        hidden_state_381 = torch._C._nn.linear(
            hidden_state_380,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_380 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_14_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_382 = torch.nn.functional.dropout(
            hidden_state_381, 0.0, False, False
        )
        hidden_state_381 = None
        hidden_state_383 = hidden_state_382 + hidden_state_378
        hidden_state_382 = hidden_state_378 = None
        layer_norm_43 = torch.nn.functional.layer_norm(
            hidden_state_383,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_before_parameters_bias_ = (None)
        split_15 = torch.functional.split(layer_norm_43, [1, 196], 1)
        layer_norm_43 = None
        cls_token_16 = split_15[0]
        hidden_state_384 = split_15[1]
        split_15 = None
        permute_171 = hidden_state_384.permute(0, 2, 1)
        hidden_state_384 = None
        hidden_state_385 = permute_171.view(1, 384, 14, 14)
        permute_171 = None
        hidden_state_386 = torch.conv2d(
            hidden_state_385,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_387 = torch.nn.functional.batch_norm(
            hidden_state_386,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_386 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_172 = hidden_state_387.view(1, 384, 49)
        hidden_state_387 = None
        hidden_state_388 = view_172.permute(0, 2, 1)
        view_172 = None
        hidden_state_389 = torch.conv2d(
            hidden_state_385,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_390 = torch.nn.functional.batch_norm(
            hidden_state_389,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_389 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_173 = hidden_state_390.view(1, 384, 196)
        hidden_state_390 = None
        hidden_state_391 = view_173.permute(0, 2, 1)
        view_173 = None
        hidden_state_392 = torch.conv2d(
            hidden_state_385,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_385 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_393 = torch.nn.functional.batch_norm(
            hidden_state_392,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_392 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_174 = hidden_state_393.view(1, 384, 49)
        hidden_state_393 = None
        hidden_state_394 = view_174.permute(0, 2, 1)
        view_174 = None
        query_35 = torch.cat((cls_token_16, hidden_state_391), dim=1)
        hidden_state_391 = None
        key_35 = torch.cat((cls_token_16, hidden_state_388), dim=1)
        hidden_state_388 = None
        value_35 = torch.cat((cls_token_16, hidden_state_394), dim=1)
        cls_token_16 = hidden_state_394 = None
        linear_120 = torch._C._nn.linear(
            query_35,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_35 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_175 = linear_120.view(1, 197, 6, 64)
        linear_120 = None
        query_36 = view_175.permute(0, 2, 1, 3)
        view_175 = None
        linear_121 = torch._C._nn.linear(
            key_35,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_35 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_176 = linear_121.view(1, 50, 6, 64)
        linear_121 = None
        key_36 = view_176.permute(0, 2, 1, 3)
        view_176 = None
        linear_122 = torch._C._nn.linear(
            value_35,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_35 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_177 = linear_122.view(1, 50, 6, 64)
        linear_122 = None
        value_36 = view_177.permute(0, 2, 1, 3)
        view_177 = None
        einsum_40 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_36, key_36])
        query_36 = key_36 = None
        attention_score_20 = einsum_40 * 0.05103103630798288
        einsum_40 = None
        attention_probs_40 = torch.nn.functional.softmax(attention_score_20, dim=-1)
        attention_score_20 = None
        attention_probs_41 = torch.nn.functional.dropout(
            attention_probs_40, 0.0, False, False
        )
        attention_probs_40 = None
        context_40 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_41, value_36]
        )
        attention_probs_41 = value_36 = None
        permute_178 = context_40.permute(0, 2, 1, 3)
        context_40 = None
        contiguous_20 = permute_178.contiguous()
        permute_178 = None
        context_41 = contiguous_20.view(1, 197, 384)
        contiguous_20 = None
        hidden_state_395 = torch._C._nn.linear(
            context_41,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_41 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_396 = torch.nn.functional.dropout(
            hidden_state_395, 0.0, False, False
        )
        hidden_state_395 = None
        hidden_state_397 = hidden_state_396 + hidden_state_383
        hidden_state_396 = hidden_state_383 = None
        layer_output_20 = torch.nn.functional.layer_norm(
            hidden_state_397,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_398 = torch._C._nn.linear(
            layer_output_20,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_20 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_399 = torch._C._nn.gelu(hidden_state_398, approximate="none")
        hidden_state_398 = None
        hidden_state_400 = torch._C._nn.linear(
            hidden_state_399,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_399 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_15_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_401 = torch.nn.functional.dropout(
            hidden_state_400, 0.0, False, False
        )
        hidden_state_400 = None
        hidden_state_402 = hidden_state_401 + hidden_state_397
        hidden_state_401 = hidden_state_397 = None
        split_16 = torch.functional.split(hidden_state_402, [1, 196], 1)
        hidden_state_402 = None
        cls_token_17 = split_16[0]
        hidden_state_403 = split_16[1]
        split_16 = None
        permute_179 = hidden_state_403.permute(0, 2, 1)
        hidden_state_403 = None
        hidden_state_404 = permute_179.view(1, 384, 14, 14)
        permute_179 = None
        return (hidden_state_404, cls_token_17)
