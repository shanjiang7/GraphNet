import torch


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_pixel_values_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_: torch.Tensor,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_pixel_values_ = L_pixel_values_
        l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_ = (
            L_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_
        pixel_values = torch.conv2d(
            l_pixel_values_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_,
            (4, 4),
            (2, 2),
            (1, 1),
            1,
        )
        l_pixel_values_ = l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = (None)
        view = pixel_values.view(1, 64, 3136)
        pixel_values = None
        pixel_values_1 = view.permute(0, 2, 1)
        view = None
        pixel_values_2 = torch.nn.functional.layer_norm(
            pixel_values_1,
            (64,),
            l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_,
            1e-05,
        )
        pixel_values_1 = l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = (None)
        permute_1 = pixel_values_2.permute(0, 2, 1)
        pixel_values_2 = None
        pixel_values_3 = permute_1.view(1, 64, 56, 56)
        permute_1 = None
        hidden_state = torch.nn.functional.dropout(pixel_values_3, 0.0, False, False)
        pixel_values_3 = None
        view_2 = hidden_state.view(1, 64, 3136)
        hidden_state = None
        hidden_state_1 = view_2.permute(0, 2, 1)
        view_2 = None
        layer_norm_1 = torch.nn.functional.layer_norm(
            hidden_state_1,
            (64,),
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = (None)
        permute_3 = layer_norm_1.permute(0, 2, 1)
        layer_norm_1 = None
        hidden_state_2 = permute_3.view(1, 64, 56, 56)
        permute_3 = None
        hidden_state_3 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_4 = torch.nn.functional.batch_norm(
            hidden_state_3,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_3 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_4 = hidden_state_4.view(1, 64, 784)
        hidden_state_4 = None
        hidden_state_5 = view_4.permute(0, 2, 1)
        view_4 = None
        hidden_state_6 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            64,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_7 = torch.nn.functional.batch_norm(
            hidden_state_6,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_6 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_5 = hidden_state_7.view(1, 64, 3136)
        hidden_state_7 = None
        hidden_state_8 = view_5.permute(0, 2, 1)
        view_5 = None
        hidden_state_9 = torch.conv2d(
            hidden_state_2,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            64,
        )
        hidden_state_2 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_10 = torch.nn.functional.batch_norm(
            hidden_state_9,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_9 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_6 = hidden_state_10.view(1, 64, 784)
        hidden_state_10 = None
        hidden_state_11 = view_6.permute(0, 2, 1)
        view_6 = None
        linear = torch._C._nn.linear(
            hidden_state_8,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        hidden_state_8 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_7 = linear.view(1, 3136, 1, 64)
        linear = None
        query = view_7.permute(0, 2, 1, 3)
        view_7 = None
        linear_1 = torch._C._nn.linear(
            hidden_state_5,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        hidden_state_5 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_8 = linear_1.view(1, 784, 1, 64)
        linear_1 = None
        key = view_8.permute(0, 2, 1, 3)
        view_8 = None
        linear_2 = torch._C._nn.linear(
            hidden_state_11,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        hidden_state_11 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_9 = linear_2.view(1, 784, 1, 64)
        linear_2 = None
        value = view_9.permute(0, 2, 1, 3)
        view_9 = None
        einsum = torch.functional.einsum("bhlk,bhtk->bhlt", [query, key])
        query = key = None
        attention_score = einsum * 0.125
        einsum = None
        attention_probs = torch.nn.functional.softmax(attention_score, dim=-1)
        attention_score = None
        attention_probs_1 = torch.nn.functional.dropout(
            attention_probs, 0.0, False, False
        )
        attention_probs = None
        context = torch.functional.einsum("bhlt,bhtv->bhlv", [attention_probs_1, value])
        attention_probs_1 = value = None
        permute_10 = context.permute(0, 2, 1, 3)
        context = None
        contiguous = permute_10.contiguous()
        permute_10 = None
        context_1 = contiguous.view(1, 3136, 64)
        contiguous = None
        hidden_state_12 = torch._C._nn.linear(
            context_1,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_1 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_13 = torch.nn.functional.dropout(
            hidden_state_12, 0.0, False, False
        )
        hidden_state_12 = None
        hidden_state_14 = hidden_state_13 + hidden_state_1
        hidden_state_13 = hidden_state_1 = None
        layer_output = torch.nn.functional.layer_norm(
            hidden_state_14,
            (64,),
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_15 = torch._C._nn.linear(
            layer_output,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_16 = torch._C._nn.gelu(hidden_state_15, approximate="none")
        hidden_state_15 = None
        hidden_state_17 = torch._C._nn.linear(
            hidden_state_16,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_16 = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_0_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_18 = torch.nn.functional.dropout(
            hidden_state_17, 0.0, False, False
        )
        hidden_state_17 = None
        hidden_state_19 = hidden_state_18 + hidden_state_14
        hidden_state_18 = hidden_state_14 = None
        permute_11 = hidden_state_19.permute(0, 2, 1)
        hidden_state_19 = None
        hidden_state_20 = permute_11.view(1, 64, 56, 56)
        permute_11 = None
        pixel_values_4 = torch.conv2d(
            hidden_state_20,
            l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_20 = l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = (None)
        view_12 = pixel_values_4.view(1, 192, 784)
        pixel_values_4 = None
        pixel_values_5 = view_12.permute(0, 2, 1)
        view_12 = None
        pixel_values_6 = torch.nn.functional.layer_norm(
            pixel_values_5,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_,
            1e-05,
        )
        pixel_values_5 = l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = (None)
        permute_13 = pixel_values_6.permute(0, 2, 1)
        pixel_values_6 = None
        pixel_values_7 = permute_13.view(1, 192, 28, 28)
        permute_13 = None
        hidden_state_21 = torch.nn.functional.dropout(pixel_values_7, 0.0, False, False)
        pixel_values_7 = None
        view_14 = hidden_state_21.view(1, 192, 784)
        hidden_state_21 = None
        hidden_state_22 = view_14.permute(0, 2, 1)
        view_14 = None
        layer_norm_4 = torch.nn.functional.layer_norm(
            hidden_state_22,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = (None)
        permute_15 = layer_norm_4.permute(0, 2, 1)
        layer_norm_4 = None
        hidden_state_23 = permute_15.view(1, 192, 28, 28)
        permute_15 = None
        hidden_state_24 = torch.conv2d(
            hidden_state_23,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_25 = torch.nn.functional.batch_norm(
            hidden_state_24,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_24 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_16 = hidden_state_25.view(1, 192, 196)
        hidden_state_25 = None
        hidden_state_26 = view_16.permute(0, 2, 1)
        view_16 = None
        hidden_state_27 = torch.conv2d(
            hidden_state_23,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_28 = torch.nn.functional.batch_norm(
            hidden_state_27,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_27 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_17 = hidden_state_28.view(1, 192, 784)
        hidden_state_28 = None
        hidden_state_29 = view_17.permute(0, 2, 1)
        view_17 = None
        hidden_state_30 = torch.conv2d(
            hidden_state_23,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        hidden_state_23 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_31 = torch.nn.functional.batch_norm(
            hidden_state_30,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_30 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_18 = hidden_state_31.view(1, 192, 196)
        hidden_state_31 = None
        hidden_state_32 = view_18.permute(0, 2, 1)
        view_18 = None
        linear_6 = torch._C._nn.linear(
            hidden_state_29,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        hidden_state_29 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_19 = linear_6.view(1, 784, 3, 64)
        linear_6 = None
        query_1 = view_19.permute(0, 2, 1, 3)
        view_19 = None
        linear_7 = torch._C._nn.linear(
            hidden_state_26,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        hidden_state_26 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_20 = linear_7.view(1, 196, 3, 64)
        linear_7 = None
        key_1 = view_20.permute(0, 2, 1, 3)
        view_20 = None
        linear_8 = torch._C._nn.linear(
            hidden_state_32,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        hidden_state_32 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_21 = linear_8.view(1, 196, 3, 64)
        linear_8 = None
        value_1 = view_21.permute(0, 2, 1, 3)
        view_21 = None
        einsum_2 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_1, key_1])
        query_1 = key_1 = None
        attention_score_1 = einsum_2 * 0.07216878364870322
        einsum_2 = None
        attention_probs_2 = torch.nn.functional.softmax(attention_score_1, dim=-1)
        attention_score_1 = None
        attention_probs_3 = torch.nn.functional.dropout(
            attention_probs_2, 0.0, False, False
        )
        attention_probs_2 = None
        context_2 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_3, value_1]
        )
        attention_probs_3 = value_1 = None
        permute_22 = context_2.permute(0, 2, 1, 3)
        context_2 = None
        contiguous_1 = permute_22.contiguous()
        permute_22 = None
        context_3 = contiguous_1.view(1, 784, 192)
        contiguous_1 = None
        hidden_state_33 = torch._C._nn.linear(
            context_3,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_3 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_34 = torch.nn.functional.dropout(
            hidden_state_33, 0.0, False, False
        )
        hidden_state_33 = None
        hidden_state_35 = hidden_state_34 + hidden_state_22
        hidden_state_34 = hidden_state_22 = None
        layer_output_1 = torch.nn.functional.layer_norm(
            hidden_state_35,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_36 = torch._C._nn.linear(
            layer_output_1,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_1 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_37 = torch._C._nn.gelu(hidden_state_36, approximate="none")
        hidden_state_36 = None
        hidden_state_38 = torch._C._nn.linear(
            hidden_state_37,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_37 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_39 = torch.nn.functional.dropout(
            hidden_state_38, 0.0, False, False
        )
        hidden_state_38 = None
        hidden_state_40 = hidden_state_39 + hidden_state_35
        hidden_state_39 = hidden_state_35 = None
        layer_norm_6 = torch.nn.functional.layer_norm(
            hidden_state_40,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_before_parameters_bias_ = (None)
        permute_23 = layer_norm_6.permute(0, 2, 1)
        layer_norm_6 = None
        hidden_state_41 = permute_23.view(1, 192, 28, 28)
        permute_23 = None
        hidden_state_42 = torch.conv2d(
            hidden_state_41,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_43 = torch.nn.functional.batch_norm(
            hidden_state_42,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_42 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_24 = hidden_state_43.view(1, 192, 196)
        hidden_state_43 = None
        hidden_state_44 = view_24.permute(0, 2, 1)
        view_24 = None
        hidden_state_45 = torch.conv2d(
            hidden_state_41,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            192,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_46 = torch.nn.functional.batch_norm(
            hidden_state_45,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_45 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_25 = hidden_state_46.view(1, 192, 784)
        hidden_state_46 = None
        hidden_state_47 = view_25.permute(0, 2, 1)
        view_25 = None
        hidden_state_48 = torch.conv2d(
            hidden_state_41,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            192,
        )
        hidden_state_41 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_49 = torch.nn.functional.batch_norm(
            hidden_state_48,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_48 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_26 = hidden_state_49.view(1, 192, 196)
        hidden_state_49 = None
        hidden_state_50 = view_26.permute(0, 2, 1)
        view_26 = None
        linear_12 = torch._C._nn.linear(
            hidden_state_47,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        hidden_state_47 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_27 = linear_12.view(1, 784, 3, 64)
        linear_12 = None
        query_2 = view_27.permute(0, 2, 1, 3)
        view_27 = None
        linear_13 = torch._C._nn.linear(
            hidden_state_44,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        hidden_state_44 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_28 = linear_13.view(1, 196, 3, 64)
        linear_13 = None
        key_2 = view_28.permute(0, 2, 1, 3)
        view_28 = None
        linear_14 = torch._C._nn.linear(
            hidden_state_50,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        hidden_state_50 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_29 = linear_14.view(1, 196, 3, 64)
        linear_14 = None
        value_2 = view_29.permute(0, 2, 1, 3)
        view_29 = None
        einsum_4 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_2, key_2])
        query_2 = key_2 = None
        attention_score_2 = einsum_4 * 0.07216878364870322
        einsum_4 = None
        attention_probs_4 = torch.nn.functional.softmax(attention_score_2, dim=-1)
        attention_score_2 = None
        attention_probs_5 = torch.nn.functional.dropout(
            attention_probs_4, 0.0, False, False
        )
        attention_probs_4 = None
        context_4 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_5, value_2]
        )
        attention_probs_5 = value_2 = None
        permute_30 = context_4.permute(0, 2, 1, 3)
        context_4 = None
        contiguous_2 = permute_30.contiguous()
        permute_30 = None
        context_5 = contiguous_2.view(1, 784, 192)
        contiguous_2 = None
        hidden_state_51 = torch._C._nn.linear(
            context_5,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_5 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_52 = torch.nn.functional.dropout(
            hidden_state_51, 0.0, False, False
        )
        hidden_state_51 = None
        hidden_state_53 = hidden_state_52 + hidden_state_40
        hidden_state_52 = hidden_state_40 = None
        layer_output_2 = torch.nn.functional.layer_norm(
            hidden_state_53,
            (192,),
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_54 = torch._C._nn.linear(
            layer_output_2,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_2 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_55 = torch._C._nn.gelu(hidden_state_54, approximate="none")
        hidden_state_54 = None
        hidden_state_56 = torch._C._nn.linear(
            hidden_state_55,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_55 = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_1_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_57 = torch.nn.functional.dropout(
            hidden_state_56, 0.0, False, False
        )
        hidden_state_56 = None
        hidden_state_58 = hidden_state_57 + hidden_state_53
        hidden_state_57 = hidden_state_53 = None
        permute_31 = hidden_state_58.permute(0, 2, 1)
        hidden_state_58 = None
        hidden_state_59 = permute_31.view(1, 192, 28, 28)
        permute_31 = None
        pixel_values_8 = torch.conv2d(
            hidden_state_59,
            l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_,
            (2, 2),
            (1, 1),
            (1, 1),
            1,
        )
        hidden_state_59 = l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_projection_parameters_bias_ = (None)
        view_32 = pixel_values_8.view(1, 384, 196)
        pixel_values_8 = None
        pixel_values_9 = view_32.permute(0, 2, 1)
        view_32 = None
        pixel_values_10 = torch.nn.functional.layer_norm(
            pixel_values_9,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_,
            1e-05,
        )
        pixel_values_9 = l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_embedding_modules_convolution_embeddings_modules_normalization_parameters_bias_ = (None)
        permute_33 = pixel_values_10.permute(0, 2, 1)
        pixel_values_10 = None
        pixel_values_11 = permute_33.view(1, 384, 14, 14)
        permute_33 = None
        hidden_state_60 = torch.nn.functional.dropout(
            pixel_values_11, 0.0, False, False
        )
        pixel_values_11 = None
        view_34 = hidden_state_60.view(1, 384, 196)
        hidden_state_60 = None
        hidden_state_61 = view_34.permute(0, 2, 1)
        view_34 = None
        cls_token = l_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_.expand(
            1, -1, -1
        )
        l_self_modules_encoder_modules_stages_modules_2_parameters_cls_token_ = None
        hidden_state_62 = torch.cat((cls_token, hidden_state_61), dim=1)
        cls_token = hidden_state_61 = None
        layer_norm_9 = torch.nn.functional.layer_norm(
            hidden_state_62,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_before_parameters_bias_ = (None)
        split = torch.functional.split(layer_norm_9, [1, 196], 1)
        layer_norm_9 = None
        cls_token_1 = split[0]
        hidden_state_63 = split[1]
        split = None
        permute_35 = hidden_state_63.permute(0, 2, 1)
        hidden_state_63 = None
        hidden_state_64 = permute_35.view(1, 384, 14, 14)
        permute_35 = None
        hidden_state_65 = torch.conv2d(
            hidden_state_64,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_66 = torch.nn.functional.batch_norm(
            hidden_state_65,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_65 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_36 = hidden_state_66.view(1, 384, 49)
        hidden_state_66 = None
        hidden_state_67 = view_36.permute(0, 2, 1)
        view_36 = None
        hidden_state_68 = torch.conv2d(
            hidden_state_64,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_69 = torch.nn.functional.batch_norm(
            hidden_state_68,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_68 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_37 = hidden_state_69.view(1, 384, 196)
        hidden_state_69 = None
        hidden_state_70 = view_37.permute(0, 2, 1)
        view_37 = None
        hidden_state_71 = torch.conv2d(
            hidden_state_64,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_64 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_72 = torch.nn.functional.batch_norm(
            hidden_state_71,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_71 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_38 = hidden_state_72.view(1, 384, 49)
        hidden_state_72 = None
        hidden_state_73 = view_38.permute(0, 2, 1)
        view_38 = None
        query_3 = torch.cat((cls_token_1, hidden_state_70), dim=1)
        hidden_state_70 = None
        key_3 = torch.cat((cls_token_1, hidden_state_67), dim=1)
        hidden_state_67 = None
        value_3 = torch.cat((cls_token_1, hidden_state_73), dim=1)
        cls_token_1 = hidden_state_73 = None
        linear_18 = torch._C._nn.linear(
            query_3,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_3 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_39 = linear_18.view(1, 197, 6, 64)
        linear_18 = None
        query_4 = view_39.permute(0, 2, 1, 3)
        view_39 = None
        linear_19 = torch._C._nn.linear(
            key_3,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_3 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_40 = linear_19.view(1, 50, 6, 64)
        linear_19 = None
        key_4 = view_40.permute(0, 2, 1, 3)
        view_40 = None
        linear_20 = torch._C._nn.linear(
            value_3,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_3 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_41 = linear_20.view(1, 50, 6, 64)
        linear_20 = None
        value_4 = view_41.permute(0, 2, 1, 3)
        view_41 = None
        einsum_6 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_4, key_4])
        query_4 = key_4 = None
        attention_score_3 = einsum_6 * 0.05103103630798288
        einsum_6 = None
        attention_probs_6 = torch.nn.functional.softmax(attention_score_3, dim=-1)
        attention_score_3 = None
        attention_probs_7 = torch.nn.functional.dropout(
            attention_probs_6, 0.0, False, False
        )
        attention_probs_6 = None
        context_6 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_7, value_4]
        )
        attention_probs_7 = value_4 = None
        permute_42 = context_6.permute(0, 2, 1, 3)
        context_6 = None
        contiguous_3 = permute_42.contiguous()
        permute_42 = None
        context_7 = contiguous_3.view(1, 197, 384)
        contiguous_3 = None
        hidden_state_74 = torch._C._nn.linear(
            context_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_75 = torch.nn.functional.dropout(
            hidden_state_74, 0.0, False, False
        )
        hidden_state_74 = None
        hidden_state_76 = hidden_state_75 + hidden_state_62
        hidden_state_75 = hidden_state_62 = None
        layer_output_3 = torch.nn.functional.layer_norm(
            hidden_state_76,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_77 = torch._C._nn.linear(
            layer_output_3,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_3 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_78 = torch._C._nn.gelu(hidden_state_77, approximate="none")
        hidden_state_77 = None
        hidden_state_79 = torch._C._nn.linear(
            hidden_state_78,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_78 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_0_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_80 = torch.nn.functional.dropout(
            hidden_state_79, 0.0, False, False
        )
        hidden_state_79 = None
        hidden_state_81 = hidden_state_80 + hidden_state_76
        hidden_state_80 = hidden_state_76 = None
        layer_norm_11 = torch.nn.functional.layer_norm(
            hidden_state_81,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_before_parameters_bias_ = (None)
        split_1 = torch.functional.split(layer_norm_11, [1, 196], 1)
        layer_norm_11 = None
        cls_token_2 = split_1[0]
        hidden_state_82 = split_1[1]
        split_1 = None
        permute_43 = hidden_state_82.permute(0, 2, 1)
        hidden_state_82 = None
        hidden_state_83 = permute_43.view(1, 384, 14, 14)
        permute_43 = None
        hidden_state_84 = torch.conv2d(
            hidden_state_83,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_85 = torch.nn.functional.batch_norm(
            hidden_state_84,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_84 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_44 = hidden_state_85.view(1, 384, 49)
        hidden_state_85 = None
        hidden_state_86 = view_44.permute(0, 2, 1)
        view_44 = None
        hidden_state_87 = torch.conv2d(
            hidden_state_83,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_88 = torch.nn.functional.batch_norm(
            hidden_state_87,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_87 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_45 = hidden_state_88.view(1, 384, 196)
        hidden_state_88 = None
        hidden_state_89 = view_45.permute(0, 2, 1)
        view_45 = None
        hidden_state_90 = torch.conv2d(
            hidden_state_83,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_83 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_91 = torch.nn.functional.batch_norm(
            hidden_state_90,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_90 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_46 = hidden_state_91.view(1, 384, 49)
        hidden_state_91 = None
        hidden_state_92 = view_46.permute(0, 2, 1)
        view_46 = None
        query_5 = torch.cat((cls_token_2, hidden_state_89), dim=1)
        hidden_state_89 = None
        key_5 = torch.cat((cls_token_2, hidden_state_86), dim=1)
        hidden_state_86 = None
        value_5 = torch.cat((cls_token_2, hidden_state_92), dim=1)
        cls_token_2 = hidden_state_92 = None
        linear_24 = torch._C._nn.linear(
            query_5,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_5 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_47 = linear_24.view(1, 197, 6, 64)
        linear_24 = None
        query_6 = view_47.permute(0, 2, 1, 3)
        view_47 = None
        linear_25 = torch._C._nn.linear(
            key_5,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_5 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_48 = linear_25.view(1, 50, 6, 64)
        linear_25 = None
        key_6 = view_48.permute(0, 2, 1, 3)
        view_48 = None
        linear_26 = torch._C._nn.linear(
            value_5,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_5 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_49 = linear_26.view(1, 50, 6, 64)
        linear_26 = None
        value_6 = view_49.permute(0, 2, 1, 3)
        view_49 = None
        einsum_8 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_6, key_6])
        query_6 = key_6 = None
        attention_score_4 = einsum_8 * 0.05103103630798288
        einsum_8 = None
        attention_probs_8 = torch.nn.functional.softmax(attention_score_4, dim=-1)
        attention_score_4 = None
        attention_probs_9 = torch.nn.functional.dropout(
            attention_probs_8, 0.0, False, False
        )
        attention_probs_8 = None
        context_8 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_9, value_6]
        )
        attention_probs_9 = value_6 = None
        permute_50 = context_8.permute(0, 2, 1, 3)
        context_8 = None
        contiguous_4 = permute_50.contiguous()
        permute_50 = None
        context_9 = contiguous_4.view(1, 197, 384)
        contiguous_4 = None
        hidden_state_93 = torch._C._nn.linear(
            context_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_94 = torch.nn.functional.dropout(
            hidden_state_93, 0.0, False, False
        )
        hidden_state_93 = None
        hidden_state_95 = hidden_state_94 + hidden_state_81
        hidden_state_94 = hidden_state_81 = None
        layer_output_4 = torch.nn.functional.layer_norm(
            hidden_state_95,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_96 = torch._C._nn.linear(
            layer_output_4,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_4 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_97 = torch._C._nn.gelu(hidden_state_96, approximate="none")
        hidden_state_96 = None
        hidden_state_98 = torch._C._nn.linear(
            hidden_state_97,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_97 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_1_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_99 = torch.nn.functional.dropout(
            hidden_state_98, 0.0, False, False
        )
        hidden_state_98 = None
        hidden_state_100 = hidden_state_99 + hidden_state_95
        hidden_state_99 = hidden_state_95 = None
        layer_norm_13 = torch.nn.functional.layer_norm(
            hidden_state_100,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_before_parameters_bias_ = (None)
        split_2 = torch.functional.split(layer_norm_13, [1, 196], 1)
        layer_norm_13 = None
        cls_token_3 = split_2[0]
        hidden_state_101 = split_2[1]
        split_2 = None
        permute_51 = hidden_state_101.permute(0, 2, 1)
        hidden_state_101 = None
        hidden_state_102 = permute_51.view(1, 384, 14, 14)
        permute_51 = None
        hidden_state_103 = torch.conv2d(
            hidden_state_102,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_104 = torch.nn.functional.batch_norm(
            hidden_state_103,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_103 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_52 = hidden_state_104.view(1, 384, 49)
        hidden_state_104 = None
        hidden_state_105 = view_52.permute(0, 2, 1)
        view_52 = None
        hidden_state_106 = torch.conv2d(
            hidden_state_102,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_107 = torch.nn.functional.batch_norm(
            hidden_state_106,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_106 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_53 = hidden_state_107.view(1, 384, 196)
        hidden_state_107 = None
        hidden_state_108 = view_53.permute(0, 2, 1)
        view_53 = None
        hidden_state_109 = torch.conv2d(
            hidden_state_102,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_102 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_110 = torch.nn.functional.batch_norm(
            hidden_state_109,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_109 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_54 = hidden_state_110.view(1, 384, 49)
        hidden_state_110 = None
        hidden_state_111 = view_54.permute(0, 2, 1)
        view_54 = None
        query_7 = torch.cat((cls_token_3, hidden_state_108), dim=1)
        hidden_state_108 = None
        key_7 = torch.cat((cls_token_3, hidden_state_105), dim=1)
        hidden_state_105 = None
        value_7 = torch.cat((cls_token_3, hidden_state_111), dim=1)
        cls_token_3 = hidden_state_111 = None
        linear_30 = torch._C._nn.linear(
            query_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_55 = linear_30.view(1, 197, 6, 64)
        linear_30 = None
        query_8 = view_55.permute(0, 2, 1, 3)
        view_55 = None
        linear_31 = torch._C._nn.linear(
            key_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_56 = linear_31.view(1, 50, 6, 64)
        linear_31 = None
        key_8 = view_56.permute(0, 2, 1, 3)
        view_56 = None
        linear_32 = torch._C._nn.linear(
            value_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_57 = linear_32.view(1, 50, 6, 64)
        linear_32 = None
        value_8 = view_57.permute(0, 2, 1, 3)
        view_57 = None
        einsum_10 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_8, key_8])
        query_8 = key_8 = None
        attention_score_5 = einsum_10 * 0.05103103630798288
        einsum_10 = None
        attention_probs_10 = torch.nn.functional.softmax(attention_score_5, dim=-1)
        attention_score_5 = None
        attention_probs_11 = torch.nn.functional.dropout(
            attention_probs_10, 0.0, False, False
        )
        attention_probs_10 = None
        context_10 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_11, value_8]
        )
        attention_probs_11 = value_8 = None
        permute_58 = context_10.permute(0, 2, 1, 3)
        context_10 = None
        contiguous_5 = permute_58.contiguous()
        permute_58 = None
        context_11 = contiguous_5.view(1, 197, 384)
        contiguous_5 = None
        hidden_state_112 = torch._C._nn.linear(
            context_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_113 = torch.nn.functional.dropout(
            hidden_state_112, 0.0, False, False
        )
        hidden_state_112 = None
        hidden_state_114 = hidden_state_113 + hidden_state_100
        hidden_state_113 = hidden_state_100 = None
        layer_output_5 = torch.nn.functional.layer_norm(
            hidden_state_114,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_115 = torch._C._nn.linear(
            layer_output_5,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_5 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_116 = torch._C._nn.gelu(hidden_state_115, approximate="none")
        hidden_state_115 = None
        hidden_state_117 = torch._C._nn.linear(
            hidden_state_116,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_116 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_2_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_118 = torch.nn.functional.dropout(
            hidden_state_117, 0.0, False, False
        )
        hidden_state_117 = None
        hidden_state_119 = hidden_state_118 + hidden_state_114
        hidden_state_118 = hidden_state_114 = None
        layer_norm_15 = torch.nn.functional.layer_norm(
            hidden_state_119,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_before_parameters_bias_ = (None)
        split_3 = torch.functional.split(layer_norm_15, [1, 196], 1)
        layer_norm_15 = None
        cls_token_4 = split_3[0]
        hidden_state_120 = split_3[1]
        split_3 = None
        permute_59 = hidden_state_120.permute(0, 2, 1)
        hidden_state_120 = None
        hidden_state_121 = permute_59.view(1, 384, 14, 14)
        permute_59 = None
        hidden_state_122 = torch.conv2d(
            hidden_state_121,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_123 = torch.nn.functional.batch_norm(
            hidden_state_122,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_122 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_60 = hidden_state_123.view(1, 384, 49)
        hidden_state_123 = None
        hidden_state_124 = view_60.permute(0, 2, 1)
        view_60 = None
        hidden_state_125 = torch.conv2d(
            hidden_state_121,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_126 = torch.nn.functional.batch_norm(
            hidden_state_125,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_125 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_61 = hidden_state_126.view(1, 384, 196)
        hidden_state_126 = None
        hidden_state_127 = view_61.permute(0, 2, 1)
        view_61 = None
        hidden_state_128 = torch.conv2d(
            hidden_state_121,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_121 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_129 = torch.nn.functional.batch_norm(
            hidden_state_128,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_128 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_62 = hidden_state_129.view(1, 384, 49)
        hidden_state_129 = None
        hidden_state_130 = view_62.permute(0, 2, 1)
        view_62 = None
        query_9 = torch.cat((cls_token_4, hidden_state_127), dim=1)
        hidden_state_127 = None
        key_9 = torch.cat((cls_token_4, hidden_state_124), dim=1)
        hidden_state_124 = None
        value_9 = torch.cat((cls_token_4, hidden_state_130), dim=1)
        cls_token_4 = hidden_state_130 = None
        linear_36 = torch._C._nn.linear(
            query_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_63 = linear_36.view(1, 197, 6, 64)
        linear_36 = None
        query_10 = view_63.permute(0, 2, 1, 3)
        view_63 = None
        linear_37 = torch._C._nn.linear(
            key_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_64 = linear_37.view(1, 50, 6, 64)
        linear_37 = None
        key_10 = view_64.permute(0, 2, 1, 3)
        view_64 = None
        linear_38 = torch._C._nn.linear(
            value_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_65 = linear_38.view(1, 50, 6, 64)
        linear_38 = None
        value_10 = view_65.permute(0, 2, 1, 3)
        view_65 = None
        einsum_12 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_10, key_10])
        query_10 = key_10 = None
        attention_score_6 = einsum_12 * 0.05103103630798288
        einsum_12 = None
        attention_probs_12 = torch.nn.functional.softmax(attention_score_6, dim=-1)
        attention_score_6 = None
        attention_probs_13 = torch.nn.functional.dropout(
            attention_probs_12, 0.0, False, False
        )
        attention_probs_12 = None
        context_12 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_13, value_10]
        )
        attention_probs_13 = value_10 = None
        permute_66 = context_12.permute(0, 2, 1, 3)
        context_12 = None
        contiguous_6 = permute_66.contiguous()
        permute_66 = None
        context_13 = contiguous_6.view(1, 197, 384)
        contiguous_6 = None
        hidden_state_131 = torch._C._nn.linear(
            context_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_132 = torch.nn.functional.dropout(
            hidden_state_131, 0.0, False, False
        )
        hidden_state_131 = None
        hidden_state_133 = hidden_state_132 + hidden_state_119
        hidden_state_132 = hidden_state_119 = None
        layer_output_6 = torch.nn.functional.layer_norm(
            hidden_state_133,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_134 = torch._C._nn.linear(
            layer_output_6,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_6 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_135 = torch._C._nn.gelu(hidden_state_134, approximate="none")
        hidden_state_134 = None
        hidden_state_136 = torch._C._nn.linear(
            hidden_state_135,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_135 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_3_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_137 = torch.nn.functional.dropout(
            hidden_state_136, 0.0, False, False
        )
        hidden_state_136 = None
        hidden_state_138 = hidden_state_137 + hidden_state_133
        hidden_state_137 = hidden_state_133 = None
        layer_norm_17 = torch.nn.functional.layer_norm(
            hidden_state_138,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_before_parameters_bias_ = (None)
        split_4 = torch.functional.split(layer_norm_17, [1, 196], 1)
        layer_norm_17 = None
        cls_token_5 = split_4[0]
        hidden_state_139 = split_4[1]
        split_4 = None
        permute_67 = hidden_state_139.permute(0, 2, 1)
        hidden_state_139 = None
        hidden_state_140 = permute_67.view(1, 384, 14, 14)
        permute_67 = None
        hidden_state_141 = torch.conv2d(
            hidden_state_140,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_142 = torch.nn.functional.batch_norm(
            hidden_state_141,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_141 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_68 = hidden_state_142.view(1, 384, 49)
        hidden_state_142 = None
        hidden_state_143 = view_68.permute(0, 2, 1)
        view_68 = None
        hidden_state_144 = torch.conv2d(
            hidden_state_140,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_145 = torch.nn.functional.batch_norm(
            hidden_state_144,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_144 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_69 = hidden_state_145.view(1, 384, 196)
        hidden_state_145 = None
        hidden_state_146 = view_69.permute(0, 2, 1)
        view_69 = None
        hidden_state_147 = torch.conv2d(
            hidden_state_140,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_140 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_148 = torch.nn.functional.batch_norm(
            hidden_state_147,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_147 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_70 = hidden_state_148.view(1, 384, 49)
        hidden_state_148 = None
        hidden_state_149 = view_70.permute(0, 2, 1)
        view_70 = None
        query_11 = torch.cat((cls_token_5, hidden_state_146), dim=1)
        hidden_state_146 = None
        key_11 = torch.cat((cls_token_5, hidden_state_143), dim=1)
        hidden_state_143 = None
        value_11 = torch.cat((cls_token_5, hidden_state_149), dim=1)
        cls_token_5 = hidden_state_149 = None
        linear_42 = torch._C._nn.linear(
            query_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_71 = linear_42.view(1, 197, 6, 64)
        linear_42 = None
        query_12 = view_71.permute(0, 2, 1, 3)
        view_71 = None
        linear_43 = torch._C._nn.linear(
            key_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_72 = linear_43.view(1, 50, 6, 64)
        linear_43 = None
        key_12 = view_72.permute(0, 2, 1, 3)
        view_72 = None
        linear_44 = torch._C._nn.linear(
            value_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_73 = linear_44.view(1, 50, 6, 64)
        linear_44 = None
        value_12 = view_73.permute(0, 2, 1, 3)
        view_73 = None
        einsum_14 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_12, key_12])
        query_12 = key_12 = None
        attention_score_7 = einsum_14 * 0.05103103630798288
        einsum_14 = None
        attention_probs_14 = torch.nn.functional.softmax(attention_score_7, dim=-1)
        attention_score_7 = None
        attention_probs_15 = torch.nn.functional.dropout(
            attention_probs_14, 0.0, False, False
        )
        attention_probs_14 = None
        context_14 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_15, value_12]
        )
        attention_probs_15 = value_12 = None
        permute_74 = context_14.permute(0, 2, 1, 3)
        context_14 = None
        contiguous_7 = permute_74.contiguous()
        permute_74 = None
        context_15 = contiguous_7.view(1, 197, 384)
        contiguous_7 = None
        hidden_state_150 = torch._C._nn.linear(
            context_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_151 = torch.nn.functional.dropout(
            hidden_state_150, 0.0, False, False
        )
        hidden_state_150 = None
        hidden_state_152 = hidden_state_151 + hidden_state_138
        hidden_state_151 = hidden_state_138 = None
        layer_output_7 = torch.nn.functional.layer_norm(
            hidden_state_152,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_153 = torch._C._nn.linear(
            layer_output_7,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_7 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_154 = torch._C._nn.gelu(hidden_state_153, approximate="none")
        hidden_state_153 = None
        hidden_state_155 = torch._C._nn.linear(
            hidden_state_154,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_154 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_4_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_156 = torch.nn.functional.dropout(
            hidden_state_155, 0.0, False, False
        )
        hidden_state_155 = None
        hidden_state_157 = hidden_state_156 + hidden_state_152
        hidden_state_156 = hidden_state_152 = None
        layer_norm_19 = torch.nn.functional.layer_norm(
            hidden_state_157,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_before_parameters_bias_ = (None)
        split_5 = torch.functional.split(layer_norm_19, [1, 196], 1)
        layer_norm_19 = None
        cls_token_6 = split_5[0]
        hidden_state_158 = split_5[1]
        split_5 = None
        permute_75 = hidden_state_158.permute(0, 2, 1)
        hidden_state_158 = None
        hidden_state_159 = permute_75.view(1, 384, 14, 14)
        permute_75 = None
        hidden_state_160 = torch.conv2d(
            hidden_state_159,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_161 = torch.nn.functional.batch_norm(
            hidden_state_160,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_160 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_76 = hidden_state_161.view(1, 384, 49)
        hidden_state_161 = None
        hidden_state_162 = view_76.permute(0, 2, 1)
        view_76 = None
        hidden_state_163 = torch.conv2d(
            hidden_state_159,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_164 = torch.nn.functional.batch_norm(
            hidden_state_163,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_163 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_77 = hidden_state_164.view(1, 384, 196)
        hidden_state_164 = None
        hidden_state_165 = view_77.permute(0, 2, 1)
        view_77 = None
        hidden_state_166 = torch.conv2d(
            hidden_state_159,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_159 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_167 = torch.nn.functional.batch_norm(
            hidden_state_166,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_166 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_78 = hidden_state_167.view(1, 384, 49)
        hidden_state_167 = None
        hidden_state_168 = view_78.permute(0, 2, 1)
        view_78 = None
        query_13 = torch.cat((cls_token_6, hidden_state_165), dim=1)
        hidden_state_165 = None
        key_13 = torch.cat((cls_token_6, hidden_state_162), dim=1)
        hidden_state_162 = None
        value_13 = torch.cat((cls_token_6, hidden_state_168), dim=1)
        cls_token_6 = hidden_state_168 = None
        linear_48 = torch._C._nn.linear(
            query_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_79 = linear_48.view(1, 197, 6, 64)
        linear_48 = None
        query_14 = view_79.permute(0, 2, 1, 3)
        view_79 = None
        linear_49 = torch._C._nn.linear(
            key_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_80 = linear_49.view(1, 50, 6, 64)
        linear_49 = None
        key_14 = view_80.permute(0, 2, 1, 3)
        view_80 = None
        linear_50 = torch._C._nn.linear(
            value_13,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_13 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_81 = linear_50.view(1, 50, 6, 64)
        linear_50 = None
        value_14 = view_81.permute(0, 2, 1, 3)
        view_81 = None
        einsum_16 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_14, key_14])
        query_14 = key_14 = None
        attention_score_8 = einsum_16 * 0.05103103630798288
        einsum_16 = None
        attention_probs_16 = torch.nn.functional.softmax(attention_score_8, dim=-1)
        attention_score_8 = None
        attention_probs_17 = torch.nn.functional.dropout(
            attention_probs_16, 0.0, False, False
        )
        attention_probs_16 = None
        context_16 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_17, value_14]
        )
        attention_probs_17 = value_14 = None
        permute_82 = context_16.permute(0, 2, 1, 3)
        context_16 = None
        contiguous_8 = permute_82.contiguous()
        permute_82 = None
        context_17 = contiguous_8.view(1, 197, 384)
        contiguous_8 = None
        hidden_state_169 = torch._C._nn.linear(
            context_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_170 = torch.nn.functional.dropout(
            hidden_state_169, 0.0, False, False
        )
        hidden_state_169 = None
        hidden_state_171 = hidden_state_170 + hidden_state_157
        hidden_state_170 = hidden_state_157 = None
        layer_output_8 = torch.nn.functional.layer_norm(
            hidden_state_171,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_172 = torch._C._nn.linear(
            layer_output_8,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_8 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_173 = torch._C._nn.gelu(hidden_state_172, approximate="none")
        hidden_state_172 = None
        hidden_state_174 = torch._C._nn.linear(
            hidden_state_173,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_173 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_5_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_175 = torch.nn.functional.dropout(
            hidden_state_174, 0.0, False, False
        )
        hidden_state_174 = None
        hidden_state_176 = hidden_state_175 + hidden_state_171
        hidden_state_175 = hidden_state_171 = None
        layer_norm_21 = torch.nn.functional.layer_norm(
            hidden_state_176,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_before_parameters_bias_ = (None)
        split_6 = torch.functional.split(layer_norm_21, [1, 196], 1)
        layer_norm_21 = None
        cls_token_7 = split_6[0]
        hidden_state_177 = split_6[1]
        split_6 = None
        permute_83 = hidden_state_177.permute(0, 2, 1)
        hidden_state_177 = None
        hidden_state_178 = permute_83.view(1, 384, 14, 14)
        permute_83 = None
        hidden_state_179 = torch.conv2d(
            hidden_state_178,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_180 = torch.nn.functional.batch_norm(
            hidden_state_179,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_179 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_84 = hidden_state_180.view(1, 384, 49)
        hidden_state_180 = None
        hidden_state_181 = view_84.permute(0, 2, 1)
        view_84 = None
        hidden_state_182 = torch.conv2d(
            hidden_state_178,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_183 = torch.nn.functional.batch_norm(
            hidden_state_182,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_182 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_85 = hidden_state_183.view(1, 384, 196)
        hidden_state_183 = None
        hidden_state_184 = view_85.permute(0, 2, 1)
        view_85 = None
        hidden_state_185 = torch.conv2d(
            hidden_state_178,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_178 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_186 = torch.nn.functional.batch_norm(
            hidden_state_185,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_185 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_86 = hidden_state_186.view(1, 384, 49)
        hidden_state_186 = None
        hidden_state_187 = view_86.permute(0, 2, 1)
        view_86 = None
        query_15 = torch.cat((cls_token_7, hidden_state_184), dim=1)
        hidden_state_184 = None
        key_15 = torch.cat((cls_token_7, hidden_state_181), dim=1)
        hidden_state_181 = None
        value_15 = torch.cat((cls_token_7, hidden_state_187), dim=1)
        cls_token_7 = hidden_state_187 = None
        linear_54 = torch._C._nn.linear(
            query_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_87 = linear_54.view(1, 197, 6, 64)
        linear_54 = None
        query_16 = view_87.permute(0, 2, 1, 3)
        view_87 = None
        linear_55 = torch._C._nn.linear(
            key_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_88 = linear_55.view(1, 50, 6, 64)
        linear_55 = None
        key_16 = view_88.permute(0, 2, 1, 3)
        view_88 = None
        linear_56 = torch._C._nn.linear(
            value_15,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_15 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_89 = linear_56.view(1, 50, 6, 64)
        linear_56 = None
        value_16 = view_89.permute(0, 2, 1, 3)
        view_89 = None
        einsum_18 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_16, key_16])
        query_16 = key_16 = None
        attention_score_9 = einsum_18 * 0.05103103630798288
        einsum_18 = None
        attention_probs_18 = torch.nn.functional.softmax(attention_score_9, dim=-1)
        attention_score_9 = None
        attention_probs_19 = torch.nn.functional.dropout(
            attention_probs_18, 0.0, False, False
        )
        attention_probs_18 = None
        context_18 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_19, value_16]
        )
        attention_probs_19 = value_16 = None
        permute_90 = context_18.permute(0, 2, 1, 3)
        context_18 = None
        contiguous_9 = permute_90.contiguous()
        permute_90 = None
        context_19 = contiguous_9.view(1, 197, 384)
        contiguous_9 = None
        hidden_state_188 = torch._C._nn.linear(
            context_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_189 = torch.nn.functional.dropout(
            hidden_state_188, 0.0, False, False
        )
        hidden_state_188 = None
        hidden_state_190 = hidden_state_189 + hidden_state_176
        hidden_state_189 = hidden_state_176 = None
        layer_output_9 = torch.nn.functional.layer_norm(
            hidden_state_190,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_191 = torch._C._nn.linear(
            layer_output_9,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_9 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_192 = torch._C._nn.gelu(hidden_state_191, approximate="none")
        hidden_state_191 = None
        hidden_state_193 = torch._C._nn.linear(
            hidden_state_192,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_192 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_6_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_194 = torch.nn.functional.dropout(
            hidden_state_193, 0.0, False, False
        )
        hidden_state_193 = None
        hidden_state_195 = hidden_state_194 + hidden_state_190
        hidden_state_194 = hidden_state_190 = None
        layer_norm_23 = torch.nn.functional.layer_norm(
            hidden_state_195,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_before_parameters_bias_ = (None)
        split_7 = torch.functional.split(layer_norm_23, [1, 196], 1)
        layer_norm_23 = None
        cls_token_8 = split_7[0]
        hidden_state_196 = split_7[1]
        split_7 = None
        permute_91 = hidden_state_196.permute(0, 2, 1)
        hidden_state_196 = None
        hidden_state_197 = permute_91.view(1, 384, 14, 14)
        permute_91 = None
        hidden_state_198 = torch.conv2d(
            hidden_state_197,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_199 = torch.nn.functional.batch_norm(
            hidden_state_198,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_198 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_92 = hidden_state_199.view(1, 384, 49)
        hidden_state_199 = None
        hidden_state_200 = view_92.permute(0, 2, 1)
        view_92 = None
        hidden_state_201 = torch.conv2d(
            hidden_state_197,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_202 = torch.nn.functional.batch_norm(
            hidden_state_201,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_201 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_93 = hidden_state_202.view(1, 384, 196)
        hidden_state_202 = None
        hidden_state_203 = view_93.permute(0, 2, 1)
        view_93 = None
        hidden_state_204 = torch.conv2d(
            hidden_state_197,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_197 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_205 = torch.nn.functional.batch_norm(
            hidden_state_204,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_204 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_94 = hidden_state_205.view(1, 384, 49)
        hidden_state_205 = None
        hidden_state_206 = view_94.permute(0, 2, 1)
        view_94 = None
        query_17 = torch.cat((cls_token_8, hidden_state_203), dim=1)
        hidden_state_203 = None
        key_17 = torch.cat((cls_token_8, hidden_state_200), dim=1)
        hidden_state_200 = None
        value_17 = torch.cat((cls_token_8, hidden_state_206), dim=1)
        cls_token_8 = hidden_state_206 = None
        linear_60 = torch._C._nn.linear(
            query_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_95 = linear_60.view(1, 197, 6, 64)
        linear_60 = None
        query_18 = view_95.permute(0, 2, 1, 3)
        view_95 = None
        linear_61 = torch._C._nn.linear(
            key_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_96 = linear_61.view(1, 50, 6, 64)
        linear_61 = None
        key_18 = view_96.permute(0, 2, 1, 3)
        view_96 = None
        linear_62 = torch._C._nn.linear(
            value_17,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_17 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_97 = linear_62.view(1, 50, 6, 64)
        linear_62 = None
        value_18 = view_97.permute(0, 2, 1, 3)
        view_97 = None
        einsum_20 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_18, key_18])
        query_18 = key_18 = None
        attention_score_10 = einsum_20 * 0.05103103630798288
        einsum_20 = None
        attention_probs_20 = torch.nn.functional.softmax(attention_score_10, dim=-1)
        attention_score_10 = None
        attention_probs_21 = torch.nn.functional.dropout(
            attention_probs_20, 0.0, False, False
        )
        attention_probs_20 = None
        context_20 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_21, value_18]
        )
        attention_probs_21 = value_18 = None
        permute_98 = context_20.permute(0, 2, 1, 3)
        context_20 = None
        contiguous_10 = permute_98.contiguous()
        permute_98 = None
        context_21 = contiguous_10.view(1, 197, 384)
        contiguous_10 = None
        hidden_state_207 = torch._C._nn.linear(
            context_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_208 = torch.nn.functional.dropout(
            hidden_state_207, 0.0, False, False
        )
        hidden_state_207 = None
        hidden_state_209 = hidden_state_208 + hidden_state_195
        hidden_state_208 = hidden_state_195 = None
        layer_output_10 = torch.nn.functional.layer_norm(
            hidden_state_209,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_210 = torch._C._nn.linear(
            layer_output_10,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_10 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_211 = torch._C._nn.gelu(hidden_state_210, approximate="none")
        hidden_state_210 = None
        hidden_state_212 = torch._C._nn.linear(
            hidden_state_211,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_211 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_7_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_213 = torch.nn.functional.dropout(
            hidden_state_212, 0.0, False, False
        )
        hidden_state_212 = None
        hidden_state_214 = hidden_state_213 + hidden_state_209
        hidden_state_213 = hidden_state_209 = None
        layer_norm_25 = torch.nn.functional.layer_norm(
            hidden_state_214,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_before_parameters_bias_ = (None)
        split_8 = torch.functional.split(layer_norm_25, [1, 196], 1)
        layer_norm_25 = None
        cls_token_9 = split_8[0]
        hidden_state_215 = split_8[1]
        split_8 = None
        permute_99 = hidden_state_215.permute(0, 2, 1)
        hidden_state_215 = None
        hidden_state_216 = permute_99.view(1, 384, 14, 14)
        permute_99 = None
        hidden_state_217 = torch.conv2d(
            hidden_state_216,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_218 = torch.nn.functional.batch_norm(
            hidden_state_217,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_217 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_100 = hidden_state_218.view(1, 384, 49)
        hidden_state_218 = None
        hidden_state_219 = view_100.permute(0, 2, 1)
        view_100 = None
        hidden_state_220 = torch.conv2d(
            hidden_state_216,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_221 = torch.nn.functional.batch_norm(
            hidden_state_220,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_220 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_101 = hidden_state_221.view(1, 384, 196)
        hidden_state_221 = None
        hidden_state_222 = view_101.permute(0, 2, 1)
        view_101 = None
        hidden_state_223 = torch.conv2d(
            hidden_state_216,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_216 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_224 = torch.nn.functional.batch_norm(
            hidden_state_223,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_223 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_102 = hidden_state_224.view(1, 384, 49)
        hidden_state_224 = None
        hidden_state_225 = view_102.permute(0, 2, 1)
        view_102 = None
        query_19 = torch.cat((cls_token_9, hidden_state_222), dim=1)
        hidden_state_222 = None
        key_19 = torch.cat((cls_token_9, hidden_state_219), dim=1)
        hidden_state_219 = None
        value_19 = torch.cat((cls_token_9, hidden_state_225), dim=1)
        cls_token_9 = hidden_state_225 = None
        linear_66 = torch._C._nn.linear(
            query_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_103 = linear_66.view(1, 197, 6, 64)
        linear_66 = None
        query_20 = view_103.permute(0, 2, 1, 3)
        view_103 = None
        linear_67 = torch._C._nn.linear(
            key_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_104 = linear_67.view(1, 50, 6, 64)
        linear_67 = None
        key_20 = view_104.permute(0, 2, 1, 3)
        view_104 = None
        linear_68 = torch._C._nn.linear(
            value_19,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_19 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_105 = linear_68.view(1, 50, 6, 64)
        linear_68 = None
        value_20 = view_105.permute(0, 2, 1, 3)
        view_105 = None
        einsum_22 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_20, key_20])
        query_20 = key_20 = None
        attention_score_11 = einsum_22 * 0.05103103630798288
        einsum_22 = None
        attention_probs_22 = torch.nn.functional.softmax(attention_score_11, dim=-1)
        attention_score_11 = None
        attention_probs_23 = torch.nn.functional.dropout(
            attention_probs_22, 0.0, False, False
        )
        attention_probs_22 = None
        context_22 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_23, value_20]
        )
        attention_probs_23 = value_20 = None
        permute_106 = context_22.permute(0, 2, 1, 3)
        context_22 = None
        contiguous_11 = permute_106.contiguous()
        permute_106 = None
        context_23 = contiguous_11.view(1, 197, 384)
        contiguous_11 = None
        hidden_state_226 = torch._C._nn.linear(
            context_23,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_23 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_227 = torch.nn.functional.dropout(
            hidden_state_226, 0.0, False, False
        )
        hidden_state_226 = None
        hidden_state_228 = hidden_state_227 + hidden_state_214
        hidden_state_227 = hidden_state_214 = None
        layer_output_11 = torch.nn.functional.layer_norm(
            hidden_state_228,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_229 = torch._C._nn.linear(
            layer_output_11,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_11 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_230 = torch._C._nn.gelu(hidden_state_229, approximate="none")
        hidden_state_229 = None
        hidden_state_231 = torch._C._nn.linear(
            hidden_state_230,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_230 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_8_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_232 = torch.nn.functional.dropout(
            hidden_state_231, 0.0, False, False
        )
        hidden_state_231 = None
        hidden_state_233 = hidden_state_232 + hidden_state_228
        hidden_state_232 = hidden_state_228 = None
        layer_norm_27 = torch.nn.functional.layer_norm(
            hidden_state_233,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_before_parameters_bias_ = (None)
        split_9 = torch.functional.split(layer_norm_27, [1, 196], 1)
        layer_norm_27 = None
        cls_token_10 = split_9[0]
        hidden_state_234 = split_9[1]
        split_9 = None
        permute_107 = hidden_state_234.permute(0, 2, 1)
        hidden_state_234 = None
        hidden_state_235 = permute_107.view(1, 384, 14, 14)
        permute_107 = None
        hidden_state_236 = torch.conv2d(
            hidden_state_235,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_237 = torch.nn.functional.batch_norm(
            hidden_state_236,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_236 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_key_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_108 = hidden_state_237.view(1, 384, 49)
        hidden_state_237 = None
        hidden_state_238 = view_108.permute(0, 2, 1)
        view_108 = None
        hidden_state_239 = torch.conv2d(
            hidden_state_235,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (1, 1),
            (1, 1),
            (1, 1),
            384,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_convolution_parameters_weight_ = (
            None
        )
        hidden_state_240 = torch.nn.functional.batch_norm(
            hidden_state_239,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_239 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_query_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_109 = hidden_state_240.view(1, 384, 196)
        hidden_state_240 = None
        hidden_state_241 = view_109.permute(0, 2, 1)
        view_109 = None
        hidden_state_242 = torch.conv2d(
            hidden_state_235,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_,
            None,
            (2, 2),
            (1, 1),
            (1, 1),
            384,
        )
        hidden_state_235 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_convolution_parameters_weight_ = (None)
        hidden_state_243 = torch.nn.functional.batch_norm(
            hidden_state_242,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_,
            False,
            0.1,
            1e-05,
        )
        hidden_state_242 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_mean_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_buffers_running_var_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_convolution_projection_value_modules_convolution_projection_modules_normalization_parameters_bias_ = (None)
        view_110 = hidden_state_243.view(1, 384, 49)
        hidden_state_243 = None
        hidden_state_244 = view_110.permute(0, 2, 1)
        view_110 = None
        query_21 = torch.cat((cls_token_10, hidden_state_241), dim=1)
        hidden_state_241 = None
        key_21 = torch.cat((cls_token_10, hidden_state_238), dim=1)
        hidden_state_238 = None
        value_21 = torch.cat((cls_token_10, hidden_state_244), dim=1)
        cls_token_10 = hidden_state_244 = None
        linear_72 = torch._C._nn.linear(
            query_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_,
        )
        query_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_query_parameters_bias_ = (None)
        view_111 = linear_72.view(1, 197, 6, 64)
        linear_72 = None
        query_22 = view_111.permute(0, 2, 1, 3)
        view_111 = None
        linear_73 = torch._C._nn.linear(
            key_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_,
        )
        key_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_key_parameters_bias_ = (None)
        view_112 = linear_73.view(1, 50, 6, 64)
        linear_73 = None
        key_22 = view_112.permute(0, 2, 1, 3)
        view_112 = None
        linear_74 = torch._C._nn.linear(
            value_21,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_,
        )
        value_21 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_attention_modules_projection_value_parameters_bias_ = (None)
        view_113 = linear_74.view(1, 50, 6, 64)
        linear_74 = None
        value_22 = view_113.permute(0, 2, 1, 3)
        view_113 = None
        einsum_24 = torch.functional.einsum("bhlk,bhtk->bhlt", [query_22, key_22])
        query_22 = key_22 = None
        attention_score_12 = einsum_24 * 0.05103103630798288
        einsum_24 = None
        attention_probs_24 = torch.nn.functional.softmax(attention_score_12, dim=-1)
        attention_score_12 = None
        attention_probs_25 = torch.nn.functional.dropout(
            attention_probs_24, 0.0, False, False
        )
        attention_probs_24 = None
        context_24 = torch.functional.einsum(
            "bhlt,bhtv->bhlv", [attention_probs_25, value_22]
        )
        attention_probs_25 = value_22 = None
        permute_114 = context_24.permute(0, 2, 1, 3)
        context_24 = None
        contiguous_12 = permute_114.contiguous()
        permute_114 = None
        context_25 = contiguous_12.view(1, 197, 384)
        contiguous_12 = None
        hidden_state_245 = torch._C._nn.linear(
            context_25,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_,
        )
        context_25 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_246 = torch.nn.functional.dropout(
            hidden_state_245, 0.0, False, False
        )
        hidden_state_245 = None
        hidden_state_247 = hidden_state_246 + hidden_state_233
        hidden_state_246 = hidden_state_233 = None
        layer_output_12 = torch.nn.functional.layer_norm(
            hidden_state_247,
            (384,),
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_,
            1e-05,
        )
        l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_layernorm_after_parameters_bias_ = (None)
        hidden_state_248 = torch._C._nn.linear(
            layer_output_12,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_,
        )
        layer_output_12 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_intermediate_modules_dense_parameters_bias_ = (None)
        hidden_state_249 = torch._C._nn.gelu(hidden_state_248, approximate="none")
        hidden_state_248 = None
        hidden_state_250 = torch._C._nn.linear(
            hidden_state_249,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_,
            l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_,
        )
        hidden_state_249 = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_stages_modules_2_modules_layers_modules_9_modules_output_modules_dense_parameters_bias_ = (None)
        hidden_state_251 = torch.nn.functional.dropout(
            hidden_state_250, 0.0, False, False
        )
        hidden_state_250 = None
        hidden_state_252 = hidden_state_251 + hidden_state_247
        hidden_state_251 = hidden_state_247 = None
        split_10 = torch.functional.split(hidden_state_252, [1, 196], 1)
        hidden_state_252 = None
        cls_token_11 = split_10[0]
        hidden_state_253 = split_10[1]
        split_10 = None
        permute_115 = hidden_state_253.permute(0, 2, 1)
        hidden_state_253 = None
        hidden_state_254 = permute_115.view(1, 384, 14, 14)
        permute_115 = None
        return (hidden_state_254, cls_token_11)
