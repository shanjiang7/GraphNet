class Program_weight_tensor_meta_L_kwargs_input_ids_:
    name = "L_kwargs_input_ids_"
    shape = [1, 19]
    dtype = "torch.int64"
    device = "cuda:0"
    mean = None
    std = None
    data = [
        12092,
        13,
        619,
        1416,
        310,
        8679,
        15,
        309,
        717,
        4715,
        670,
        1781,
        3448,
        3210,
        285,
        616,
        35615,
        15,
        209,
    ]


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_embed_in_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_embed_in_parameters_weight_"
    shape = [50304, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_kwargs_attention_mask_:
    name = "L_kwargs_attention_mask_"
    shape = [1, 19]
    dtype = "torch.int64"
    device = "cuda:0"
    mean = None
    std = None
    data = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_rotary_emb_buffers_inv_freq_:
    name = "L_self_modules_gpt_neox_modules_rotary_emb_buffers_inv_freq_"
    shape = [8]
    dtype = "torch.float32"
    device = "cuda:0"
    mean = 0.183
    std = 0.347
    data = [
        1.000000,
        0.316228,
        0.100000,
        0.031623,
        0.010000,
        0.003162,
        0.001000,
        0.000316,
    ]


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_input_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_input_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_input_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_input_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_attention_modules_query_key_value_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_attention_modules_query_key_value_parameters_weight_"
    shape = [1536, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_attention_modules_query_key_value_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_attention_modules_query_key_value_parameters_bias_"
    shape = [1536]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_attention_modules_dense_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_attention_modules_dense_parameters_weight_"
    shape = [512, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_attention_modules_dense_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_attention_modules_dense_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_post_attention_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_post_attention_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_mlp_modules_dense_h_to_4h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_mlp_modules_dense_h_to_4h_parameters_weight_"
    shape = [2048, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_mlp_modules_dense_h_to_4h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_mlp_modules_dense_h_to_4h_parameters_bias_"
    shape = [2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_mlp_modules_dense_4h_to_h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_mlp_modules_dense_4h_to_h_parameters_weight_"
    shape = [512, 2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_0_modules_mlp_modules_dense_4h_to_h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_0_modules_mlp_modules_dense_4h_to_h_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_input_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_input_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_input_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_input_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_attention_modules_query_key_value_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_attention_modules_query_key_value_parameters_weight_"
    shape = [1536, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_attention_modules_query_key_value_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_attention_modules_query_key_value_parameters_bias_"
    shape = [1536]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_attention_modules_dense_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_attention_modules_dense_parameters_weight_"
    shape = [512, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_attention_modules_dense_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_attention_modules_dense_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_post_attention_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_post_attention_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_mlp_modules_dense_h_to_4h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_mlp_modules_dense_h_to_4h_parameters_weight_"
    shape = [2048, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_mlp_modules_dense_h_to_4h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_mlp_modules_dense_h_to_4h_parameters_bias_"
    shape = [2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_mlp_modules_dense_4h_to_h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_mlp_modules_dense_4h_to_h_parameters_weight_"
    shape = [512, 2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_1_modules_mlp_modules_dense_4h_to_h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_1_modules_mlp_modules_dense_4h_to_h_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_input_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_input_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_input_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_input_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_attention_modules_query_key_value_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_attention_modules_query_key_value_parameters_weight_"
    shape = [1536, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_attention_modules_query_key_value_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_attention_modules_query_key_value_parameters_bias_"
    shape = [1536]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_attention_modules_dense_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_attention_modules_dense_parameters_weight_"
    shape = [512, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_attention_modules_dense_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_attention_modules_dense_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_post_attention_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_post_attention_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_mlp_modules_dense_h_to_4h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_mlp_modules_dense_h_to_4h_parameters_weight_"
    shape = [2048, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_mlp_modules_dense_h_to_4h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_mlp_modules_dense_h_to_4h_parameters_bias_"
    shape = [2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_mlp_modules_dense_4h_to_h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_mlp_modules_dense_4h_to_h_parameters_weight_"
    shape = [512, 2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_2_modules_mlp_modules_dense_4h_to_h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_2_modules_mlp_modules_dense_4h_to_h_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_input_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_input_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_input_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_input_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_attention_modules_query_key_value_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_attention_modules_query_key_value_parameters_weight_"
    shape = [1536, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_attention_modules_query_key_value_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_attention_modules_query_key_value_parameters_bias_"
    shape = [1536]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_attention_modules_dense_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_attention_modules_dense_parameters_weight_"
    shape = [512, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_attention_modules_dense_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_attention_modules_dense_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_post_attention_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_post_attention_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_mlp_modules_dense_h_to_4h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_mlp_modules_dense_h_to_4h_parameters_weight_"
    shape = [2048, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_mlp_modules_dense_h_to_4h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_mlp_modules_dense_h_to_4h_parameters_bias_"
    shape = [2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_mlp_modules_dense_4h_to_h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_mlp_modules_dense_4h_to_h_parameters_weight_"
    shape = [512, 2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_3_modules_mlp_modules_dense_4h_to_h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_3_modules_mlp_modules_dense_4h_to_h_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_input_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_input_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_input_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_input_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_attention_modules_query_key_value_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_attention_modules_query_key_value_parameters_weight_"
    shape = [1536, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_attention_modules_query_key_value_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_attention_modules_query_key_value_parameters_bias_"
    shape = [1536]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_attention_modules_dense_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_attention_modules_dense_parameters_weight_"
    shape = [512, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_attention_modules_dense_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_attention_modules_dense_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_post_attention_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_post_attention_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_mlp_modules_dense_h_to_4h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_mlp_modules_dense_h_to_4h_parameters_weight_"
    shape = [2048, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_mlp_modules_dense_h_to_4h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_mlp_modules_dense_h_to_4h_parameters_bias_"
    shape = [2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_mlp_modules_dense_4h_to_h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_mlp_modules_dense_4h_to_h_parameters_weight_"
    shape = [512, 2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_4_modules_mlp_modules_dense_4h_to_h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_4_modules_mlp_modules_dense_4h_to_h_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_input_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_input_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_input_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_input_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_attention_modules_query_key_value_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_attention_modules_query_key_value_parameters_weight_"
    shape = [1536, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_attention_modules_query_key_value_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_attention_modules_query_key_value_parameters_bias_"
    shape = [1536]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_attention_modules_dense_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_attention_modules_dense_parameters_weight_"
    shape = [512, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_attention_modules_dense_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_attention_modules_dense_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_post_attention_layernorm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_post_attention_layernorm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_mlp_modules_dense_h_to_4h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_mlp_modules_dense_h_to_4h_parameters_weight_"
    shape = [2048, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_mlp_modules_dense_h_to_4h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_mlp_modules_dense_h_to_4h_parameters_bias_"
    shape = [2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_mlp_modules_dense_4h_to_h_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_mlp_modules_dense_4h_to_h_parameters_weight_"
    shape = [512, 2048]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_layers_modules_5_modules_mlp_modules_dense_4h_to_h_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_layers_modules_5_modules_mlp_modules_dense_4h_to_h_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_final_layer_norm_parameters_weight_:
    name = "L_self_modules_gpt_neox_modules_final_layer_norm_parameters_weight_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 1.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_gpt_neox_modules_final_layer_norm_parameters_bias_:
    name = "L_self_modules_gpt_neox_modules_final_layer_norm_parameters_bias_"
    shape = [512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = 0.000
    std = 0.000
    data = None


class Program_weight_tensor_meta_L_self_modules_embed_out_parameters_weight_:
    name = "L_self_modules_embed_out_parameters_weight_"
    shape = [50304, 512]
    dtype = "torch.float16"
    device = "cuda:0"
    mean = -0.000
    std = 0.020
    data = None
