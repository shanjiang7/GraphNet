import torch

from torch import device


class GraphModule(torch.nn.Module):
    def forward(
        self,
        L_waveforms_: torch.Tensor,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_parameters_gru_rel_pos_const_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_bias_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_weight_: torch.nn.parameter.Parameter,
        L_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_bias_: torch.nn.parameter.Parameter,
    ):
        l_waveforms_ = L_waveforms_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_weight_ = L_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_weight_
        l_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_bias_ = L_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_ = L_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_
        l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_ = L_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_
        l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_parameters_gru_rel_pos_const_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_parameters_gru_rel_pos_const_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_bias_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_weight_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_weight_
        l_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_bias_ = L_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_bias_
        waveforms = torch.nn.functional.layer_norm(l_waveforms_, (1, 64000))
        l_waveforms_ = None
        x = waveforms.unsqueeze(1)
        waveforms = None
        x_1 = torch.conv1d(
            x,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_,
            None,
            (5,),
            (0,),
            (1,),
            1,
        )
        x = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_conv_parameters_weight_ = (None)
        x_2 = x_1.transpose(-2, -1)
        x_1 = None
        x_3 = torch.nn.functional.layer_norm(
            x_2,
            (512,),
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_2 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_0_modules_layer_norm_parameters_bias_ = (None)
        x_4 = x_3.transpose(-2, -1)
        x_3 = None
        x_5 = torch._C._nn.gelu(x_4)
        x_4 = None
        x_6 = torch.conv1d(
            x_5,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        x_5 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_conv_parameters_weight_ = (None)
        x_7 = x_6.transpose(-2, -1)
        x_6 = None
        x_8 = torch.nn.functional.layer_norm(
            x_7,
            (512,),
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_7 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_1_modules_layer_norm_parameters_bias_ = (None)
        x_9 = x_8.transpose(-2, -1)
        x_8 = None
        x_10 = torch._C._nn.gelu(x_9)
        x_9 = None
        x_11 = torch.conv1d(
            x_10,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        x_10 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_conv_parameters_weight_ = (None)
        x_12 = x_11.transpose(-2, -1)
        x_11 = None
        x_13 = torch.nn.functional.layer_norm(
            x_12,
            (512,),
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_12 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_2_modules_layer_norm_parameters_bias_ = (None)
        x_14 = x_13.transpose(-2, -1)
        x_13 = None
        x_15 = torch._C._nn.gelu(x_14)
        x_14 = None
        x_16 = torch.conv1d(
            x_15,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        x_15 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_conv_parameters_weight_ = (None)
        x_17 = x_16.transpose(-2, -1)
        x_16 = None
        x_18 = torch.nn.functional.layer_norm(
            x_17,
            (512,),
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_17 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_3_modules_layer_norm_parameters_bias_ = (None)
        x_19 = x_18.transpose(-2, -1)
        x_18 = None
        x_20 = torch._C._nn.gelu(x_19)
        x_19 = None
        x_21 = torch.conv1d(
            x_20,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        x_20 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_conv_parameters_weight_ = (None)
        x_22 = x_21.transpose(-2, -1)
        x_21 = None
        x_23 = torch.nn.functional.layer_norm(
            x_22,
            (512,),
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_22 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_4_modules_layer_norm_parameters_bias_ = (None)
        x_24 = x_23.transpose(-2, -1)
        x_23 = None
        x_25 = torch._C._nn.gelu(x_24)
        x_24 = None
        x_26 = torch.conv1d(
            x_25,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        x_25 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_conv_parameters_weight_ = (None)
        x_27 = x_26.transpose(-2, -1)
        x_26 = None
        x_28 = torch.nn.functional.layer_norm(
            x_27,
            (512,),
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_27 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_5_modules_layer_norm_parameters_bias_ = (None)
        x_29 = x_28.transpose(-2, -1)
        x_28 = None
        x_30 = torch._C._nn.gelu(x_29)
        x_29 = None
        x_31 = torch.conv1d(
            x_30,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_,
            None,
            (2,),
            (0,),
            (1,),
            1,
        )
        x_30 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_conv_parameters_weight_ = (None)
        x_32 = x_31.transpose(-2, -1)
        x_31 = None
        x_33 = torch.nn.functional.layer_norm(
            x_32,
            (512,),
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_32 = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_feature_extractor_modules_conv_layers_modules_6_modules_layer_norm_parameters_bias_ = (None)
        x_34 = x_33.transpose(-2, -1)
        x_33 = None
        x_35 = torch._C._nn.gelu(x_34)
        x_34 = None
        x_36 = x_35.transpose(1, 2)
        x_35 = None
        x_37 = torch.nn.functional.layer_norm(
            x_36,
            (512,),
            l_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_36 = l_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_feature_projection_modules_layer_norm_parameters_bias_ = (None)
        x_38 = torch._C._nn.linear(
            x_37,
            l_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_bias_,
        )
        x_37 = l_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_weight_ = l_self_modules_model_modules_encoder_modules_feature_projection_modules_projection_parameters_bias_ = (None)
        x_39 = torch.nn.functional.dropout(x_38, 0.1, False, False)
        x_38 = None
        x_40 = x_39.transpose(-2, -1)
        x_41 = torch._weight_norm(
            l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_,
            2,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original1_ = l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_modules_parametrizations_modules_weight_parameters_original0_ = (None)
        x_42 = torch.conv1d(
            x_40,
            x_41,
            l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_parameters_bias_,
            (1,),
            (64,),
            (1,),
            16,
        )
        x_40 = (
            x_41
        ) = l_self_modules_model_modules_encoder_modules_transformer_modules_pos_conv_embed_modules_conv_parameters_bias_ = (None)
        x_43 = x_42[(Ellipsis, slice(None, -1, None))]
        x_42 = None
        x_44 = torch._C._nn.gelu(x_43)
        x_43 = None
        x_45 = x_44.transpose(-2, -1)
        x_44 = None
        x_46 = x_39 + x_45
        x_39 = x_45 = None
        x_47 = torch.nn.functional.dropout(x_46, 0.1, False, False)
        x_46 = None
        x_48 = torch.nn.functional.layer_norm(
            x_47,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_layer_norm_parameters_bias_ = (None)
        arange = torch.arange(199, dtype=torch.int64)
        context_position = arange[(slice(None, None, None), None)]
        arange = None
        arange_1 = torch.arange(199, dtype=torch.int64)
        memory_position = arange_1[(None, slice(None, None, None))]
        arange_1 = None
        relative_position = memory_position - context_position
        memory_position = context_position = None
        relative_buckets = torch.zeros_like(relative_position, dtype=torch.int64)
        gt = relative_position > 0
        to = gt.to(torch.int64)
        gt = None
        mul = to * 160
        to = None
        relative_buckets += mul
        relative_buckets_1 = relative_buckets
        relative_buckets = mul = None
        relative_positions = torch.abs(relative_position)
        relative_position = None
        is_small = relative_positions < 80
        float_1 = relative_positions.float()
        truediv = float_1 / 80
        float_1 = None
        log = torch.log(truediv)
        truediv = None
        truediv_1 = log / 2.302585092994046
        log = None
        mul_1 = truediv_1 * 80
        truediv_1 = None
        to_1 = mul_1.to(torch.int64)
        mul_1 = None
        relative_postion_if_large = 80 + to_1
        to_1 = None
        full_like = torch.full_like(relative_postion_if_large, 159)
        relative_postion_if_large_1 = torch.min(relative_postion_if_large, full_like)
        relative_postion_if_large = full_like = None
        where = torch.where(is_small, relative_positions, relative_postion_if_large_1)
        is_small = relative_positions = relative_postion_if_large_1 = None
        relative_buckets_1 += where
        relative_buckets_2 = relative_buckets_1
        relative_buckets_1 = where = None
        relative_position_bucket = relative_buckets_2.to(device(type="cuda", index=0))
        relative_buckets_2 = None
        values = torch.nn.functional.embedding(
            relative_position_bucket,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_,
            None,
            None,
            2.0,
            False,
            False,
        )
        relative_position_bucket = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_rel_attn_embed_parameters_weight_ = (None)
        values_1 = values.permute([2, 0, 1])
        values = None
        unsqueeze_1 = values_1.unsqueeze(0)
        values_1 = None
        position_bias = unsqueeze_1.repeat(1, 1, 1, 1)
        unsqueeze_1 = None
        query_layer = x_48.view(1, 199, 16, -1)
        query_layer_1 = query_layer.permute(0, 2, 1, 3)
        query_layer = None
        linear_1 = torch._C._nn.linear(
            query_layer_1,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_1 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_1 = linear_1.view(1, 16, 199, 2, 4)
        linear_1 = None
        sum_1 = view_1.sum(-1, keepdim=False)
        view_1 = None
        sigmoid = torch.sigmoid(sum_1)
        sum_1 = None
        chunk = sigmoid.chunk(2, dim=-1)
        sigmoid = None
        gate_a = chunk[0]
        gate_b = chunk[1]
        chunk = None
        mul_2 = (
            gate_b
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_1 = mul_2 - 1.0
        mul_2 = None
        mul_3 = gate_a * sub_1
        gate_a = sub_1 = None
        gate_a_1 = mul_3 + 2.0
        mul_3 = None
        view_2 = gate_a_1.view(1, 16, -1, 1)
        gate_a_1 = None
        attn_mask_rel_pos = view_2 * position_bias
        view_2 = None
        attn_mask_rel_pos_1 = attn_mask_rel_pos.view((1, 16, 199, 199))
        attn_mask_rel_pos = None
        query_projected = torch._C._nn.linear(
            x_48,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_48 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_1 = query_projected.chunk(3, -1)
        query_projected = None
        query = chunk_1[0]
        key = chunk_1[1]
        value = chunk_1[2]
        chunk_1 = None
        view_4 = query.view((1, 199, 16, 64))
        query = None
        query_1 = view_4.transpose(2, 1)
        view_4 = None
        view_5 = key.view((1, 199, 16, 64))
        key = None
        key_1 = view_5.transpose(2, 1)
        view_5 = None
        view_6 = value.view((1, 199, 16, 64))
        value = None
        value_1 = view_6.transpose(2, 1)
        view_6 = None
        attn_output = torch._C._nn.scaled_dot_product_attention(
            query_1,
            key_1,
            value_1,
            attn_mask=attn_mask_rel_pos_1,
            dropout_p=0.0,
            is_causal=False,
        )
        query_1 = key_1 = value_1 = attn_mask_rel_pos_1 = None
        transpose_20 = attn_output.transpose(1, 2)
        attn_output = None
        attn_output_1 = transpose_20.reshape(1, -1, 1024)
        transpose_20 = None
        attn_output_2 = torch._C._nn.linear(
            attn_output_1,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_1 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_49 = torch.nn.functional.dropout(attn_output_2, 0.1, False, False)
        attn_output_2 = None
        x_50 = x_47 + x_49
        x_47 = x_49 = None
        layer_norm_10 = torch.nn.functional.layer_norm(
            x_50,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_final_layer_norm_parameters_bias_ = (None)
        x_51 = torch._C._nn.linear(
            layer_norm_10,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_10 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_52 = torch._C._nn.gelu(x_51)
        x_51 = None
        x_53 = torch.nn.functional.dropout(x_52, 0.0, False, False)
        x_52 = None
        x_54 = torch._C._nn.linear(
            x_53,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_53 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_0_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_55 = torch.nn.functional.dropout(x_54, 0.1, False, False)
        x_54 = None
        x_56 = x_50 + x_55
        x_50 = x_55 = None
        x_57 = torch.nn.functional.layer_norm(
            x_56,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_layer_norm_parameters_bias_ = (None)
        query_layer_2 = x_57.view(1, 199, 16, -1)
        query_layer_3 = query_layer_2.permute(0, 2, 1, 3)
        query_layer_2 = None
        linear_6 = torch._C._nn.linear(
            query_layer_3,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_3 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_8 = linear_6.view(1, 16, 199, 2, 4)
        linear_6 = None
        sum_2 = view_8.sum(-1, keepdim=False)
        view_8 = None
        sigmoid_1 = torch.sigmoid(sum_2)
        sum_2 = None
        chunk_2 = sigmoid_1.chunk(2, dim=-1)
        sigmoid_1 = None
        gate_a_2 = chunk_2[0]
        gate_b_1 = chunk_2[1]
        chunk_2 = None
        mul_5 = (
            gate_b_1
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_1 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_2 = mul_5 - 1.0
        mul_5 = None
        mul_6 = gate_a_2 * sub_2
        gate_a_2 = sub_2 = None
        gate_a_3 = mul_6 + 2.0
        mul_6 = None
        view_9 = gate_a_3.view(1, 16, -1, 1)
        gate_a_3 = None
        attn_mask_rel_pos_2 = view_9 * position_bias
        view_9 = None
        attn_mask_rel_pos_3 = attn_mask_rel_pos_2.view((1, 16, 199, 199))
        attn_mask_rel_pos_2 = None
        query_projected_1 = torch._C._nn.linear(
            x_57,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_57 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_3 = query_projected_1.chunk(3, -1)
        query_projected_1 = None
        query_2 = chunk_3[0]
        key_2 = chunk_3[1]
        value_2 = chunk_3[2]
        chunk_3 = None
        view_11 = query_2.view((1, 199, 16, 64))
        query_2 = None
        query_3 = view_11.transpose(2, 1)
        view_11 = None
        view_12 = key_2.view((1, 199, 16, 64))
        key_2 = None
        key_3 = view_12.transpose(2, 1)
        view_12 = None
        view_13 = value_2.view((1, 199, 16, 64))
        value_2 = None
        value_3 = view_13.transpose(2, 1)
        view_13 = None
        attn_output_3 = torch._C._nn.scaled_dot_product_attention(
            query_3,
            key_3,
            value_3,
            attn_mask=attn_mask_rel_pos_3,
            dropout_p=0.0,
            is_causal=False,
        )
        query_3 = key_3 = value_3 = attn_mask_rel_pos_3 = None
        transpose_24 = attn_output_3.transpose(1, 2)
        attn_output_3 = None
        attn_output_4 = transpose_24.reshape(1, -1, 1024)
        transpose_24 = None
        attn_output_5 = torch._C._nn.linear(
            attn_output_4,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_4 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_58 = torch.nn.functional.dropout(attn_output_5, 0.1, False, False)
        attn_output_5 = None
        x_59 = x_56 + x_58
        x_56 = x_58 = None
        layer_norm_12 = torch.nn.functional.layer_norm(
            x_59,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_final_layer_norm_parameters_bias_ = (None)
        x_60 = torch._C._nn.linear(
            layer_norm_12,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_12 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_61 = torch._C._nn.gelu(x_60)
        x_60 = None
        x_62 = torch.nn.functional.dropout(x_61, 0.0, False, False)
        x_61 = None
        x_63 = torch._C._nn.linear(
            x_62,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_62 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_1_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_64 = torch.nn.functional.dropout(x_63, 0.1, False, False)
        x_63 = None
        x_65 = x_59 + x_64
        x_59 = x_64 = None
        x_66 = torch.nn.functional.layer_norm(
            x_65,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_layer_norm_parameters_bias_ = (None)
        query_layer_4 = x_66.view(1, 199, 16, -1)
        query_layer_5 = query_layer_4.permute(0, 2, 1, 3)
        query_layer_4 = None
        linear_11 = torch._C._nn.linear(
            query_layer_5,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_5 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_15 = linear_11.view(1, 16, 199, 2, 4)
        linear_11 = None
        sum_3 = view_15.sum(-1, keepdim=False)
        view_15 = None
        sigmoid_2 = torch.sigmoid(sum_3)
        sum_3 = None
        chunk_4 = sigmoid_2.chunk(2, dim=-1)
        sigmoid_2 = None
        gate_a_4 = chunk_4[0]
        gate_b_2 = chunk_4[1]
        chunk_4 = None
        mul_8 = (
            gate_b_2
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_2 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_3 = mul_8 - 1.0
        mul_8 = None
        mul_9 = gate_a_4 * sub_3
        gate_a_4 = sub_3 = None
        gate_a_5 = mul_9 + 2.0
        mul_9 = None
        view_16 = gate_a_5.view(1, 16, -1, 1)
        gate_a_5 = None
        attn_mask_rel_pos_4 = view_16 * position_bias
        view_16 = None
        attn_mask_rel_pos_5 = attn_mask_rel_pos_4.view((1, 16, 199, 199))
        attn_mask_rel_pos_4 = None
        query_projected_2 = torch._C._nn.linear(
            x_66,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_66 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_5 = query_projected_2.chunk(3, -1)
        query_projected_2 = None
        query_4 = chunk_5[0]
        key_4 = chunk_5[1]
        value_4 = chunk_5[2]
        chunk_5 = None
        view_18 = query_4.view((1, 199, 16, 64))
        query_4 = None
        query_5 = view_18.transpose(2, 1)
        view_18 = None
        view_19 = key_4.view((1, 199, 16, 64))
        key_4 = None
        key_5 = view_19.transpose(2, 1)
        view_19 = None
        view_20 = value_4.view((1, 199, 16, 64))
        value_4 = None
        value_5 = view_20.transpose(2, 1)
        view_20 = None
        attn_output_6 = torch._C._nn.scaled_dot_product_attention(
            query_5,
            key_5,
            value_5,
            attn_mask=attn_mask_rel_pos_5,
            dropout_p=0.0,
            is_causal=False,
        )
        query_5 = key_5 = value_5 = attn_mask_rel_pos_5 = None
        transpose_28 = attn_output_6.transpose(1, 2)
        attn_output_6 = None
        attn_output_7 = transpose_28.reshape(1, -1, 1024)
        transpose_28 = None
        attn_output_8 = torch._C._nn.linear(
            attn_output_7,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_7 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_67 = torch.nn.functional.dropout(attn_output_8, 0.1, False, False)
        attn_output_8 = None
        x_68 = x_65 + x_67
        x_65 = x_67 = None
        layer_norm_14 = torch.nn.functional.layer_norm(
            x_68,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_final_layer_norm_parameters_bias_ = (None)
        x_69 = torch._C._nn.linear(
            layer_norm_14,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_14 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_70 = torch._C._nn.gelu(x_69)
        x_69 = None
        x_71 = torch.nn.functional.dropout(x_70, 0.0, False, False)
        x_70 = None
        x_72 = torch._C._nn.linear(
            x_71,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_71 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_2_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_73 = torch.nn.functional.dropout(x_72, 0.1, False, False)
        x_72 = None
        x_74 = x_68 + x_73
        x_68 = x_73 = None
        x_75 = torch.nn.functional.layer_norm(
            x_74,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_layer_norm_parameters_bias_ = (None)
        query_layer_6 = x_75.view(1, 199, 16, -1)
        query_layer_7 = query_layer_6.permute(0, 2, 1, 3)
        query_layer_6 = None
        linear_16 = torch._C._nn.linear(
            query_layer_7,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_7 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_22 = linear_16.view(1, 16, 199, 2, 4)
        linear_16 = None
        sum_4 = view_22.sum(-1, keepdim=False)
        view_22 = None
        sigmoid_3 = torch.sigmoid(sum_4)
        sum_4 = None
        chunk_6 = sigmoid_3.chunk(2, dim=-1)
        sigmoid_3 = None
        gate_a_6 = chunk_6[0]
        gate_b_3 = chunk_6[1]
        chunk_6 = None
        mul_11 = (
            gate_b_3
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_3 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_4 = mul_11 - 1.0
        mul_11 = None
        mul_12 = gate_a_6 * sub_4
        gate_a_6 = sub_4 = None
        gate_a_7 = mul_12 + 2.0
        mul_12 = None
        view_23 = gate_a_7.view(1, 16, -1, 1)
        gate_a_7 = None
        attn_mask_rel_pos_6 = view_23 * position_bias
        view_23 = None
        attn_mask_rel_pos_7 = attn_mask_rel_pos_6.view((1, 16, 199, 199))
        attn_mask_rel_pos_6 = None
        query_projected_3 = torch._C._nn.linear(
            x_75,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_75 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_7 = query_projected_3.chunk(3, -1)
        query_projected_3 = None
        query_6 = chunk_7[0]
        key_6 = chunk_7[1]
        value_6 = chunk_7[2]
        chunk_7 = None
        view_25 = query_6.view((1, 199, 16, 64))
        query_6 = None
        query_7 = view_25.transpose(2, 1)
        view_25 = None
        view_26 = key_6.view((1, 199, 16, 64))
        key_6 = None
        key_7 = view_26.transpose(2, 1)
        view_26 = None
        view_27 = value_6.view((1, 199, 16, 64))
        value_6 = None
        value_7 = view_27.transpose(2, 1)
        view_27 = None
        attn_output_9 = torch._C._nn.scaled_dot_product_attention(
            query_7,
            key_7,
            value_7,
            attn_mask=attn_mask_rel_pos_7,
            dropout_p=0.0,
            is_causal=False,
        )
        query_7 = key_7 = value_7 = attn_mask_rel_pos_7 = None
        transpose_32 = attn_output_9.transpose(1, 2)
        attn_output_9 = None
        attn_output_10 = transpose_32.reshape(1, -1, 1024)
        transpose_32 = None
        attn_output_11 = torch._C._nn.linear(
            attn_output_10,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_10 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_76 = torch.nn.functional.dropout(attn_output_11, 0.1, False, False)
        attn_output_11 = None
        x_77 = x_74 + x_76
        x_74 = x_76 = None
        layer_norm_16 = torch.nn.functional.layer_norm(
            x_77,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_final_layer_norm_parameters_bias_ = (None)
        x_78 = torch._C._nn.linear(
            layer_norm_16,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_16 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_79 = torch._C._nn.gelu(x_78)
        x_78 = None
        x_80 = torch.nn.functional.dropout(x_79, 0.0, False, False)
        x_79 = None
        x_81 = torch._C._nn.linear(
            x_80,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_80 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_3_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_82 = torch.nn.functional.dropout(x_81, 0.1, False, False)
        x_81 = None
        x_83 = x_77 + x_82
        x_77 = x_82 = None
        x_84 = torch.nn.functional.layer_norm(
            x_83,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_layer_norm_parameters_bias_ = (None)
        query_layer_8 = x_84.view(1, 199, 16, -1)
        query_layer_9 = query_layer_8.permute(0, 2, 1, 3)
        query_layer_8 = None
        linear_21 = torch._C._nn.linear(
            query_layer_9,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_9 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_29 = linear_21.view(1, 16, 199, 2, 4)
        linear_21 = None
        sum_5 = view_29.sum(-1, keepdim=False)
        view_29 = None
        sigmoid_4 = torch.sigmoid(sum_5)
        sum_5 = None
        chunk_8 = sigmoid_4.chunk(2, dim=-1)
        sigmoid_4 = None
        gate_a_8 = chunk_8[0]
        gate_b_4 = chunk_8[1]
        chunk_8 = None
        mul_14 = (
            gate_b_4
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_4 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_5 = mul_14 - 1.0
        mul_14 = None
        mul_15 = gate_a_8 * sub_5
        gate_a_8 = sub_5 = None
        gate_a_9 = mul_15 + 2.0
        mul_15 = None
        view_30 = gate_a_9.view(1, 16, -1, 1)
        gate_a_9 = None
        attn_mask_rel_pos_8 = view_30 * position_bias
        view_30 = None
        attn_mask_rel_pos_9 = attn_mask_rel_pos_8.view((1, 16, 199, 199))
        attn_mask_rel_pos_8 = None
        query_projected_4 = torch._C._nn.linear(
            x_84,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_84 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_9 = query_projected_4.chunk(3, -1)
        query_projected_4 = None
        query_8 = chunk_9[0]
        key_8 = chunk_9[1]
        value_8 = chunk_9[2]
        chunk_9 = None
        view_32 = query_8.view((1, 199, 16, 64))
        query_8 = None
        query_9 = view_32.transpose(2, 1)
        view_32 = None
        view_33 = key_8.view((1, 199, 16, 64))
        key_8 = None
        key_9 = view_33.transpose(2, 1)
        view_33 = None
        view_34 = value_8.view((1, 199, 16, 64))
        value_8 = None
        value_9 = view_34.transpose(2, 1)
        view_34 = None
        attn_output_12 = torch._C._nn.scaled_dot_product_attention(
            query_9,
            key_9,
            value_9,
            attn_mask=attn_mask_rel_pos_9,
            dropout_p=0.0,
            is_causal=False,
        )
        query_9 = key_9 = value_9 = attn_mask_rel_pos_9 = None
        transpose_36 = attn_output_12.transpose(1, 2)
        attn_output_12 = None
        attn_output_13 = transpose_36.reshape(1, -1, 1024)
        transpose_36 = None
        attn_output_14 = torch._C._nn.linear(
            attn_output_13,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_13 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_85 = torch.nn.functional.dropout(attn_output_14, 0.1, False, False)
        attn_output_14 = None
        x_86 = x_83 + x_85
        x_83 = x_85 = None
        layer_norm_18 = torch.nn.functional.layer_norm(
            x_86,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_final_layer_norm_parameters_bias_ = (None)
        x_87 = torch._C._nn.linear(
            layer_norm_18,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_18 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_88 = torch._C._nn.gelu(x_87)
        x_87 = None
        x_89 = torch.nn.functional.dropout(x_88, 0.0, False, False)
        x_88 = None
        x_90 = torch._C._nn.linear(
            x_89,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_89 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_4_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_91 = torch.nn.functional.dropout(x_90, 0.1, False, False)
        x_90 = None
        x_92 = x_86 + x_91
        x_86 = x_91 = None
        x_93 = torch.nn.functional.layer_norm(
            x_92,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_layer_norm_parameters_bias_ = (None)
        query_layer_10 = x_93.view(1, 199, 16, -1)
        query_layer_11 = query_layer_10.permute(0, 2, 1, 3)
        query_layer_10 = None
        linear_26 = torch._C._nn.linear(
            query_layer_11,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_11 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_36 = linear_26.view(1, 16, 199, 2, 4)
        linear_26 = None
        sum_6 = view_36.sum(-1, keepdim=False)
        view_36 = None
        sigmoid_5 = torch.sigmoid(sum_6)
        sum_6 = None
        chunk_10 = sigmoid_5.chunk(2, dim=-1)
        sigmoid_5 = None
        gate_a_10 = chunk_10[0]
        gate_b_5 = chunk_10[1]
        chunk_10 = None
        mul_17 = (
            gate_b_5
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_5 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_6 = mul_17 - 1.0
        mul_17 = None
        mul_18 = gate_a_10 * sub_6
        gate_a_10 = sub_6 = None
        gate_a_11 = mul_18 + 2.0
        mul_18 = None
        view_37 = gate_a_11.view(1, 16, -1, 1)
        gate_a_11 = None
        attn_mask_rel_pos_10 = view_37 * position_bias
        view_37 = None
        attn_mask_rel_pos_11 = attn_mask_rel_pos_10.view((1, 16, 199, 199))
        attn_mask_rel_pos_10 = None
        query_projected_5 = torch._C._nn.linear(
            x_93,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_93 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_11 = query_projected_5.chunk(3, -1)
        query_projected_5 = None
        query_10 = chunk_11[0]
        key_10 = chunk_11[1]
        value_10 = chunk_11[2]
        chunk_11 = None
        view_39 = query_10.view((1, 199, 16, 64))
        query_10 = None
        query_11 = view_39.transpose(2, 1)
        view_39 = None
        view_40 = key_10.view((1, 199, 16, 64))
        key_10 = None
        key_11 = view_40.transpose(2, 1)
        view_40 = None
        view_41 = value_10.view((1, 199, 16, 64))
        value_10 = None
        value_11 = view_41.transpose(2, 1)
        view_41 = None
        attn_output_15 = torch._C._nn.scaled_dot_product_attention(
            query_11,
            key_11,
            value_11,
            attn_mask=attn_mask_rel_pos_11,
            dropout_p=0.0,
            is_causal=False,
        )
        query_11 = key_11 = value_11 = attn_mask_rel_pos_11 = None
        transpose_40 = attn_output_15.transpose(1, 2)
        attn_output_15 = None
        attn_output_16 = transpose_40.reshape(1, -1, 1024)
        transpose_40 = None
        attn_output_17 = torch._C._nn.linear(
            attn_output_16,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_16 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_94 = torch.nn.functional.dropout(attn_output_17, 0.1, False, False)
        attn_output_17 = None
        x_95 = x_92 + x_94
        x_92 = x_94 = None
        layer_norm_20 = torch.nn.functional.layer_norm(
            x_95,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_final_layer_norm_parameters_bias_ = (None)
        x_96 = torch._C._nn.linear(
            layer_norm_20,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_20 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_97 = torch._C._nn.gelu(x_96)
        x_96 = None
        x_98 = torch.nn.functional.dropout(x_97, 0.0, False, False)
        x_97 = None
        x_99 = torch._C._nn.linear(
            x_98,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_98 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_5_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_100 = torch.nn.functional.dropout(x_99, 0.1, False, False)
        x_99 = None
        x_101 = x_95 + x_100
        x_95 = x_100 = None
        x_102 = torch.nn.functional.layer_norm(
            x_101,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_layer_norm_parameters_bias_ = (None)
        query_layer_12 = x_102.view(1, 199, 16, -1)
        query_layer_13 = query_layer_12.permute(0, 2, 1, 3)
        query_layer_12 = None
        linear_31 = torch._C._nn.linear(
            query_layer_13,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_13 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_43 = linear_31.view(1, 16, 199, 2, 4)
        linear_31 = None
        sum_7 = view_43.sum(-1, keepdim=False)
        view_43 = None
        sigmoid_6 = torch.sigmoid(sum_7)
        sum_7 = None
        chunk_12 = sigmoid_6.chunk(2, dim=-1)
        sigmoid_6 = None
        gate_a_12 = chunk_12[0]
        gate_b_6 = chunk_12[1]
        chunk_12 = None
        mul_20 = (
            gate_b_6
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_6 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_7 = mul_20 - 1.0
        mul_20 = None
        mul_21 = gate_a_12 * sub_7
        gate_a_12 = sub_7 = None
        gate_a_13 = mul_21 + 2.0
        mul_21 = None
        view_44 = gate_a_13.view(1, 16, -1, 1)
        gate_a_13 = None
        attn_mask_rel_pos_12 = view_44 * position_bias
        view_44 = None
        attn_mask_rel_pos_13 = attn_mask_rel_pos_12.view((1, 16, 199, 199))
        attn_mask_rel_pos_12 = None
        query_projected_6 = torch._C._nn.linear(
            x_102,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_102 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_13 = query_projected_6.chunk(3, -1)
        query_projected_6 = None
        query_12 = chunk_13[0]
        key_12 = chunk_13[1]
        value_12 = chunk_13[2]
        chunk_13 = None
        view_46 = query_12.view((1, 199, 16, 64))
        query_12 = None
        query_13 = view_46.transpose(2, 1)
        view_46 = None
        view_47 = key_12.view((1, 199, 16, 64))
        key_12 = None
        key_13 = view_47.transpose(2, 1)
        view_47 = None
        view_48 = value_12.view((1, 199, 16, 64))
        value_12 = None
        value_13 = view_48.transpose(2, 1)
        view_48 = None
        attn_output_18 = torch._C._nn.scaled_dot_product_attention(
            query_13,
            key_13,
            value_13,
            attn_mask=attn_mask_rel_pos_13,
            dropout_p=0.0,
            is_causal=False,
        )
        query_13 = key_13 = value_13 = attn_mask_rel_pos_13 = None
        transpose_44 = attn_output_18.transpose(1, 2)
        attn_output_18 = None
        attn_output_19 = transpose_44.reshape(1, -1, 1024)
        transpose_44 = None
        attn_output_20 = torch._C._nn.linear(
            attn_output_19,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_19 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_103 = torch.nn.functional.dropout(attn_output_20, 0.1, False, False)
        attn_output_20 = None
        x_104 = x_101 + x_103
        x_101 = x_103 = None
        layer_norm_22 = torch.nn.functional.layer_norm(
            x_104,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_final_layer_norm_parameters_bias_ = (None)
        x_105 = torch._C._nn.linear(
            layer_norm_22,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_22 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_106 = torch._C._nn.gelu(x_105)
        x_105 = None
        x_107 = torch.nn.functional.dropout(x_106, 0.0, False, False)
        x_106 = None
        x_108 = torch._C._nn.linear(
            x_107,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_107 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_6_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_109 = torch.nn.functional.dropout(x_108, 0.1, False, False)
        x_108 = None
        x_110 = x_104 + x_109
        x_104 = x_109 = None
        x_111 = torch.nn.functional.layer_norm(
            x_110,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_layer_norm_parameters_bias_ = (None)
        query_layer_14 = x_111.view(1, 199, 16, -1)
        query_layer_15 = query_layer_14.permute(0, 2, 1, 3)
        query_layer_14 = None
        linear_36 = torch._C._nn.linear(
            query_layer_15,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_15 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_50 = linear_36.view(1, 16, 199, 2, 4)
        linear_36 = None
        sum_8 = view_50.sum(-1, keepdim=False)
        view_50 = None
        sigmoid_7 = torch.sigmoid(sum_8)
        sum_8 = None
        chunk_14 = sigmoid_7.chunk(2, dim=-1)
        sigmoid_7 = None
        gate_a_14 = chunk_14[0]
        gate_b_7 = chunk_14[1]
        chunk_14 = None
        mul_23 = (
            gate_b_7
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_7 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_8 = mul_23 - 1.0
        mul_23 = None
        mul_24 = gate_a_14 * sub_8
        gate_a_14 = sub_8 = None
        gate_a_15 = mul_24 + 2.0
        mul_24 = None
        view_51 = gate_a_15.view(1, 16, -1, 1)
        gate_a_15 = None
        attn_mask_rel_pos_14 = view_51 * position_bias
        view_51 = None
        attn_mask_rel_pos_15 = attn_mask_rel_pos_14.view((1, 16, 199, 199))
        attn_mask_rel_pos_14 = None
        query_projected_7 = torch._C._nn.linear(
            x_111,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_111 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_15 = query_projected_7.chunk(3, -1)
        query_projected_7 = None
        query_14 = chunk_15[0]
        key_14 = chunk_15[1]
        value_14 = chunk_15[2]
        chunk_15 = None
        view_53 = query_14.view((1, 199, 16, 64))
        query_14 = None
        query_15 = view_53.transpose(2, 1)
        view_53 = None
        view_54 = key_14.view((1, 199, 16, 64))
        key_14 = None
        key_15 = view_54.transpose(2, 1)
        view_54 = None
        view_55 = value_14.view((1, 199, 16, 64))
        value_14 = None
        value_15 = view_55.transpose(2, 1)
        view_55 = None
        attn_output_21 = torch._C._nn.scaled_dot_product_attention(
            query_15,
            key_15,
            value_15,
            attn_mask=attn_mask_rel_pos_15,
            dropout_p=0.0,
            is_causal=False,
        )
        query_15 = key_15 = value_15 = attn_mask_rel_pos_15 = None
        transpose_48 = attn_output_21.transpose(1, 2)
        attn_output_21 = None
        attn_output_22 = transpose_48.reshape(1, -1, 1024)
        transpose_48 = None
        attn_output_23 = torch._C._nn.linear(
            attn_output_22,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_22 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_112 = torch.nn.functional.dropout(attn_output_23, 0.1, False, False)
        attn_output_23 = None
        x_113 = x_110 + x_112
        x_110 = x_112 = None
        layer_norm_24 = torch.nn.functional.layer_norm(
            x_113,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_final_layer_norm_parameters_bias_ = (None)
        x_114 = torch._C._nn.linear(
            layer_norm_24,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_24 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_115 = torch._C._nn.gelu(x_114)
        x_114 = None
        x_116 = torch.nn.functional.dropout(x_115, 0.0, False, False)
        x_115 = None
        x_117 = torch._C._nn.linear(
            x_116,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_116 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_7_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_118 = torch.nn.functional.dropout(x_117, 0.1, False, False)
        x_117 = None
        x_119 = x_113 + x_118
        x_113 = x_118 = None
        x_120 = torch.nn.functional.layer_norm(
            x_119,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_layer_norm_parameters_bias_ = (None)
        query_layer_16 = x_120.view(1, 199, 16, -1)
        query_layer_17 = query_layer_16.permute(0, 2, 1, 3)
        query_layer_16 = None
        linear_41 = torch._C._nn.linear(
            query_layer_17,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_17 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_57 = linear_41.view(1, 16, 199, 2, 4)
        linear_41 = None
        sum_9 = view_57.sum(-1, keepdim=False)
        view_57 = None
        sigmoid_8 = torch.sigmoid(sum_9)
        sum_9 = None
        chunk_16 = sigmoid_8.chunk(2, dim=-1)
        sigmoid_8 = None
        gate_a_16 = chunk_16[0]
        gate_b_8 = chunk_16[1]
        chunk_16 = None
        mul_26 = (
            gate_b_8
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_8 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_9 = mul_26 - 1.0
        mul_26 = None
        mul_27 = gate_a_16 * sub_9
        gate_a_16 = sub_9 = None
        gate_a_17 = mul_27 + 2.0
        mul_27 = None
        view_58 = gate_a_17.view(1, 16, -1, 1)
        gate_a_17 = None
        attn_mask_rel_pos_16 = view_58 * position_bias
        view_58 = None
        attn_mask_rel_pos_17 = attn_mask_rel_pos_16.view((1, 16, 199, 199))
        attn_mask_rel_pos_16 = None
        query_projected_8 = torch._C._nn.linear(
            x_120,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_120 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_17 = query_projected_8.chunk(3, -1)
        query_projected_8 = None
        query_16 = chunk_17[0]
        key_16 = chunk_17[1]
        value_16 = chunk_17[2]
        chunk_17 = None
        view_60 = query_16.view((1, 199, 16, 64))
        query_16 = None
        query_17 = view_60.transpose(2, 1)
        view_60 = None
        view_61 = key_16.view((1, 199, 16, 64))
        key_16 = None
        key_17 = view_61.transpose(2, 1)
        view_61 = None
        view_62 = value_16.view((1, 199, 16, 64))
        value_16 = None
        value_17 = view_62.transpose(2, 1)
        view_62 = None
        attn_output_24 = torch._C._nn.scaled_dot_product_attention(
            query_17,
            key_17,
            value_17,
            attn_mask=attn_mask_rel_pos_17,
            dropout_p=0.0,
            is_causal=False,
        )
        query_17 = key_17 = value_17 = attn_mask_rel_pos_17 = None
        transpose_52 = attn_output_24.transpose(1, 2)
        attn_output_24 = None
        attn_output_25 = transpose_52.reshape(1, -1, 1024)
        transpose_52 = None
        attn_output_26 = torch._C._nn.linear(
            attn_output_25,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_25 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_121 = torch.nn.functional.dropout(attn_output_26, 0.1, False, False)
        attn_output_26 = None
        x_122 = x_119 + x_121
        x_119 = x_121 = None
        layer_norm_26 = torch.nn.functional.layer_norm(
            x_122,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_final_layer_norm_parameters_bias_ = (None)
        x_123 = torch._C._nn.linear(
            layer_norm_26,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_26 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_124 = torch._C._nn.gelu(x_123)
        x_123 = None
        x_125 = torch.nn.functional.dropout(x_124, 0.0, False, False)
        x_124 = None
        x_126 = torch._C._nn.linear(
            x_125,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_125 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_8_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_127 = torch.nn.functional.dropout(x_126, 0.1, False, False)
        x_126 = None
        x_128 = x_122 + x_127
        x_122 = x_127 = None
        x_129 = torch.nn.functional.layer_norm(
            x_128,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_layer_norm_parameters_bias_ = (None)
        query_layer_18 = x_129.view(1, 199, 16, -1)
        query_layer_19 = query_layer_18.permute(0, 2, 1, 3)
        query_layer_18 = None
        linear_46 = torch._C._nn.linear(
            query_layer_19,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_19 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_64 = linear_46.view(1, 16, 199, 2, 4)
        linear_46 = None
        sum_10 = view_64.sum(-1, keepdim=False)
        view_64 = None
        sigmoid_9 = torch.sigmoid(sum_10)
        sum_10 = None
        chunk_18 = sigmoid_9.chunk(2, dim=-1)
        sigmoid_9 = None
        gate_a_18 = chunk_18[0]
        gate_b_9 = chunk_18[1]
        chunk_18 = None
        mul_29 = (
            gate_b_9
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_9 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_10 = mul_29 - 1.0
        mul_29 = None
        mul_30 = gate_a_18 * sub_10
        gate_a_18 = sub_10 = None
        gate_a_19 = mul_30 + 2.0
        mul_30 = None
        view_65 = gate_a_19.view(1, 16, -1, 1)
        gate_a_19 = None
        attn_mask_rel_pos_18 = view_65 * position_bias
        view_65 = None
        attn_mask_rel_pos_19 = attn_mask_rel_pos_18.view((1, 16, 199, 199))
        attn_mask_rel_pos_18 = None
        query_projected_9 = torch._C._nn.linear(
            x_129,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_129 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_19 = query_projected_9.chunk(3, -1)
        query_projected_9 = None
        query_18 = chunk_19[0]
        key_18 = chunk_19[1]
        value_18 = chunk_19[2]
        chunk_19 = None
        view_67 = query_18.view((1, 199, 16, 64))
        query_18 = None
        query_19 = view_67.transpose(2, 1)
        view_67 = None
        view_68 = key_18.view((1, 199, 16, 64))
        key_18 = None
        key_19 = view_68.transpose(2, 1)
        view_68 = None
        view_69 = value_18.view((1, 199, 16, 64))
        value_18 = None
        value_19 = view_69.transpose(2, 1)
        view_69 = None
        attn_output_27 = torch._C._nn.scaled_dot_product_attention(
            query_19,
            key_19,
            value_19,
            attn_mask=attn_mask_rel_pos_19,
            dropout_p=0.0,
            is_causal=False,
        )
        query_19 = key_19 = value_19 = attn_mask_rel_pos_19 = None
        transpose_56 = attn_output_27.transpose(1, 2)
        attn_output_27 = None
        attn_output_28 = transpose_56.reshape(1, -1, 1024)
        transpose_56 = None
        attn_output_29 = torch._C._nn.linear(
            attn_output_28,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_28 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_130 = torch.nn.functional.dropout(attn_output_29, 0.1, False, False)
        attn_output_29 = None
        x_131 = x_128 + x_130
        x_128 = x_130 = None
        layer_norm_28 = torch.nn.functional.layer_norm(
            x_131,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_final_layer_norm_parameters_bias_ = (None)
        x_132 = torch._C._nn.linear(
            layer_norm_28,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_28 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_133 = torch._C._nn.gelu(x_132)
        x_132 = None
        x_134 = torch.nn.functional.dropout(x_133, 0.0, False, False)
        x_133 = None
        x_135 = torch._C._nn.linear(
            x_134,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_134 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_9_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_136 = torch.nn.functional.dropout(x_135, 0.1, False, False)
        x_135 = None
        x_137 = x_131 + x_136
        x_131 = x_136 = None
        x_138 = torch.nn.functional.layer_norm(
            x_137,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_layer_norm_parameters_bias_ = (None)
        query_layer_20 = x_138.view(1, 199, 16, -1)
        query_layer_21 = query_layer_20.permute(0, 2, 1, 3)
        query_layer_20 = None
        linear_51 = torch._C._nn.linear(
            query_layer_21,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_21 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_71 = linear_51.view(1, 16, 199, 2, 4)
        linear_51 = None
        sum_11 = view_71.sum(-1, keepdim=False)
        view_71 = None
        sigmoid_10 = torch.sigmoid(sum_11)
        sum_11 = None
        chunk_20 = sigmoid_10.chunk(2, dim=-1)
        sigmoid_10 = None
        gate_a_20 = chunk_20[0]
        gate_b_10 = chunk_20[1]
        chunk_20 = None
        mul_32 = (
            gate_b_10
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_10 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_11 = mul_32 - 1.0
        mul_32 = None
        mul_33 = gate_a_20 * sub_11
        gate_a_20 = sub_11 = None
        gate_a_21 = mul_33 + 2.0
        mul_33 = None
        view_72 = gate_a_21.view(1, 16, -1, 1)
        gate_a_21 = None
        attn_mask_rel_pos_20 = view_72 * position_bias
        view_72 = None
        attn_mask_rel_pos_21 = attn_mask_rel_pos_20.view((1, 16, 199, 199))
        attn_mask_rel_pos_20 = None
        query_projected_10 = torch._C._nn.linear(
            x_138,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_138 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_21 = query_projected_10.chunk(3, -1)
        query_projected_10 = None
        query_20 = chunk_21[0]
        key_20 = chunk_21[1]
        value_20 = chunk_21[2]
        chunk_21 = None
        view_74 = query_20.view((1, 199, 16, 64))
        query_20 = None
        query_21 = view_74.transpose(2, 1)
        view_74 = None
        view_75 = key_20.view((1, 199, 16, 64))
        key_20 = None
        key_21 = view_75.transpose(2, 1)
        view_75 = None
        view_76 = value_20.view((1, 199, 16, 64))
        value_20 = None
        value_21 = view_76.transpose(2, 1)
        view_76 = None
        attn_output_30 = torch._C._nn.scaled_dot_product_attention(
            query_21,
            key_21,
            value_21,
            attn_mask=attn_mask_rel_pos_21,
            dropout_p=0.0,
            is_causal=False,
        )
        query_21 = key_21 = value_21 = attn_mask_rel_pos_21 = None
        transpose_60 = attn_output_30.transpose(1, 2)
        attn_output_30 = None
        attn_output_31 = transpose_60.reshape(1, -1, 1024)
        transpose_60 = None
        attn_output_32 = torch._C._nn.linear(
            attn_output_31,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_31 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_139 = torch.nn.functional.dropout(attn_output_32, 0.1, False, False)
        attn_output_32 = None
        x_140 = x_137 + x_139
        x_137 = x_139 = None
        layer_norm_30 = torch.nn.functional.layer_norm(
            x_140,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_final_layer_norm_parameters_bias_ = (None)
        x_141 = torch._C._nn.linear(
            layer_norm_30,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_30 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_142 = torch._C._nn.gelu(x_141)
        x_141 = None
        x_143 = torch.nn.functional.dropout(x_142, 0.0, False, False)
        x_142 = None
        x_144 = torch._C._nn.linear(
            x_143,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_143 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_10_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_145 = torch.nn.functional.dropout(x_144, 0.1, False, False)
        x_144 = None
        x_146 = x_140 + x_145
        x_140 = x_145 = None
        x_147 = torch.nn.functional.layer_norm(
            x_146,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_layer_norm_parameters_bias_ = (None)
        query_layer_22 = x_147.view(1, 199, 16, -1)
        query_layer_23 = query_layer_22.permute(0, 2, 1, 3)
        query_layer_22 = None
        linear_56 = torch._C._nn.linear(
            query_layer_23,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_23 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_78 = linear_56.view(1, 16, 199, 2, 4)
        linear_56 = None
        sum_12 = view_78.sum(-1, keepdim=False)
        view_78 = None
        sigmoid_11 = torch.sigmoid(sum_12)
        sum_12 = None
        chunk_22 = sigmoid_11.chunk(2, dim=-1)
        sigmoid_11 = None
        gate_a_22 = chunk_22[0]
        gate_b_11 = chunk_22[1]
        chunk_22 = None
        mul_35 = (
            gate_b_11
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_11 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_12 = mul_35 - 1.0
        mul_35 = None
        mul_36 = gate_a_22 * sub_12
        gate_a_22 = sub_12 = None
        gate_a_23 = mul_36 + 2.0
        mul_36 = None
        view_79 = gate_a_23.view(1, 16, -1, 1)
        gate_a_23 = None
        attn_mask_rel_pos_22 = view_79 * position_bias
        view_79 = None
        attn_mask_rel_pos_23 = attn_mask_rel_pos_22.view((1, 16, 199, 199))
        attn_mask_rel_pos_22 = None
        query_projected_11 = torch._C._nn.linear(
            x_147,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_147 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_23 = query_projected_11.chunk(3, -1)
        query_projected_11 = None
        query_22 = chunk_23[0]
        key_22 = chunk_23[1]
        value_22 = chunk_23[2]
        chunk_23 = None
        view_81 = query_22.view((1, 199, 16, 64))
        query_22 = None
        query_23 = view_81.transpose(2, 1)
        view_81 = None
        view_82 = key_22.view((1, 199, 16, 64))
        key_22 = None
        key_23 = view_82.transpose(2, 1)
        view_82 = None
        view_83 = value_22.view((1, 199, 16, 64))
        value_22 = None
        value_23 = view_83.transpose(2, 1)
        view_83 = None
        attn_output_33 = torch._C._nn.scaled_dot_product_attention(
            query_23,
            key_23,
            value_23,
            attn_mask=attn_mask_rel_pos_23,
            dropout_p=0.0,
            is_causal=False,
        )
        query_23 = key_23 = value_23 = attn_mask_rel_pos_23 = None
        transpose_64 = attn_output_33.transpose(1, 2)
        attn_output_33 = None
        attn_output_34 = transpose_64.reshape(1, -1, 1024)
        transpose_64 = None
        attn_output_35 = torch._C._nn.linear(
            attn_output_34,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_34 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_148 = torch.nn.functional.dropout(attn_output_35, 0.1, False, False)
        attn_output_35 = None
        x_149 = x_146 + x_148
        x_146 = x_148 = None
        layer_norm_32 = torch.nn.functional.layer_norm(
            x_149,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_final_layer_norm_parameters_bias_ = (None)
        x_150 = torch._C._nn.linear(
            layer_norm_32,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_32 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_151 = torch._C._nn.gelu(x_150)
        x_150 = None
        x_152 = torch.nn.functional.dropout(x_151, 0.0, False, False)
        x_151 = None
        x_153 = torch._C._nn.linear(
            x_152,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_152 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_11_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_154 = torch.nn.functional.dropout(x_153, 0.1, False, False)
        x_153 = None
        x_155 = x_149 + x_154
        x_149 = x_154 = None
        x_156 = torch.nn.functional.layer_norm(
            x_155,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_layer_norm_parameters_bias_ = (None)
        query_layer_24 = x_156.view(1, 199, 16, -1)
        query_layer_25 = query_layer_24.permute(0, 2, 1, 3)
        query_layer_24 = None
        linear_61 = torch._C._nn.linear(
            query_layer_25,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_25 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_85 = linear_61.view(1, 16, 199, 2, 4)
        linear_61 = None
        sum_13 = view_85.sum(-1, keepdim=False)
        view_85 = None
        sigmoid_12 = torch.sigmoid(sum_13)
        sum_13 = None
        chunk_24 = sigmoid_12.chunk(2, dim=-1)
        sigmoid_12 = None
        gate_a_24 = chunk_24[0]
        gate_b_12 = chunk_24[1]
        chunk_24 = None
        mul_38 = (
            gate_b_12
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_12 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_13 = mul_38 - 1.0
        mul_38 = None
        mul_39 = gate_a_24 * sub_13
        gate_a_24 = sub_13 = None
        gate_a_25 = mul_39 + 2.0
        mul_39 = None
        view_86 = gate_a_25.view(1, 16, -1, 1)
        gate_a_25 = None
        attn_mask_rel_pos_24 = view_86 * position_bias
        view_86 = None
        attn_mask_rel_pos_25 = attn_mask_rel_pos_24.view((1, 16, 199, 199))
        attn_mask_rel_pos_24 = None
        query_projected_12 = torch._C._nn.linear(
            x_156,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_156 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_25 = query_projected_12.chunk(3, -1)
        query_projected_12 = None
        query_24 = chunk_25[0]
        key_24 = chunk_25[1]
        value_24 = chunk_25[2]
        chunk_25 = None
        view_88 = query_24.view((1, 199, 16, 64))
        query_24 = None
        query_25 = view_88.transpose(2, 1)
        view_88 = None
        view_89 = key_24.view((1, 199, 16, 64))
        key_24 = None
        key_25 = view_89.transpose(2, 1)
        view_89 = None
        view_90 = value_24.view((1, 199, 16, 64))
        value_24 = None
        value_25 = view_90.transpose(2, 1)
        view_90 = None
        attn_output_36 = torch._C._nn.scaled_dot_product_attention(
            query_25,
            key_25,
            value_25,
            attn_mask=attn_mask_rel_pos_25,
            dropout_p=0.0,
            is_causal=False,
        )
        query_25 = key_25 = value_25 = attn_mask_rel_pos_25 = None
        transpose_68 = attn_output_36.transpose(1, 2)
        attn_output_36 = None
        attn_output_37 = transpose_68.reshape(1, -1, 1024)
        transpose_68 = None
        attn_output_38 = torch._C._nn.linear(
            attn_output_37,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_37 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_157 = torch.nn.functional.dropout(attn_output_38, 0.1, False, False)
        attn_output_38 = None
        x_158 = x_155 + x_157
        x_155 = x_157 = None
        layer_norm_34 = torch.nn.functional.layer_norm(
            x_158,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_final_layer_norm_parameters_bias_ = (None)
        x_159 = torch._C._nn.linear(
            layer_norm_34,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_34 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_160 = torch._C._nn.gelu(x_159)
        x_159 = None
        x_161 = torch.nn.functional.dropout(x_160, 0.0, False, False)
        x_160 = None
        x_162 = torch._C._nn.linear(
            x_161,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_161 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_12_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_163 = torch.nn.functional.dropout(x_162, 0.1, False, False)
        x_162 = None
        x_164 = x_158 + x_163
        x_158 = x_163 = None
        x_165 = torch.nn.functional.layer_norm(
            x_164,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_layer_norm_parameters_bias_ = (None)
        query_layer_26 = x_165.view(1, 199, 16, -1)
        query_layer_27 = query_layer_26.permute(0, 2, 1, 3)
        query_layer_26 = None
        linear_66 = torch._C._nn.linear(
            query_layer_27,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_27 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_92 = linear_66.view(1, 16, 199, 2, 4)
        linear_66 = None
        sum_14 = view_92.sum(-1, keepdim=False)
        view_92 = None
        sigmoid_13 = torch.sigmoid(sum_14)
        sum_14 = None
        chunk_26 = sigmoid_13.chunk(2, dim=-1)
        sigmoid_13 = None
        gate_a_26 = chunk_26[0]
        gate_b_13 = chunk_26[1]
        chunk_26 = None
        mul_41 = (
            gate_b_13
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_13 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_14 = mul_41 - 1.0
        mul_41 = None
        mul_42 = gate_a_26 * sub_14
        gate_a_26 = sub_14 = None
        gate_a_27 = mul_42 + 2.0
        mul_42 = None
        view_93 = gate_a_27.view(1, 16, -1, 1)
        gate_a_27 = None
        attn_mask_rel_pos_26 = view_93 * position_bias
        view_93 = None
        attn_mask_rel_pos_27 = attn_mask_rel_pos_26.view((1, 16, 199, 199))
        attn_mask_rel_pos_26 = None
        query_projected_13 = torch._C._nn.linear(
            x_165,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_165 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_27 = query_projected_13.chunk(3, -1)
        query_projected_13 = None
        query_26 = chunk_27[0]
        key_26 = chunk_27[1]
        value_26 = chunk_27[2]
        chunk_27 = None
        view_95 = query_26.view((1, 199, 16, 64))
        query_26 = None
        query_27 = view_95.transpose(2, 1)
        view_95 = None
        view_96 = key_26.view((1, 199, 16, 64))
        key_26 = None
        key_27 = view_96.transpose(2, 1)
        view_96 = None
        view_97 = value_26.view((1, 199, 16, 64))
        value_26 = None
        value_27 = view_97.transpose(2, 1)
        view_97 = None
        attn_output_39 = torch._C._nn.scaled_dot_product_attention(
            query_27,
            key_27,
            value_27,
            attn_mask=attn_mask_rel_pos_27,
            dropout_p=0.0,
            is_causal=False,
        )
        query_27 = key_27 = value_27 = attn_mask_rel_pos_27 = None
        transpose_72 = attn_output_39.transpose(1, 2)
        attn_output_39 = None
        attn_output_40 = transpose_72.reshape(1, -1, 1024)
        transpose_72 = None
        attn_output_41 = torch._C._nn.linear(
            attn_output_40,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_40 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_166 = torch.nn.functional.dropout(attn_output_41, 0.1, False, False)
        attn_output_41 = None
        x_167 = x_164 + x_166
        x_164 = x_166 = None
        layer_norm_36 = torch.nn.functional.layer_norm(
            x_167,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_final_layer_norm_parameters_bias_ = (None)
        x_168 = torch._C._nn.linear(
            layer_norm_36,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_36 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_169 = torch._C._nn.gelu(x_168)
        x_168 = None
        x_170 = torch.nn.functional.dropout(x_169, 0.0, False, False)
        x_169 = None
        x_171 = torch._C._nn.linear(
            x_170,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_170 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_13_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_172 = torch.nn.functional.dropout(x_171, 0.1, False, False)
        x_171 = None
        x_173 = x_167 + x_172
        x_167 = x_172 = None
        x_174 = torch.nn.functional.layer_norm(
            x_173,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_layer_norm_parameters_bias_ = (None)
        query_layer_28 = x_174.view(1, 199, 16, -1)
        query_layer_29 = query_layer_28.permute(0, 2, 1, 3)
        query_layer_28 = None
        linear_71 = torch._C._nn.linear(
            query_layer_29,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_29 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_99 = linear_71.view(1, 16, 199, 2, 4)
        linear_71 = None
        sum_15 = view_99.sum(-1, keepdim=False)
        view_99 = None
        sigmoid_14 = torch.sigmoid(sum_15)
        sum_15 = None
        chunk_28 = sigmoid_14.chunk(2, dim=-1)
        sigmoid_14 = None
        gate_a_28 = chunk_28[0]
        gate_b_14 = chunk_28[1]
        chunk_28 = None
        mul_44 = (
            gate_b_14
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_14 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_15 = mul_44 - 1.0
        mul_44 = None
        mul_45 = gate_a_28 * sub_15
        gate_a_28 = sub_15 = None
        gate_a_29 = mul_45 + 2.0
        mul_45 = None
        view_100 = gate_a_29.view(1, 16, -1, 1)
        gate_a_29 = None
        attn_mask_rel_pos_28 = view_100 * position_bias
        view_100 = None
        attn_mask_rel_pos_29 = attn_mask_rel_pos_28.view((1, 16, 199, 199))
        attn_mask_rel_pos_28 = None
        query_projected_14 = torch._C._nn.linear(
            x_174,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_174 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_29 = query_projected_14.chunk(3, -1)
        query_projected_14 = None
        query_28 = chunk_29[0]
        key_28 = chunk_29[1]
        value_28 = chunk_29[2]
        chunk_29 = None
        view_102 = query_28.view((1, 199, 16, 64))
        query_28 = None
        query_29 = view_102.transpose(2, 1)
        view_102 = None
        view_103 = key_28.view((1, 199, 16, 64))
        key_28 = None
        key_29 = view_103.transpose(2, 1)
        view_103 = None
        view_104 = value_28.view((1, 199, 16, 64))
        value_28 = None
        value_29 = view_104.transpose(2, 1)
        view_104 = None
        attn_output_42 = torch._C._nn.scaled_dot_product_attention(
            query_29,
            key_29,
            value_29,
            attn_mask=attn_mask_rel_pos_29,
            dropout_p=0.0,
            is_causal=False,
        )
        query_29 = key_29 = value_29 = attn_mask_rel_pos_29 = None
        transpose_76 = attn_output_42.transpose(1, 2)
        attn_output_42 = None
        attn_output_43 = transpose_76.reshape(1, -1, 1024)
        transpose_76 = None
        attn_output_44 = torch._C._nn.linear(
            attn_output_43,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_43 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_175 = torch.nn.functional.dropout(attn_output_44, 0.1, False, False)
        attn_output_44 = None
        x_176 = x_173 + x_175
        x_173 = x_175 = None
        layer_norm_38 = torch.nn.functional.layer_norm(
            x_176,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_final_layer_norm_parameters_bias_ = (None)
        x_177 = torch._C._nn.linear(
            layer_norm_38,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_38 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_178 = torch._C._nn.gelu(x_177)
        x_177 = None
        x_179 = torch.nn.functional.dropout(x_178, 0.0, False, False)
        x_178 = None
        x_180 = torch._C._nn.linear(
            x_179,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_179 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_14_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_181 = torch.nn.functional.dropout(x_180, 0.1, False, False)
        x_180 = None
        x_182 = x_176 + x_181
        x_176 = x_181 = None
        x_183 = torch.nn.functional.layer_norm(
            x_182,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_layer_norm_parameters_bias_ = (None)
        query_layer_30 = x_183.view(1, 199, 16, -1)
        query_layer_31 = query_layer_30.permute(0, 2, 1, 3)
        query_layer_30 = None
        linear_76 = torch._C._nn.linear(
            query_layer_31,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_31 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_106 = linear_76.view(1, 16, 199, 2, 4)
        linear_76 = None
        sum_16 = view_106.sum(-1, keepdim=False)
        view_106 = None
        sigmoid_15 = torch.sigmoid(sum_16)
        sum_16 = None
        chunk_30 = sigmoid_15.chunk(2, dim=-1)
        sigmoid_15 = None
        gate_a_30 = chunk_30[0]
        gate_b_15 = chunk_30[1]
        chunk_30 = None
        mul_47 = (
            gate_b_15
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_15 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_16 = mul_47 - 1.0
        mul_47 = None
        mul_48 = gate_a_30 * sub_16
        gate_a_30 = sub_16 = None
        gate_a_31 = mul_48 + 2.0
        mul_48 = None
        view_107 = gate_a_31.view(1, 16, -1, 1)
        gate_a_31 = None
        attn_mask_rel_pos_30 = view_107 * position_bias
        view_107 = None
        attn_mask_rel_pos_31 = attn_mask_rel_pos_30.view((1, 16, 199, 199))
        attn_mask_rel_pos_30 = None
        query_projected_15 = torch._C._nn.linear(
            x_183,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_183 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_31 = query_projected_15.chunk(3, -1)
        query_projected_15 = None
        query_30 = chunk_31[0]
        key_30 = chunk_31[1]
        value_30 = chunk_31[2]
        chunk_31 = None
        view_109 = query_30.view((1, 199, 16, 64))
        query_30 = None
        query_31 = view_109.transpose(2, 1)
        view_109 = None
        view_110 = key_30.view((1, 199, 16, 64))
        key_30 = None
        key_31 = view_110.transpose(2, 1)
        view_110 = None
        view_111 = value_30.view((1, 199, 16, 64))
        value_30 = None
        value_31 = view_111.transpose(2, 1)
        view_111 = None
        attn_output_45 = torch._C._nn.scaled_dot_product_attention(
            query_31,
            key_31,
            value_31,
            attn_mask=attn_mask_rel_pos_31,
            dropout_p=0.0,
            is_causal=False,
        )
        query_31 = key_31 = value_31 = attn_mask_rel_pos_31 = None
        transpose_80 = attn_output_45.transpose(1, 2)
        attn_output_45 = None
        attn_output_46 = transpose_80.reshape(1, -1, 1024)
        transpose_80 = None
        attn_output_47 = torch._C._nn.linear(
            attn_output_46,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_46 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_184 = torch.nn.functional.dropout(attn_output_47, 0.1, False, False)
        attn_output_47 = None
        x_185 = x_182 + x_184
        x_182 = x_184 = None
        layer_norm_40 = torch.nn.functional.layer_norm(
            x_185,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_final_layer_norm_parameters_bias_ = (None)
        x_186 = torch._C._nn.linear(
            layer_norm_40,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_40 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_187 = torch._C._nn.gelu(x_186)
        x_186 = None
        x_188 = torch.nn.functional.dropout(x_187, 0.0, False, False)
        x_187 = None
        x_189 = torch._C._nn.linear(
            x_188,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_188 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_15_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_190 = torch.nn.functional.dropout(x_189, 0.1, False, False)
        x_189 = None
        x_191 = x_185 + x_190
        x_185 = x_190 = None
        x_192 = torch.nn.functional.layer_norm(
            x_191,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_layer_norm_parameters_bias_ = (None)
        query_layer_32 = x_192.view(1, 199, 16, -1)
        query_layer_33 = query_layer_32.permute(0, 2, 1, 3)
        query_layer_32 = None
        linear_81 = torch._C._nn.linear(
            query_layer_33,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_33 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_113 = linear_81.view(1, 16, 199, 2, 4)
        linear_81 = None
        sum_17 = view_113.sum(-1, keepdim=False)
        view_113 = None
        sigmoid_16 = torch.sigmoid(sum_17)
        sum_17 = None
        chunk_32 = sigmoid_16.chunk(2, dim=-1)
        sigmoid_16 = None
        gate_a_32 = chunk_32[0]
        gate_b_16 = chunk_32[1]
        chunk_32 = None
        mul_50 = (
            gate_b_16
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_16 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_17 = mul_50 - 1.0
        mul_50 = None
        mul_51 = gate_a_32 * sub_17
        gate_a_32 = sub_17 = None
        gate_a_33 = mul_51 + 2.0
        mul_51 = None
        view_114 = gate_a_33.view(1, 16, -1, 1)
        gate_a_33 = None
        attn_mask_rel_pos_32 = view_114 * position_bias
        view_114 = None
        attn_mask_rel_pos_33 = attn_mask_rel_pos_32.view((1, 16, 199, 199))
        attn_mask_rel_pos_32 = None
        query_projected_16 = torch._C._nn.linear(
            x_192,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_192 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_33 = query_projected_16.chunk(3, -1)
        query_projected_16 = None
        query_32 = chunk_33[0]
        key_32 = chunk_33[1]
        value_32 = chunk_33[2]
        chunk_33 = None
        view_116 = query_32.view((1, 199, 16, 64))
        query_32 = None
        query_33 = view_116.transpose(2, 1)
        view_116 = None
        view_117 = key_32.view((1, 199, 16, 64))
        key_32 = None
        key_33 = view_117.transpose(2, 1)
        view_117 = None
        view_118 = value_32.view((1, 199, 16, 64))
        value_32 = None
        value_33 = view_118.transpose(2, 1)
        view_118 = None
        attn_output_48 = torch._C._nn.scaled_dot_product_attention(
            query_33,
            key_33,
            value_33,
            attn_mask=attn_mask_rel_pos_33,
            dropout_p=0.0,
            is_causal=False,
        )
        query_33 = key_33 = value_33 = attn_mask_rel_pos_33 = None
        transpose_84 = attn_output_48.transpose(1, 2)
        attn_output_48 = None
        attn_output_49 = transpose_84.reshape(1, -1, 1024)
        transpose_84 = None
        attn_output_50 = torch._C._nn.linear(
            attn_output_49,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_49 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_193 = torch.nn.functional.dropout(attn_output_50, 0.1, False, False)
        attn_output_50 = None
        x_194 = x_191 + x_193
        x_191 = x_193 = None
        layer_norm_42 = torch.nn.functional.layer_norm(
            x_194,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_final_layer_norm_parameters_bias_ = (None)
        x_195 = torch._C._nn.linear(
            layer_norm_42,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_42 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_196 = torch._C._nn.gelu(x_195)
        x_195 = None
        x_197 = torch.nn.functional.dropout(x_196, 0.0, False, False)
        x_196 = None
        x_198 = torch._C._nn.linear(
            x_197,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_197 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_16_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_199 = torch.nn.functional.dropout(x_198, 0.1, False, False)
        x_198 = None
        x_200 = x_194 + x_199
        x_194 = x_199 = None
        x_201 = torch.nn.functional.layer_norm(
            x_200,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_layer_norm_parameters_bias_ = (None)
        query_layer_34 = x_201.view(1, 199, 16, -1)
        query_layer_35 = query_layer_34.permute(0, 2, 1, 3)
        query_layer_34 = None
        linear_86 = torch._C._nn.linear(
            query_layer_35,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_35 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_120 = linear_86.view(1, 16, 199, 2, 4)
        linear_86 = None
        sum_18 = view_120.sum(-1, keepdim=False)
        view_120 = None
        sigmoid_17 = torch.sigmoid(sum_18)
        sum_18 = None
        chunk_34 = sigmoid_17.chunk(2, dim=-1)
        sigmoid_17 = None
        gate_a_34 = chunk_34[0]
        gate_b_17 = chunk_34[1]
        chunk_34 = None
        mul_53 = (
            gate_b_17
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_17 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_18 = mul_53 - 1.0
        mul_53 = None
        mul_54 = gate_a_34 * sub_18
        gate_a_34 = sub_18 = None
        gate_a_35 = mul_54 + 2.0
        mul_54 = None
        view_121 = gate_a_35.view(1, 16, -1, 1)
        gate_a_35 = None
        attn_mask_rel_pos_34 = view_121 * position_bias
        view_121 = None
        attn_mask_rel_pos_35 = attn_mask_rel_pos_34.view((1, 16, 199, 199))
        attn_mask_rel_pos_34 = None
        query_projected_17 = torch._C._nn.linear(
            x_201,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_201 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_35 = query_projected_17.chunk(3, -1)
        query_projected_17 = None
        query_34 = chunk_35[0]
        key_34 = chunk_35[1]
        value_34 = chunk_35[2]
        chunk_35 = None
        view_123 = query_34.view((1, 199, 16, 64))
        query_34 = None
        query_35 = view_123.transpose(2, 1)
        view_123 = None
        view_124 = key_34.view((1, 199, 16, 64))
        key_34 = None
        key_35 = view_124.transpose(2, 1)
        view_124 = None
        view_125 = value_34.view((1, 199, 16, 64))
        value_34 = None
        value_35 = view_125.transpose(2, 1)
        view_125 = None
        attn_output_51 = torch._C._nn.scaled_dot_product_attention(
            query_35,
            key_35,
            value_35,
            attn_mask=attn_mask_rel_pos_35,
            dropout_p=0.0,
            is_causal=False,
        )
        query_35 = key_35 = value_35 = attn_mask_rel_pos_35 = None
        transpose_88 = attn_output_51.transpose(1, 2)
        attn_output_51 = None
        attn_output_52 = transpose_88.reshape(1, -1, 1024)
        transpose_88 = None
        attn_output_53 = torch._C._nn.linear(
            attn_output_52,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_52 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_202 = torch.nn.functional.dropout(attn_output_53, 0.1, False, False)
        attn_output_53 = None
        x_203 = x_200 + x_202
        x_200 = x_202 = None
        layer_norm_44 = torch.nn.functional.layer_norm(
            x_203,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_final_layer_norm_parameters_bias_ = (None)
        x_204 = torch._C._nn.linear(
            layer_norm_44,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_44 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_205 = torch._C._nn.gelu(x_204)
        x_204 = None
        x_206 = torch.nn.functional.dropout(x_205, 0.0, False, False)
        x_205 = None
        x_207 = torch._C._nn.linear(
            x_206,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_206 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_17_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_208 = torch.nn.functional.dropout(x_207, 0.1, False, False)
        x_207 = None
        x_209 = x_203 + x_208
        x_203 = x_208 = None
        x_210 = torch.nn.functional.layer_norm(
            x_209,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_layer_norm_parameters_bias_ = (None)
        query_layer_36 = x_210.view(1, 199, 16, -1)
        query_layer_37 = query_layer_36.permute(0, 2, 1, 3)
        query_layer_36 = None
        linear_91 = torch._C._nn.linear(
            query_layer_37,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_37 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_127 = linear_91.view(1, 16, 199, 2, 4)
        linear_91 = None
        sum_19 = view_127.sum(-1, keepdim=False)
        view_127 = None
        sigmoid_18 = torch.sigmoid(sum_19)
        sum_19 = None
        chunk_36 = sigmoid_18.chunk(2, dim=-1)
        sigmoid_18 = None
        gate_a_36 = chunk_36[0]
        gate_b_18 = chunk_36[1]
        chunk_36 = None
        mul_56 = (
            gate_b_18
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_18 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_19 = mul_56 - 1.0
        mul_56 = None
        mul_57 = gate_a_36 * sub_19
        gate_a_36 = sub_19 = None
        gate_a_37 = mul_57 + 2.0
        mul_57 = None
        view_128 = gate_a_37.view(1, 16, -1, 1)
        gate_a_37 = None
        attn_mask_rel_pos_36 = view_128 * position_bias
        view_128 = None
        attn_mask_rel_pos_37 = attn_mask_rel_pos_36.view((1, 16, 199, 199))
        attn_mask_rel_pos_36 = None
        query_projected_18 = torch._C._nn.linear(
            x_210,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_210 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_37 = query_projected_18.chunk(3, -1)
        query_projected_18 = None
        query_36 = chunk_37[0]
        key_36 = chunk_37[1]
        value_36 = chunk_37[2]
        chunk_37 = None
        view_130 = query_36.view((1, 199, 16, 64))
        query_36 = None
        query_37 = view_130.transpose(2, 1)
        view_130 = None
        view_131 = key_36.view((1, 199, 16, 64))
        key_36 = None
        key_37 = view_131.transpose(2, 1)
        view_131 = None
        view_132 = value_36.view((1, 199, 16, 64))
        value_36 = None
        value_37 = view_132.transpose(2, 1)
        view_132 = None
        attn_output_54 = torch._C._nn.scaled_dot_product_attention(
            query_37,
            key_37,
            value_37,
            attn_mask=attn_mask_rel_pos_37,
            dropout_p=0.0,
            is_causal=False,
        )
        query_37 = key_37 = value_37 = attn_mask_rel_pos_37 = None
        transpose_92 = attn_output_54.transpose(1, 2)
        attn_output_54 = None
        attn_output_55 = transpose_92.reshape(1, -1, 1024)
        transpose_92 = None
        attn_output_56 = torch._C._nn.linear(
            attn_output_55,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_55 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_211 = torch.nn.functional.dropout(attn_output_56, 0.1, False, False)
        attn_output_56 = None
        x_212 = x_209 + x_211
        x_209 = x_211 = None
        layer_norm_46 = torch.nn.functional.layer_norm(
            x_212,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_final_layer_norm_parameters_bias_ = (None)
        x_213 = torch._C._nn.linear(
            layer_norm_46,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_46 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_214 = torch._C._nn.gelu(x_213)
        x_213 = None
        x_215 = torch.nn.functional.dropout(x_214, 0.0, False, False)
        x_214 = None
        x_216 = torch._C._nn.linear(
            x_215,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_215 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_18_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_217 = torch.nn.functional.dropout(x_216, 0.1, False, False)
        x_216 = None
        x_218 = x_212 + x_217
        x_212 = x_217 = None
        x_219 = torch.nn.functional.layer_norm(
            x_218,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_layer_norm_parameters_bias_ = (None)
        query_layer_38 = x_219.view(1, 199, 16, -1)
        query_layer_39 = query_layer_38.permute(0, 2, 1, 3)
        query_layer_38 = None
        linear_96 = torch._C._nn.linear(
            query_layer_39,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_39 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_134 = linear_96.view(1, 16, 199, 2, 4)
        linear_96 = None
        sum_20 = view_134.sum(-1, keepdim=False)
        view_134 = None
        sigmoid_19 = torch.sigmoid(sum_20)
        sum_20 = None
        chunk_38 = sigmoid_19.chunk(2, dim=-1)
        sigmoid_19 = None
        gate_a_38 = chunk_38[0]
        gate_b_19 = chunk_38[1]
        chunk_38 = None
        mul_59 = (
            gate_b_19
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_19 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_20 = mul_59 - 1.0
        mul_59 = None
        mul_60 = gate_a_38 * sub_20
        gate_a_38 = sub_20 = None
        gate_a_39 = mul_60 + 2.0
        mul_60 = None
        view_135 = gate_a_39.view(1, 16, -1, 1)
        gate_a_39 = None
        attn_mask_rel_pos_38 = view_135 * position_bias
        view_135 = None
        attn_mask_rel_pos_39 = attn_mask_rel_pos_38.view((1, 16, 199, 199))
        attn_mask_rel_pos_38 = None
        query_projected_19 = torch._C._nn.linear(
            x_219,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_219 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_39 = query_projected_19.chunk(3, -1)
        query_projected_19 = None
        query_38 = chunk_39[0]
        key_38 = chunk_39[1]
        value_38 = chunk_39[2]
        chunk_39 = None
        view_137 = query_38.view((1, 199, 16, 64))
        query_38 = None
        query_39 = view_137.transpose(2, 1)
        view_137 = None
        view_138 = key_38.view((1, 199, 16, 64))
        key_38 = None
        key_39 = view_138.transpose(2, 1)
        view_138 = None
        view_139 = value_38.view((1, 199, 16, 64))
        value_38 = None
        value_39 = view_139.transpose(2, 1)
        view_139 = None
        attn_output_57 = torch._C._nn.scaled_dot_product_attention(
            query_39,
            key_39,
            value_39,
            attn_mask=attn_mask_rel_pos_39,
            dropout_p=0.0,
            is_causal=False,
        )
        query_39 = key_39 = value_39 = attn_mask_rel_pos_39 = None
        transpose_96 = attn_output_57.transpose(1, 2)
        attn_output_57 = None
        attn_output_58 = transpose_96.reshape(1, -1, 1024)
        transpose_96 = None
        attn_output_59 = torch._C._nn.linear(
            attn_output_58,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_58 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_220 = torch.nn.functional.dropout(attn_output_59, 0.1, False, False)
        attn_output_59 = None
        x_221 = x_218 + x_220
        x_218 = x_220 = None
        layer_norm_48 = torch.nn.functional.layer_norm(
            x_221,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_final_layer_norm_parameters_bias_ = (None)
        x_222 = torch._C._nn.linear(
            layer_norm_48,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_48 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_223 = torch._C._nn.gelu(x_222)
        x_222 = None
        x_224 = torch.nn.functional.dropout(x_223, 0.0, False, False)
        x_223 = None
        x_225 = torch._C._nn.linear(
            x_224,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_224 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_19_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_226 = torch.nn.functional.dropout(x_225, 0.1, False, False)
        x_225 = None
        x_227 = x_221 + x_226
        x_221 = x_226 = None
        x_228 = torch.nn.functional.layer_norm(
            x_227,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_layer_norm_parameters_bias_ = (None)
        query_layer_40 = x_228.view(1, 199, 16, -1)
        query_layer_41 = query_layer_40.permute(0, 2, 1, 3)
        query_layer_40 = None
        linear_101 = torch._C._nn.linear(
            query_layer_41,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_41 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_141 = linear_101.view(1, 16, 199, 2, 4)
        linear_101 = None
        sum_21 = view_141.sum(-1, keepdim=False)
        view_141 = None
        sigmoid_20 = torch.sigmoid(sum_21)
        sum_21 = None
        chunk_40 = sigmoid_20.chunk(2, dim=-1)
        sigmoid_20 = None
        gate_a_40 = chunk_40[0]
        gate_b_20 = chunk_40[1]
        chunk_40 = None
        mul_62 = (
            gate_b_20
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_20 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_21 = mul_62 - 1.0
        mul_62 = None
        mul_63 = gate_a_40 * sub_21
        gate_a_40 = sub_21 = None
        gate_a_41 = mul_63 + 2.0
        mul_63 = None
        view_142 = gate_a_41.view(1, 16, -1, 1)
        gate_a_41 = None
        attn_mask_rel_pos_40 = view_142 * position_bias
        view_142 = None
        attn_mask_rel_pos_41 = attn_mask_rel_pos_40.view((1, 16, 199, 199))
        attn_mask_rel_pos_40 = None
        query_projected_20 = torch._C._nn.linear(
            x_228,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_228 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_41 = query_projected_20.chunk(3, -1)
        query_projected_20 = None
        query_40 = chunk_41[0]
        key_40 = chunk_41[1]
        value_40 = chunk_41[2]
        chunk_41 = None
        view_144 = query_40.view((1, 199, 16, 64))
        query_40 = None
        query_41 = view_144.transpose(2, 1)
        view_144 = None
        view_145 = key_40.view((1, 199, 16, 64))
        key_40 = None
        key_41 = view_145.transpose(2, 1)
        view_145 = None
        view_146 = value_40.view((1, 199, 16, 64))
        value_40 = None
        value_41 = view_146.transpose(2, 1)
        view_146 = None
        attn_output_60 = torch._C._nn.scaled_dot_product_attention(
            query_41,
            key_41,
            value_41,
            attn_mask=attn_mask_rel_pos_41,
            dropout_p=0.0,
            is_causal=False,
        )
        query_41 = key_41 = value_41 = attn_mask_rel_pos_41 = None
        transpose_100 = attn_output_60.transpose(1, 2)
        attn_output_60 = None
        attn_output_61 = transpose_100.reshape(1, -1, 1024)
        transpose_100 = None
        attn_output_62 = torch._C._nn.linear(
            attn_output_61,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_61 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_229 = torch.nn.functional.dropout(attn_output_62, 0.1, False, False)
        attn_output_62 = None
        x_230 = x_227 + x_229
        x_227 = x_229 = None
        layer_norm_50 = torch.nn.functional.layer_norm(
            x_230,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_final_layer_norm_parameters_bias_ = (None)
        x_231 = torch._C._nn.linear(
            layer_norm_50,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_50 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_232 = torch._C._nn.gelu(x_231)
        x_231 = None
        x_233 = torch.nn.functional.dropout(x_232, 0.0, False, False)
        x_232 = None
        x_234 = torch._C._nn.linear(
            x_233,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_233 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_20_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_235 = torch.nn.functional.dropout(x_234, 0.1, False, False)
        x_234 = None
        x_236 = x_230 + x_235
        x_230 = x_235 = None
        x_237 = torch.nn.functional.layer_norm(
            x_236,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_layer_norm_parameters_bias_ = (None)
        query_layer_42 = x_237.view(1, 199, 16, -1)
        query_layer_43 = query_layer_42.permute(0, 2, 1, 3)
        query_layer_42 = None
        linear_106 = torch._C._nn.linear(
            query_layer_43,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_43 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_148 = linear_106.view(1, 16, 199, 2, 4)
        linear_106 = None
        sum_22 = view_148.sum(-1, keepdim=False)
        view_148 = None
        sigmoid_21 = torch.sigmoid(sum_22)
        sum_22 = None
        chunk_42 = sigmoid_21.chunk(2, dim=-1)
        sigmoid_21 = None
        gate_a_42 = chunk_42[0]
        gate_b_21 = chunk_42[1]
        chunk_42 = None
        mul_65 = (
            gate_b_21
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_21 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_22 = mul_65 - 1.0
        mul_65 = None
        mul_66 = gate_a_42 * sub_22
        gate_a_42 = sub_22 = None
        gate_a_43 = mul_66 + 2.0
        mul_66 = None
        view_149 = gate_a_43.view(1, 16, -1, 1)
        gate_a_43 = None
        attn_mask_rel_pos_42 = view_149 * position_bias
        view_149 = None
        attn_mask_rel_pos_43 = attn_mask_rel_pos_42.view((1, 16, 199, 199))
        attn_mask_rel_pos_42 = None
        query_projected_21 = torch._C._nn.linear(
            x_237,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_237 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_43 = query_projected_21.chunk(3, -1)
        query_projected_21 = None
        query_42 = chunk_43[0]
        key_42 = chunk_43[1]
        value_42 = chunk_43[2]
        chunk_43 = None
        view_151 = query_42.view((1, 199, 16, 64))
        query_42 = None
        query_43 = view_151.transpose(2, 1)
        view_151 = None
        view_152 = key_42.view((1, 199, 16, 64))
        key_42 = None
        key_43 = view_152.transpose(2, 1)
        view_152 = None
        view_153 = value_42.view((1, 199, 16, 64))
        value_42 = None
        value_43 = view_153.transpose(2, 1)
        view_153 = None
        attn_output_63 = torch._C._nn.scaled_dot_product_attention(
            query_43,
            key_43,
            value_43,
            attn_mask=attn_mask_rel_pos_43,
            dropout_p=0.0,
            is_causal=False,
        )
        query_43 = key_43 = value_43 = attn_mask_rel_pos_43 = None
        transpose_104 = attn_output_63.transpose(1, 2)
        attn_output_63 = None
        attn_output_64 = transpose_104.reshape(1, -1, 1024)
        transpose_104 = None
        attn_output_65 = torch._C._nn.linear(
            attn_output_64,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_64 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_238 = torch.nn.functional.dropout(attn_output_65, 0.1, False, False)
        attn_output_65 = None
        x_239 = x_236 + x_238
        x_236 = x_238 = None
        layer_norm_52 = torch.nn.functional.layer_norm(
            x_239,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_final_layer_norm_parameters_bias_ = (None)
        x_240 = torch._C._nn.linear(
            layer_norm_52,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_52 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_241 = torch._C._nn.gelu(x_240)
        x_240 = None
        x_242 = torch.nn.functional.dropout(x_241, 0.0, False, False)
        x_241 = None
        x_243 = torch._C._nn.linear(
            x_242,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_242 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_21_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_244 = torch.nn.functional.dropout(x_243, 0.1, False, False)
        x_243 = None
        x_245 = x_239 + x_244
        x_239 = x_244 = None
        x_246 = torch.nn.functional.layer_norm(
            x_245,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_layer_norm_parameters_bias_ = (None)
        query_layer_44 = x_246.view(1, 199, 16, -1)
        query_layer_45 = query_layer_44.permute(0, 2, 1, 3)
        query_layer_44 = None
        linear_111 = torch._C._nn.linear(
            query_layer_45,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_45 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_155 = linear_111.view(1, 16, 199, 2, 4)
        linear_111 = None
        sum_23 = view_155.sum(-1, keepdim=False)
        view_155 = None
        sigmoid_22 = torch.sigmoid(sum_23)
        sum_23 = None
        chunk_44 = sigmoid_22.chunk(2, dim=-1)
        sigmoid_22 = None
        gate_a_44 = chunk_44[0]
        gate_b_22 = chunk_44[1]
        chunk_44 = None
        mul_68 = (
            gate_b_22
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_22 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_23 = mul_68 - 1.0
        mul_68 = None
        mul_69 = gate_a_44 * sub_23
        gate_a_44 = sub_23 = None
        gate_a_45 = mul_69 + 2.0
        mul_69 = None
        view_156 = gate_a_45.view(1, 16, -1, 1)
        gate_a_45 = None
        attn_mask_rel_pos_44 = view_156 * position_bias
        view_156 = None
        attn_mask_rel_pos_45 = attn_mask_rel_pos_44.view((1, 16, 199, 199))
        attn_mask_rel_pos_44 = None
        query_projected_22 = torch._C._nn.linear(
            x_246,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_246 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_45 = query_projected_22.chunk(3, -1)
        query_projected_22 = None
        query_44 = chunk_45[0]
        key_44 = chunk_45[1]
        value_44 = chunk_45[2]
        chunk_45 = None
        view_158 = query_44.view((1, 199, 16, 64))
        query_44 = None
        query_45 = view_158.transpose(2, 1)
        view_158 = None
        view_159 = key_44.view((1, 199, 16, 64))
        key_44 = None
        key_45 = view_159.transpose(2, 1)
        view_159 = None
        view_160 = value_44.view((1, 199, 16, 64))
        value_44 = None
        value_45 = view_160.transpose(2, 1)
        view_160 = None
        attn_output_66 = torch._C._nn.scaled_dot_product_attention(
            query_45,
            key_45,
            value_45,
            attn_mask=attn_mask_rel_pos_45,
            dropout_p=0.0,
            is_causal=False,
        )
        query_45 = key_45 = value_45 = attn_mask_rel_pos_45 = None
        transpose_108 = attn_output_66.transpose(1, 2)
        attn_output_66 = None
        attn_output_67 = transpose_108.reshape(1, -1, 1024)
        transpose_108 = None
        attn_output_68 = torch._C._nn.linear(
            attn_output_67,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_67 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_247 = torch.nn.functional.dropout(attn_output_68, 0.1, False, False)
        attn_output_68 = None
        x_248 = x_245 + x_247
        x_245 = x_247 = None
        layer_norm_54 = torch.nn.functional.layer_norm(
            x_248,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_final_layer_norm_parameters_bias_ = (None)
        x_249 = torch._C._nn.linear(
            layer_norm_54,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_54 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_250 = torch._C._nn.gelu(x_249)
        x_249 = None
        x_251 = torch.nn.functional.dropout(x_250, 0.0, False, False)
        x_250 = None
        x_252 = torch._C._nn.linear(
            x_251,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_251 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_22_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_253 = torch.nn.functional.dropout(x_252, 0.1, False, False)
        x_252 = None
        x_254 = x_248 + x_253
        x_248 = x_253 = None
        x_255 = torch.nn.functional.layer_norm(
            x_254,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_layer_norm_parameters_bias_ = (None)
        query_layer_46 = x_255.view(1, 199, 16, -1)
        query_layer_47 = query_layer_46.permute(0, 2, 1, 3)
        query_layer_46 = None
        linear_116 = torch._C._nn.linear(
            query_layer_47,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_bias_,
        )
        query_layer_47 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_gru_rel_pos_linear_parameters_bias_ = (None)
        view_162 = linear_116.view(1, 16, 199, 2, 4)
        linear_116 = None
        sum_24 = view_162.sum(-1, keepdim=False)
        view_162 = None
        sigmoid_23 = torch.sigmoid(sum_24)
        sum_24 = None
        chunk_46 = sigmoid_23.chunk(2, dim=-1)
        sigmoid_23 = None
        gate_a_46 = chunk_46[0]
        gate_b_23 = chunk_46[1]
        chunk_46 = None
        mul_71 = (
            gate_b_23
            * l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_parameters_gru_rel_pos_const_
        )
        gate_b_23 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_parameters_gru_rel_pos_const_ = (None)
        sub_24 = mul_71 - 1.0
        mul_71 = None
        mul_72 = gate_a_46 * sub_24
        gate_a_46 = sub_24 = None
        gate_a_47 = mul_72 + 2.0
        mul_72 = None
        view_163 = gate_a_47.view(1, 16, -1, 1)
        gate_a_47 = None
        attn_mask_rel_pos_46 = view_163 * position_bias
        view_163 = position_bias = None
        attn_mask_rel_pos_47 = attn_mask_rel_pos_46.view((1, 16, 199, 199))
        attn_mask_rel_pos_46 = None
        query_projected_23 = torch._C._nn.linear(
            x_255,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_bias_,
        )
        x_255 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_parameters_in_proj_bias_ = (None)
        chunk_47 = query_projected_23.chunk(3, -1)
        query_projected_23 = None
        query_46 = chunk_47[0]
        key_46 = chunk_47[1]
        value_46 = chunk_47[2]
        chunk_47 = None
        view_165 = query_46.view((1, 199, 16, 64))
        query_46 = None
        query_47 = view_165.transpose(2, 1)
        view_165 = None
        view_166 = key_46.view((1, 199, 16, 64))
        key_46 = None
        key_47 = view_166.transpose(2, 1)
        view_166 = None
        view_167 = value_46.view((1, 199, 16, 64))
        value_46 = None
        value_47 = view_167.transpose(2, 1)
        view_167 = None
        attn_output_69 = torch._C._nn.scaled_dot_product_attention(
            query_47,
            key_47,
            value_47,
            attn_mask=attn_mask_rel_pos_47,
            dropout_p=0.0,
            is_causal=False,
        )
        query_47 = key_47 = value_47 = attn_mask_rel_pos_47 = None
        transpose_112 = attn_output_69.transpose(1, 2)
        attn_output_69 = None
        attn_output_70 = transpose_112.reshape(1, -1, 1024)
        transpose_112 = None
        attn_output_71 = torch._C._nn.linear(
            attn_output_70,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_bias_,
        )
        attn_output_70 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_attention_modules_attention_modules_out_proj_parameters_bias_ = (None)
        x_256 = torch.nn.functional.dropout(attn_output_71, 0.1, False, False)
        attn_output_71 = None
        x_257 = x_254 + x_256
        x_254 = x_256 = None
        layer_norm_56 = torch.nn.functional.layer_norm(
            x_257,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_bias_,
            1e-05,
        )
        l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_final_layer_norm_parameters_bias_ = (None)
        x_258 = torch._C._nn.linear(
            layer_norm_56,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_bias_,
        )
        layer_norm_56 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_intermediate_dense_parameters_bias_ = (None)
        x_259 = torch._C._nn.gelu(x_258)
        x_258 = None
        x_260 = torch.nn.functional.dropout(x_259, 0.0, False, False)
        x_259 = None
        x_261 = torch._C._nn.linear(
            x_260,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_bias_,
        )
        x_260 = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layers_modules_23_modules_feed_forward_modules_output_dense_parameters_bias_ = (None)
        x_262 = torch.nn.functional.dropout(x_261, 0.1, False, False)
        x_261 = None
        x_263 = x_257 + x_262
        x_257 = x_262 = None
        x_264 = torch.nn.functional.layer_norm(
            x_263,
            (1024,),
            l_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_weight_,
            l_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_bias_,
            1e-05,
        )
        x_263 = l_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_weight_ = l_self_modules_model_modules_encoder_modules_transformer_modules_layer_norm_parameters_bias_ = (None)
        return (x_264,)
